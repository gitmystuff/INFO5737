{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/INFO5737/blob/main/Anomaly_Detection_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection II\n",
        "\n",
        "Your Name"
      ],
      "metadata": {
        "id": "7FtKv6iu_TF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isolation Forest"
      ],
      "metadata": {
        "id": "7IfLCUq_tEYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outliers"
      ],
      "metadata": {
        "id": "e7eJp0aKtGLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are data points that deviate significantly from the general pattern or distribution of the rest of the data. They are observations that are far removed from the \"typical\" or \"expected\" values.\n",
        "\n",
        "**Types of Outliers:**\n",
        "\n",
        "* **Point Outliers:** Single data points that are unusual compared to the rest of the data (e.g., a very high income in a neighborhood).\n",
        "* **Contextual Outliers:** Data points that are outliers only in a specific context (e.g., high electricity usage at night in a residential area).\n",
        "* **Collective Outliers:** A group of data points that are unusual when considered together, even if individual points might not be outliers on their own (e.g., a coordinated series of small fraudulent transactions).\n",
        "\n",
        "**Why are Outliers Important?**\n",
        "\n",
        "* **Data Quality:** They can indicate errors in data collection or measurement.\n",
        "* **Insights:** They can reveal valuable information about rare events or unusual behavior (e.g., fraud, network intrusions).\n",
        "* **Model Accuracy:** They can distort statistical analyses and machine learning models.\n",
        "\n",
        "**Python Methods for Outlier Detection:**\n",
        "\n",
        "Here are some common Python methods for detecting outliers:\n",
        "\n",
        "**1. Statistical Methods:**\n",
        "\n",
        "   * **Z-Score:**\n",
        "        * Calculates how many standard deviations a data point is away from the mean.\n",
        "        * Points with a Z-score above a certain threshold (e.g., 3) are considered outliers."
      ],
      "metadata": {
        "id": "d_Xx8KpKuy9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# normal_values = np.random.normal(loc=15, scale=3, size=20)\n",
        "# outlier_values = [50, 60]\n",
        "# data = {'value': np.concatenate([normal_values, outlier_values])}\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# df['z_score'] = np.abs(stats.zscore(df['value']))\n",
        "# df['is_outlier'] = df['z_score'] > 3\n",
        "\n",
        "# mean = df['value'].mean()\n",
        "# std = df['value'].std()\n",
        "\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# sns.histplot(df['value'], kde=True)\n",
        "# plt.axvline(mean, color='green', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n",
        "# plt.axvline(mean + std, color='orange', linestyle='dashed', linewidth=1, label=f'+1 Std Dev: {mean + std:.2f}')\n",
        "# plt.axvline(mean - std, color='orange', linestyle='dashed', linewidth=1, label=f'-1 Std Dev: {mean - std:.2f}')\n",
        "# plt.axvline(mean + 2 * std, color='cyan', linestyle='dashed', linewidth=1, label=f'+2 Std Dev: {mean + 2 * std:.2f}')\n",
        "# plt.axvline(mean - 2 * std, color='cyan', linestyle='dashed', linewidth=1, label=f'-2 Std Dev: {mean - 2 * std:.2f}')\n",
        "# plt.axvline(mean + 3 * std, color='magenta', linestyle='dashed', linewidth=1, label=f'+3 Std Dev: {mean + 3 * std:.2f}')\n",
        "# plt.axvline(mean - 3 * std, color='magenta', linestyle='dashed', linewidth=1, label=f'-3 Std Dev: {mean - 3 * std:.2f}')\n",
        "# # plt.axvline(df[df['is_outlier'] == True]['value'].min(), color='red', linestyle='dashed', linewidth=1, label='Outlier Bounds')\n",
        "# # plt.axvline(df[df['is_outlier'] == True]['value'].max(), color='red', linestyle='dashed', linewidth=1)\n",
        "# plt.title('Distribution of Data with Outliers and Standard Deviations')\n",
        "# plt.xlabel('Value')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "mS8gDvWLxgh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **IQR (Interquartile Range):**\n",
        "  * Defines a \"normal\" range using the 1st quartile (Q1) and 3rd quartile (Q3).\n",
        "  * Points outside 1.5 * IQR from Q1 or Q3 are considered outliers."
      ],
      "metadata": {
        "id": "95PcaB-Huy9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def find_iqr_outliers(data):\n",
        "#   q1 = np.percentile(data, 25)\n",
        "#   q3 = np.percentile(data, 75)\n",
        "#   iqr = q3 - q1\n",
        "#   lower_bound = q1 - 1.5 * iqr\n",
        "#   upper_bound = q3 + 1.5 * iqr\n",
        "#   return data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "# outliers = find_iqr_outliers(df['value'])\n",
        "# print(outliers)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.boxplot(y=df['value'])\n",
        "# plt.title('Box Plot with IQR Outlier Visualization')\n",
        "# plt.ylabel('Value')\n",
        "# plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FVflRMtTuy9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Machine Learning Methods:**\n",
        "\n",
        "   * **Isolation Forest:**\n",
        "        * An unsupervised algorithm that isolates outliers by randomly partitioning the data space.\n",
        "        * Anomalies are \"easier\" to isolate and have shorter path lengths in the trees."
      ],
      "metadata": {
        "id": "toMjFZ2huy9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# model = IsolationForest(contamination=0.1)  # Adjust contamination\n",
        "# model.fit(df[['value']])\n",
        "# df['anomaly'] = model.predict(df[['value']])\n",
        "# outliers = df[df['anomaly'] == -1]\n",
        "# print(outliers)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "u8tntt0huy9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA\n",
        "\n",
        "* Dimensionality Notebook (DSChunks)"
      ],
      "metadata": {
        "id": "qJ3wQi8g8RTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a powerful technique used for dimensionality reduction. In simpler terms, it helps to simplify complex data by reducing the number of variables while retaining the most important information.\n",
        "\n",
        "Here's a breakdown of what PCA is and how it works:\n",
        "\n",
        "**What is PCA?**\n",
        "\n",
        "* **Dimensionality Reduction:** PCA's primary goal is to reduce the number of features (or dimensions) in a dataset. When you have a dataset with many variables (e.g., hundreds or thousands), it can be difficult to analyze and visualize. PCA helps to simplify this.\n",
        "* **Information Preservation:** While reducing dimensionality, PCA tries to keep as much of the original information as possible. It does this by identifying the most important patterns or trends in the data.\n",
        "* **Unsupervised Learning:** PCA is an unsupervised learning technique, meaning it doesn't need labeled data to work. It finds patterns in the data itself.\n",
        "\n",
        "**How Does PCA Work?**\n",
        "\n",
        "PCA works by transforming the original variables into a new set of variables called \"principal components.\" These principal components have some special properties:\n",
        "\n",
        "1.  **Principal Components are Linear Combinations:** Each principal component is a linear combination of the original variables. This means it's calculated by adding up the original variables, each multiplied by a specific weight.\n",
        "2.  **Principal Components are Uncorrelated:** The principal components are uncorrelated with each other. This means they measure different, independent aspects of the data.\n",
        "3.  **Principal Components Capture Variance:** The most important principal components are the ones that capture the most variance (spread or variability) in the data. The first principal component captures the most variance, the second captures the second most, and so on.\n",
        "\n",
        "**Steps Involved in PCA:**\n",
        "\n",
        "1.  **Standardize the Data:** It's important to standardize the data before applying PCA. This means scaling the data so that each variable has a mean of 0 and a standard deviation of 1. This prevents variables with larger scales from dominating the PCA results.\n",
        "2.  **Calculate the Covariance Matrix:** The covariance matrix measures how much the variables change together. It helps to identify relationships between the variables.\n",
        "3.  **Calculate Eigenvectors and Eigenvalues:**\n",
        "    * **Eigenvectors:** These are vectors that define the directions of the principal components. They show the directions in which the data has the most variance.\n",
        "    * **Eigenvalues:** These values indicate the magnitude of the variance captured by each principal component. The higher the eigenvalue, the more important the corresponding principal component.\n",
        "4.  **Select Principal Components:**\n",
        "    * The principal components are ordered by their eigenvalues, from highest to lowest.\n",
        "    * You can choose to keep the top k principal components, where k is the desired number of dimensions. This reduces the dimensionality of the data while preserving the most important information.\n",
        "5.  **Transform the Data:** The original data is transformed into the new coordinate system defined by the selected principal components.\n",
        "\n",
        "**Why Use PCA?**\n",
        "\n",
        "* **Visualization:** PCA can reduce high-dimensional data to 2 or 3 dimensions, making it possible to visualize the data in a scatter plot and identify patterns or clusters.\n",
        "* **Feature Extraction:** PCA can extract the most important features from a dataset, which can be useful for machine learning.\n",
        "* **Noise Reduction:** PCA can help to reduce noise in the data by focusing on the principal components that capture the most significant variance.\n",
        "* **Speeding Up Machine Learning Algorithms:** Reducing the number of features can speed up the training and prediction time of machine learning algorithms.\n",
        "\n",
        "In summary, PCA is a valuable technique for simplifying complex data by reducing dimensionality while retaining essential information. It has applications in various fields, including data analysis, machine learning, and image processing."
      ],
      "metadata": {
        "id": "nSJ-IHdz9nvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anomaly Detection Through Isolation"
      ],
      "metadata": {
        "id": "vhzFtXW2zeUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isolation Forest is an unsupervised machine learning algorithm specifically designed for **anomaly detection**. Unlike many other anomaly detection methods that try to model \"normal\" data and then identify deviations, Isolation Forest takes a different approach: it explicitly tries to **isolate** anomalies.\n",
        "\n",
        "Here's a breakdown of the core ideas behind Isolation Forest:\n",
        "\n",
        "**1. The Principle of Isolation:**\n",
        "\n",
        "* Anomalies are data points that are \"few and different.\" This means they have attribute values that are far from the typical values of the majority of the data.\n",
        "* Because anomalies are different, they should be easier to separate (isolate) from the rest of the data with fewer partitioning steps.\n",
        "\n",
        "**2. Isolation Trees (iTrees):**\n",
        "\n",
        "* Isolation Forest builds an ensemble (a \"forest\") of **Isolation Trees (iTrees)**.\n",
        "* Each iTree is a binary tree constructed by randomly partitioning the data. The partitioning process is as follows:\n",
        "    * **Random Feature Selection:** A feature (attribute) from the dataset is randomly selected.\n",
        "    * **Random Split Value Selection:** A split value within the range of the selected feature is randomly chosen.\n",
        "    * **Data Partitioning:** The data points are divided into two child nodes based on whether their value for the chosen feature is below or above the split value.\n",
        "    * This process is repeated recursively until each data point is isolated in its own leaf node or a predefined tree height limit is reached.\n",
        "\n",
        "**3. Path Length:**\n",
        "\n",
        "* The key concept in Isolation Forest is the **path length** of a data point in an iTree. The path length is the number of edges traversed from the root of the tree to the leaf node containing that data point.\n",
        "* **Anomalies tend to have shorter path lengths.** This is because they are different and can be isolated with fewer random partitions.\n",
        "* **Normal data points tend to have longer path lengths** because they are similar to other points and require more partitions to be isolated.\n",
        "\n",
        "**4. Anomaly Score:**\n",
        "\n",
        "* To determine an anomaly score for each data point, the algorithm calculates the average path length of that point across all the iTrees in the forest.\n",
        "* This average path length is then normalized to produce an **anomaly score** between 0 and 1:\n",
        "    * Scores closer to 1 indicate a higher likelihood of being an anomaly.\n",
        "    * Scores closer to 0 indicate that the data point is likely normal.\n",
        "    * A score around 0.5 typically means the point is neither clearly an anomaly nor a normal instance.\n",
        "\n",
        "**In essence, Isolation Forest works by:**\n",
        "\n",
        "* Randomly partitioning the data in multiple trees.\n",
        "* Measuring how quickly each data point gets isolated.\n",
        "* Assigning an anomaly score based on the average isolation path length across all trees. Data points that are isolated in fewer steps are considered more likely to be anomalies.\n",
        "\n",
        "**Advantages of Isolation Forest:**\n",
        "\n",
        "* **Efficient:** It has a linear time complexity with respect to the number of data points and features, making it suitable for large datasets.\n",
        "* **Fast:** The random partitioning process is computationally inexpensive.\n",
        "* **Effective for High-Dimensional Data:** It can handle datasets with many features.\n",
        "* **Unsupervised:** It doesn't require labeled anomaly data for training.\n",
        "* **Robust to Irrelevant Features:** The random feature selection helps in focusing on the features that contribute to isolating anomalies.\n",
        "* **Low Memory Usage:** The memory requirements are relatively low compared to some other anomaly detection methods.\n",
        "\n",
        "**Use Cases in Cybersecurity (and beyond):**\n",
        "\n",
        "* **Network Intrusion Detection:** Identifying unusual network traffic patterns that might indicate attacks.\n",
        "* **Fraud Detection:** Detecting anomalous financial transactions.\n",
        "* **Endpoint Security:** Identifying unusual process behavior or file modifications on computers.\n",
        "* **System Monitoring:** Detecting abnormal resource usage or system events.\n",
        "* **Industrial Anomaly Detection:** Identifying faulty machinery behavior based on sensor data.\n",
        "\n",
        "By understanding the principles of isolation, path length, and anomaly scoring, you can effectively use and explain the Isolation Forest algorithm in your cybersecurity class for analyzing network packets and detecting anomalies."
      ],
      "metadata": {
        "id": "W-L7ffXf-ur5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The dataset we are using is a subset from a Machine Learning competition run by Xeek and FORCE 2020 (Bormann et al., 2020). It is released under a NOLD 2.0 licence from the Norwegian Government, details of which can be found here: Norwegian Licence for Open Government Data (NLOD) 2.0.\n",
        "\n",
        "The full dataset can be accessed at the following link: https://doi.org/10.5281/zenodo.4351155."
      ],
      "metadata": {
        "id": "YoqjnuvfJrnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b490772f"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/refs/heads/main/Xeek_Well.csv')\n",
        "# df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efc81b8e"
      },
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Context:**\n",
        "\n",
        "The image shows the output of the `df.describe()` method in Python's Pandas library after reading a CSV file named 'Xeek_Well_15-9-15.csv' into a DataFrame called `df`. This method provides descriptive statistics of the numerical columns in the DataFrame.\n",
        "\n",
        "**Dataset Description:**\n",
        "\n",
        "The dataset appears to contain well log data, likely from an oil or gas exploration context. Each row represents a measurement taken at a specific depth in the well, and the columns represent various geophysical measurements.\n",
        "\n",
        "**Columns and Their Descriptive Statistics:**\n",
        "\n",
        "Here's a detailed explanation of each column and the statistics provided:\n",
        "\n",
        "* **DEPTH_MD:**\n",
        "    * This column likely represents the measured depth in meters (or feet) in the well.\n",
        "    * **count:** 17717 - There are 17,717 data points (depth measurements).\n",
        "    * **mean:** 1837.363674 - The average depth is approximately 1837.36 meters.\n",
        "    * **std:** 784.314256 - The standard deviation indicates the spread or variability of the depth measurements.\n",
        "    * **min:** 485.256000 - The minimum depth recorded is 485.256 meters.\n",
        "    * **25%:** 1158.464000 - 25% of the depth measurements are less than or equal to 1158.464 meters.\n",
        "    * **50%:** 1831.672000 - The median depth is 1831.672 meters (half the measurements are above, half are below).\n",
        "    * **75%:** 2515.672000 - 75% of the depth measurements are less than or equal to 2515.672 meters.\n",
        "    * **max:** 3200.128000 - The maximum depth recorded is 3200.128 meters.\n",
        "\n",
        "* **CALI:**\n",
        "    * This column likely represents the caliper log, which measures the diameter of the wellbore.\n",
        "    * **count:** 17635.000000 - There are 17,635 caliper measurements (note: fewer than DEPTH_MD, indicating missing data).\n",
        "    * **mean:** 14.006030 - The average caliper measurement is 14.006030 (units depend on the tool).\n",
        "    * **std:** 3.873367 - The standard deviation indicates the variability in wellbore diameter.\n",
        "    * **min:** 7.325138 - The minimum caliper measurement.\n",
        "    * **25%:** 12.045594\n",
        "    * **50%:** 13.956721\n",
        "    * **75%:** 17.324830\n",
        "    * **max:** 25.717396\n",
        "\n",
        "* **RDEP:**\n",
        "    * This column likely represents the deep resistivity log, which measures the electrical resistance of the formation far from the wellbore.\n",
        "    * **count:** 17717.000000\n",
        "    * **mean:** 1.474893\n",
        "    * **std:** 1.356896\n",
        "    * **min:** 0.264479\n",
        "    * **25%:** 0.760341\n",
        "    * **50%:** 1.007371\n",
        "    * **75%:** 1.554278\n",
        "    * **max:** 14.046203\n",
        "\n",
        "* **RHOB:**\n",
        "    * This column likely represents the bulk density log, which measures the density of the formation.\n",
        "    * **count:** 17521.000000 - Again, missing data.\n",
        "    * **mean:** 2.134858\n",
        "    * **std:** 0.223124\n",
        "    * **min:** 1.438999\n",
        "    * **25%:** 1.978679\n",
        "    * **50%:** 2.042522\n",
        "    * **75%:** 2.333431\n",
        "    * **max:** 2.648847\n",
        "\n",
        "* **GR:**\n",
        "    * This column likely represents the gamma ray log, which measures the natural radioactivity of the formation.\n",
        "    * **count:** 17717.000000\n",
        "    * **mean:** 59.154202\n",
        "    * **std:** 29.483140\n",
        "    * **min:** 6.024419\n",
        "    * **25%:** 41.260944\n",
        "    * **50%:** 62.451527\n",
        "    * **75%:** 75.398460\n",
        "    * **max:** 804.298950 - This high max value suggests potential outliers or unusual readings.\n",
        "\n",
        "* **NPHI:**\n",
        "    * This column likely represents the neutron porosity log, which measures the porosity (pore space) of the formation.\n",
        "    * **count:** 13346.000000 - Significant missing data.\n",
        "    * **mean:** 0.384906\n",
        "    * **std:** 0.152182\n",
        "    * **min:** 0.039013\n",
        "    * **25%:** 0.249594\n",
        "    * **50%:** 0.451589\n",
        "    * **75%:** 0.510851\n",
        "    * **max:** 0.733152\n",
        "\n",
        "* **PEF:**\n",
        "    * This column likely represents the photoelectric effect log, which measures the formation's response to gamma rays.\n",
        "    * **count:** 17662.000000\n",
        "    * **mean:** 4.095857\n",
        "    * **std:** 8.318817 - A relatively high standard deviation suggests significant variability.\n",
        "    * **min:** 1.525528\n",
        "    * **25%:** 2.400372\n",
        "    * **50%:** 2.910137\n",
        "    * **75%:** 4.222030\n",
        "    * **max:** 365.575592 - The extremely high max value points to outliers.\n",
        "\n",
        "* **DTC:**\n",
        "    * This column likely represents the compressional wave slowness (Delta-T Compressional) log, which measures the time it takes for a compressional sound wave to travel through the formation.\n",
        "    * **count:** 17708.000000\n",
        "    * **mean:** 127.240157\n",
        "    * **std:** 36.507057\n",
        "    * **min:** 7.415132\n",
        "    * **25%:** 88.318405\n",
        "    * **50%:** 142.943245\n",
        "    * **75%:** 153.226116\n",
        "    * **max:** 207.382553\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "* **Missing Data:** The `CALI`, `RHOB`, and especially `NPHI` columns have fewer data points than `DEPTH_MD`, indicating missing values. This needs to be addressed before using the data for analysis or modeling.\n",
        "* **Outliers:** The `GR` and `PEF` columns have very high maximum values, suggesting potential outliers that could skew statistical analyses.\n",
        "* **Data Ranges:** The columns have vastly different ranges and units, which is typical for well log data. This often necessitates scaling or normalization before applying machine learning algorithms.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "This dataset contains well log measurements with varying statistical properties and issues like missing data and outliers. Understanding these characteristics is crucial for proper data cleaning, preprocessing, and analysis to extract meaningful insights about the subsurface formation."
      ],
      "metadata": {
        "id": "7sKEErqzLBdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e12079a"
      },
      "outputs": [],
      "source": [
        "# print(df.shape)\n",
        "# df = df.dropna()\n",
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6857727"
      },
      "outputs": [],
      "source": [
        "# code along"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **`anomaly_inputs = ['NPHI', 'RHOB']`**:\n",
        "    * This line creates a list named `anomaly_inputs`.\n",
        "    * The list contains two strings: 'NPHI' and 'RHOB'.\n",
        "    * These strings likely represent column names in a Pandas DataFrame.\n",
        "    * The code indicates that these two columns ('NPHI' and 'RHOB') will be used as the input features for the anomaly detection model.\n",
        "\n",
        "2.  **`model_IF = IsolationForest(contamination=float(0.1), random_state=42)`**:\n",
        "    * This line creates an instance of the `IsolationForest` class from scikit-learn.\n",
        "    * The instance is assigned to the variable `model_IF`.\n",
        "    * `contamination=float(0.1)`: This parameter tells the Isolation Forest algorithm to expect that approximately 10% of the data points are anomalies. It helps the algorithm determine a threshold for classifying data points as anomalies.\n",
        "    * `random_state=42`: This parameter sets the random seed for the algorithm. Setting a random seed ensures that the results of the algorithm are reproducible. If you run the code multiple times with the same random seed, you will get the same results.\n",
        "\n",
        "3.  **`model_IF.fit(df[anomaly_inputs])`**:\n",
        "    * This line trains the `model_IF` (the Isolation Forest model) on the data.\n",
        "    * `df[anomaly_inputs]`: This selects the columns specified in the `anomaly_inputs` list (i.e., 'NPHI' and 'RHOB') from a Pandas DataFrame called `df`. It creates a new DataFrame containing only these two columns.\n",
        "    * `model_IF.fit(...)`: The `fit()` method is called on the Isolation Forest model to learn the patterns of \"normal\" data from the selected columns.\n",
        "\n",
        "In summary, the code selects two columns from a DataFrame, configures an Isolation Forest model to detect anomalies assuming 10% contamination, and then trains the model on the selected data."
      ],
      "metadata": {
        "id": "1JRAe_WnLzUP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "477e8d20"
      },
      "outputs": [],
      "source": [
        "# code along"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's break down the code snippet without referencing the image.\n",
        "\n",
        "The code performs anomaly detection using a trained Isolation Forest model on a Pandas DataFrame. It calculates anomaly scores and assigns anomaly labels to the data.\n",
        "\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "1.  **`df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])`**:\n",
        "    * This line calculates an anomaly score for each data point using the trained Isolation Forest model (`model_IF`).\n",
        "    * `model_IF.decision_function(df[anomaly_inputs])`:\n",
        "        * `model_IF`: This is the trained Isolation Forest model.\n",
        "        * `decision_function()`: This is a method of the Isolation Forest model that calculates an \"anomaly score\" for each data point. The anomaly score reflects how \"isolated\" a data point is; lower scores generally indicate higher likelihood of being an anomaly.\n",
        "        * `df[anomaly_inputs]`: This selects the columns from the DataFrame `df` that were used as input features during the model training. These are the same columns for which we now want to calculate anomaly scores.\n",
        "    * `df['anomaly_scores'] = ...`: The calculated anomaly scores are then assigned to a new column named `anomaly_scores` in the DataFrame `df`.\n",
        "\n",
        "2.  **`df['anomaly'] = model_IF.predict(df[anomaly_inputs])`**:\n",
        "    * This line assigns anomaly labels to each data point based on the anomaly scores calculated in the previous step.\n",
        "    * `model_IF.predict(df[anomaly_inputs])`:\n",
        "        * `model_IF`: The trained Isolation Forest model.\n",
        "        * `predict()`: This is a method of the Isolation Forest model that assigns a label to each data point. Typically, it assigns:\n",
        "            * `1`: for normal data points (inliers).\n",
        "            * `-1`: for anomalous data points (outliers).\n",
        "        * `df[anomaly_inputs]`: Again, this selects the input feature columns from the DataFrame.\n",
        "    * `df['anomaly'] = ...`: The predicted anomaly labels (1 or -1) are assigned to a new column named `anomaly` in the DataFrame `df`.\n",
        "\n",
        "3.  **`df.loc[:, ['NPHI', 'RHOB', 'anomaly_scores', 'anomaly']]`**:\n",
        "    * This line selects and displays specific columns from the DataFrame `df`.\n",
        "    * `df.loc[:, ... ]`: This is a way to select rows and columns by label in Pandas.\n",
        "        * `:`: Selects all rows.\n",
        "        * `['NPHI', 'RHOB', 'anomaly_scores', 'anomaly']`: Selects the columns named 'NPHI', 'RHOB', 'anomaly_scores', and 'anomaly'.\n",
        "    * The result of this line is a new DataFrame (or a view of the original DataFrame) containing only the specified columns, which is then displayed. This allows you to see the original data ('NPHI', 'RHOB') along with the calculated anomaly information.\n",
        "\n",
        "In summary, the code calculates anomaly scores and labels using the Isolation Forest model and then displays a subset of the DataFrame to show the original data and the anomaly results."
      ],
      "metadata": {
        "id": "_jgYx3bnMNx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# print(\"Mean values for normal and anomalous data:\")\n",
        "# print(df.groupby('anomaly')[['NPHI', 'RHOB', 'anomaly_scores']].mean())\n",
        "\n",
        "# print(\"\\nStandard deviations:\")\n",
        "# print(df.groupby('anomaly')[['NPHI', 'RHOB', 'anomaly_scores']].std())\n",
        "\n",
        "# print(\"\\nDescriptive statistics for normal data:\")\n",
        "# print(df[df['anomaly'] == 1][['NPHI', 'RHOB']].describe())  # Normal data\n",
        "\n",
        "# print(\"\\nDescriptive statistics for anomalous data:\")\n",
        "# print(df[df['anomaly'] == -1][['NPHI', 'RHOB']].describe()) # Anomalous data\n",
        "\n",
        "# # Box plots to compare distributions\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.boxplot(x='anomaly', y='NPHI', data=df)\n",
        "# plt.title('Box Plot of NPHI by Anomaly Group')\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.boxplot(x='anomaly', y='RHOB', data=df)\n",
        "# plt.title('Box Plot of RHOB by Anomaly Group')\n",
        "# plt.show()\n",
        "\n",
        "# # Histograms to see frequency distributions\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(data=df, x='anomaly_scores', hue='anomaly', kde=True)\n",
        "# plt.title('Distribution of Anomaly Scores')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "tK3NlgjaMvQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c57a5bc0"
      },
      "outputs": [],
      "source": [
        "# def outlier_plot(data, outlier_method_name, x_var, y_var,\n",
        "#                  xaxis_limits=[0,1], yaxis_limits=[0,1]):\n",
        "\n",
        "#     print(f'Outlier Method: {outlier_method_name}')\n",
        "\n",
        "#     # Create a dynamic title based on the method\n",
        "#     method = f'{outlier_method_name}_anomaly'\n",
        "\n",
        "#     # Print out key statistics\n",
        "#     print(f\"Number of anomalous values {len(data[data['anomaly']==-1])}\")\n",
        "#     print(f\"Number of non anomalous values  {len(data[data['anomaly']== 1])}\")\n",
        "#     print(f'Total Number of Values: {len(data)}')\n",
        "\n",
        "#     # Create the chart using seaborn\n",
        "#     g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[1,-1])\n",
        "#     g.map(sns.scatterplot, x_var, y_var)\n",
        "#     g.fig.suptitle(f'Outlier Method: {outlier_method_name}', y=1.10, fontweight='bold')\n",
        "#     g.set(xlim=xaxis_limits, ylim=yaxis_limits)\n",
        "#     axes = g.axes.flatten()\n",
        "#     axes[0].set_title(f\"Outliers\\n{len(data[data['anomaly']== -1])} points\")\n",
        "#     axes[1].set_title(f\"Inliers\\n {len(data[data['anomaly']==  1])} points\")\n",
        "#     return g"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Function Definition:**\n",
        "\n",
        "```python\n",
        "def outlier_plot(data, outlier_method_name, x_var, y_var, xaxis_limits=[0,1], yaxis_limits=[0,1]):\n",
        "```\n",
        "\n",
        "* This line defines a function named `outlier_plot`.\n",
        "* It takes several arguments:\n",
        "    * `data`: The Pandas DataFrame containing the data.\n",
        "    * `outlier_method_name`: A string representing the name of the outlier detection method used (e.g., \"Isolation Forest\").\n",
        "    * `x_var`: The name of the column in the DataFrame to be used for the x-axis of the plot.\n",
        "    * `y_var`: The name of the column in the DataFrame to be used for the y-axis of the plot.\n",
        "    * `xaxis_limits`: (Optional) A list specifying the x-axis limits for the plot. It defaults to [0, 1].\n",
        "    * `yaxis_limits`: (Optional) A list specifying the y-axis limits for the plot. It defaults to [0, 1].\n",
        "\n",
        "**2. Print Outlier Method Name:**\n",
        "\n",
        "```python\n",
        "print(f'Outlier Method: {outlier_method_name}')\n",
        "```\n",
        "\n",
        "* This line prints the name of the outlier detection method to the console. This helps in identifying the results.\n",
        "\n",
        "**3. Create a Dynamic Title:**\n",
        "\n",
        "```python\n",
        "method = f'{outlier_method_name}_anomaly'\n",
        "```\n",
        "\n",
        "* This line creates a string variable `method` by appending \"_anomaly\" to the `outlier_method_name`. This is likely used to create a more descriptive title for the plot.\n",
        "\n",
        "**4. Print Key Statistics:**\n",
        "\n",
        "```python\n",
        "print(f\"Number of anomalous values {len(data[data['anomaly']== -1])}\")\n",
        "print(f\"Number of non anomalous values {len(data[data['anomaly']== 1])}\")\n",
        "print(f\"Total Number of Values: {len(data)}\")\n",
        "```\n",
        "\n",
        "* These lines print some important statistics about the detected anomalies:\n",
        "    * `len(data[data['anomaly'] == -1])`: Calculates the number of data points labeled as anomalous (assuming -1 represents anomalies).\n",
        "    * `len(data[data['anomaly'] == 1])`: Calculates the number of data points labeled as non-anomalous (assuming 1 represents normal data).\n",
        "    * `len(data)`: Prints the total number of data points in the DataFrame.\n",
        "\n",
        "**5. Create the Chart using Seaborn:**\n",
        "\n",
        "```python\n",
        "g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[1,-1])\n",
        "g.map(sns.scatterplot, x_var, y_var)\n",
        "```\n",
        "\n",
        "* This section uses the Seaborn library to create a visualization:\n",
        "    * `g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[1,-1])`: Creates a `FacetGrid`. This is a multi-plot grid that allows you to create separate plots for different subsets of your data.\n",
        "        * `data`: The DataFrame.\n",
        "        * `col='anomaly'`:  Specifies that the grid should have columns based on the unique values in the 'anomaly' column (i.e., it will create one subplot for anomalies and one for non-anomalies).\n",
        "        * `height=4`: Sets the height of each subplot.\n",
        "        * `hue='anomaly'`:  Colors the data points based on the 'anomaly' column.\n",
        "        * `hue_order=[1,-1]`:  Specifies the order of the hues (colors) in the legend.\n",
        "    * `g.map(sns.scatterplot, x_var, y_var)`:  Maps a scatter plot onto each facet (subplot) of the grid.\n",
        "        * `sns.scatterplot`:  The type of plot (a scatter plot).\n",
        "        * `x_var`:  The column to use for the x-axis.\n",
        "        * `y_var`:  The column to use for the y-axis.\n",
        "\n",
        "**6. Set Chart Titles and Labels:**\n",
        "\n",
        "```python\n",
        "g.fig.suptitle(f'Outlier Method: {outlier_method_name}', y=1.10, fontweight='bold')\n",
        "g.set(xlim=xaxis_limits, ylim=yaxis_limits)\n",
        "\n",
        "axes = g.axes.flatten()\n",
        "axes[0].set_title(f\"Outliers\\n{len(data[data['anomaly'] == -1])} points\")\n",
        "axes[1].set_title(f\"Inliers\\n{len(data[data['anomaly'] == 1])} points\")\n",
        "```\n",
        "\n",
        "* `g.fig.suptitle(...)`: Sets the overall title of the entire figure (the grid of plots).\n",
        "    * `f'Outlier Method: {outlier_method_name}'`:  Uses an f-string to include the name of the outlier detection method in the title.\n",
        "    * `y=1.10`:  Adjusts the vertical position of the title.\n",
        "    * `fontweight='bold'`:  Makes the title bold.\n",
        "* `g.set(xlim=xaxis_limits, ylim=yaxis_limits)`:  Sets the x-axis and y-axis limits for all subplots, using the `xaxis_limits` and `yaxis_limits` arguments passed to the function.\n",
        "* `axes = g.axes.flatten()`:  Gets a 1D array of all the axes (subplots) in the grid.\n",
        "* `axes[0].set_title(...)`: Sets the title of the first subplot (likely the one showing outliers). It includes the number of outlier points.\n",
        "* `axes[1].set_title(...)`: Sets the title of the second subplot (likely the one showing inliers/normal data). It includes the number of inlier points.\n",
        "\n",
        "**7. Return the Plot:**\n",
        "\n",
        "```python\n",
        "return g\n",
        "```\n",
        "\n",
        "* The function returns the `FacetGrid` object (`g`), which can be further customized or displayed.\n",
        "\n",
        "**In summary, this `outlier_plot` function takes data, the outlier detection method name, and two variables to plot, and then generates a visualization that:**\n",
        "\n",
        "* Shows the data points in a scatter plot.\n",
        "* Separates the data into two plots: one for outliers and one for normal data.\n",
        "* Provides titles and labels that clearly indicate the outlier detection method and the number of points in each group.\n",
        "\n",
        "This function is a useful tool for visualizing and understanding the results of outlier detection algorithms."
      ],
      "metadata": {
        "id": "tNBYP9jBNUot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d80ca8a"
      },
      "outputs": [],
      "source": [
        "# code along\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Function Call:**\n",
        "\n",
        "    ```python\n",
        "    outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);\n",
        "    ```\n",
        "\n",
        "    * The `outlier_plot` function is called.\n",
        "    * It's passed the following arguments:\n",
        "        * `df`:  A Pandas DataFrame containing the data.\n",
        "        * `'Isolation Forest'`:  The name of the outlier detection method used.\n",
        "        * `'NPHI'`:  The column from the DataFrame to be used for the x-axis of the plot.\n",
        "        * `'RHOB'`: The column from the DataFrame to be used for the y-axis of the plot.\n",
        "        * `[0, 0.8]`:  The x-axis limits for the plot.\n",
        "        * `[3, 1.5]`: The y-axis limits for the plot.\n",
        "\n",
        "2.  **Function Execution:**\n",
        "\n",
        "    Inside the `outlier_plot` function (as defined in a previous snippet), the following actions are performed:\n",
        "\n",
        "    * **Print Outlier Method Name:** The string 'Outlier Method: Isolation Forest' is printed.\n",
        "    * **Print Key Statistics:**\n",
        "        * The number of anomalous values is calculated and printed. This is done by counting the rows in the DataFrame where the 'anomaly' column is equal to -1.\n",
        "        * The number of non-anomalous values is calculated and printed. This is done by counting the rows in the DataFrame where the 'anomaly' column is equal to 1.\n",
        "        * The total number of values (rows) in the DataFrame is printed.\n",
        "    * **Create the Chart:**\n",
        "        * A Seaborn `FacetGrid` is created, which will generate a multi-plot grid. The data is split into two columns based on the 'anomaly' column. Data points are colored based on the 'anomaly' column.\n",
        "        * A scatter plot is mapped onto each facet of the grid, using 'NPHI' for the x-axis and 'RHOB' for the y-axis.\n",
        "        * Titles and labels are set for the figure and the individual subplots. The number of anomalous and non-anomalous points is included in the subplot titles.\n",
        "\n",
        "3.  **Output:**\n",
        "\n",
        "    The function generates a visualization (and prints some statistics) that shows:\n",
        "\n",
        "    * A scatter plot of 'NPHI' versus 'RHOB'.\n",
        "    * Two separate plots: one for data points identified as outliers, and one for data points identified as inliers (normal data).\n",
        "    * The number of data points classified as outliers and inliers.\n",
        "    * The name of the outlier detection method used (Isolation Forest).\n",
        "\n",
        "In summary, this code segment calls a plotting function to visualize and summarize the results of an Isolation Forest anomaly detection analysis, displaying the relationship between two variables ('NPHI' and 'RHOB') and highlighting the separation between detected outliers and normal data."
      ],
      "metadata": {
        "id": "qJSrQwIsOD59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e350e18"
      },
      "outputs": [],
      "source": [
        "# model_IF = IsolationForest(contamination=float(0.3), random_state=42)\n",
        "# model_IF.fit(df[anomaly_inputs])\n",
        "\n",
        "# df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "# df['anomaly'] = model_IF.predict(df[anomaly_inputs])\n",
        "# outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Model Initialization and Training:**\n",
        "\n",
        "    ```python\n",
        "    model_IF = IsolationForest(contamination=float(0.3), random_state=42)\n",
        "    model_IF.fit(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * An Isolation Forest model (`model_IF`) is created.\n",
        "    * `contamination=float(0.3)`: This parameter is set, indicating the model is expected to find approximately 30% of the data points as anomalies.\n",
        "    * `random_state=42`: A seed is set for the random number generator, ensuring the model's behavior is reproducible across different runs.\n",
        "    * `model_IF.fit(df[anomaly_inputs])`: The model is trained using the data specified by the `anomaly_inputs` variable. This means the model learns what \"normal\" data looks like.\n",
        "\n",
        "2.  **Anomaly Score and Label Assignment:**\n",
        "\n",
        "    ```python\n",
        "    df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "    df['anomaly'] = model_IF.predict(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * `df['anomaly_scores'] = ...`:  Anomaly scores are calculated for each data point using the trained model's `decision_function()`. Lower scores indicate a higher likelihood of being an anomaly. These scores are stored in a new column named `anomaly_scores` in the DataFrame.\n",
        "    * `df['anomaly'] = ...`: Anomaly labels are assigned to each data point using the trained model's `predict()` method.  The typical output is 1 for normal data points and -1 for anomalies. These labels are stored in a new column named `anomaly`.\n",
        "\n",
        "3.  **Visualization (Implied):**\n",
        "\n",
        "    ```python\n",
        "    outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);\n",
        "    ```\n",
        "\n",
        "    * A function `outlier_plot` is called to visualize the results.\n",
        "    * This function likely generates a scatter plot of the data, separating the data points labeled as anomalies from those labeled as normal.\n",
        "    * The plot uses the 'NPHI' and 'RHOB' columns as x and y axes, respectively.\n",
        "    * The x-axis limits are set to [0, 0.8], and the y-axis limits are set to [3, 1.5].\n",
        "    * The function also likely prints some summary statistics.\n",
        "\n",
        "4.  **Output Statistics:**\n",
        "\n",
        "    ```\n",
        "    Outlier Method: Isolation Forest\n",
        "    Number of anomalous values 3986\n",
        "    Number of non anomalous values 9304\n",
        "    Total Number of Values: 13290\n",
        "    ```\n",
        "\n",
        "    * These lines print:\n",
        "        * The name of the method used: \"Isolation Forest.\"\n",
        "        * The number of data points classified as anomalies: 3986.\n",
        "        * The number of data points classified as non-anomalous: 9304.\n",
        "        * The total number of data points: 13290.\n",
        "\n",
        "**In essence, the code:**\n",
        "\n",
        "1.  Trains an Isolation Forest model on a subset of the data.\n",
        "2.  Uses that model to assign anomaly scores and labels to the entire dataset.\n",
        "3.  Visualizes the results and provides summary statistics about the number of anomalies detected.\n",
        "\n",
        "The key takeaway is that the Isolation Forest algorithm identifies data points that deviate significantly from the learned \"normal\" patterns, and these deviations are then highlighted and quantified."
      ],
      "metadata": {
        "id": "YaM9D_49OfNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The `contamination` Parameter**\n",
        "\n",
        "The primary driver of the difference in the number of anomalous values is the `contamination` parameter of the `IsolationForest` algorithm in scikit-learn.\n",
        "\n",
        "* **What it does:** The `contamination` parameter is an estimate of the proportion of outliers in the dataset. It's a crucial parameter that influences how the Isolation Forest algorithm determines the threshold for classifying data points as anomalies.\n",
        "* **How it affects the results:**\n",
        "    * If you set a higher `contamination` value (e.g., 0.3, meaning you expect 30% outliers), the algorithm will be more aggressive in flagging data points as anomalies. It will find more points that it considers \"different\" to reach that target.\n",
        "    * If you set a lower `contamination` value (e.g., 0.01, meaning you expect 1% outliers), the algorithm will be more conservative and flag fewer points as anomalies. It will only identify the most extreme deviations.\n",
        "\n",
        "**Why the Numbers Change in Your Examples**\n",
        "\n",
        "The different numbers of anomalous and non-anomalous values indicate that the `contamination` parameter was likely set to different values in the different runs of the Isolation Forest algorithm.\n",
        "\n",
        "* **Example 1 (1329 Anomalies):** In the first example, the code shows:\n",
        "\n",
        "    ```\n",
        "    Number of anomalous values 1329\n",
        "    Number of non anomalous values 11961\n",
        "    Total Number of Values: 13290\n",
        "    ```\n",
        "\n",
        "    This suggests a relatively low contamination setting, as only a small fraction of the 13290 points are classified as outliers.\n",
        "\n",
        "* **Example 2 (3986 Anomalies):** In the second example, the code shows:\n",
        "\n",
        "    ```\n",
        "    Number of anomalous values 3986\n",
        "    Number of non anomalous values 9304\n",
        "    Total Number of Values : 13290\n",
        "    ```\n",
        "\n",
        "    This indicates a higher contamination setting, as a significantly larger proportion of the data is now labeled as anomalous.\n",
        "\n",
        "**Important Considerations**\n",
        "\n",
        "* **Choosing `contamination`:**\n",
        "    * Ideally, you should have some prior knowledge or estimate of the actual proportion of outliers in your data.\n",
        "    * If you don't have a good estimate, you might need to experiment with different `contamination` values and evaluate the results based on your domain knowledge and the visual patterns in the data.\n",
        "* **Impact on Interpretation:** The `contamination` parameter heavily influences how you interpret the results. A higher value might highlight more subtle deviations, while a lower value will focus on the most extreme outliers.\n",
        "\n",
        "In conclusion, the difference in the number of detected anomalies is directly caused by the different settings of the `contamination` parameter in the Isolation Forest algorithm."
      ],
      "metadata": {
        "id": "JDAmVdicQPYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17a20a6a"
      },
      "outputs": [],
      "source": [
        "# anomaly_inputs = ['NPHI', 'RHOB', 'GR', 'CALI', 'PEF', 'DTC']\n",
        "# model_IF = IsolationForest(contamination=0.1, random_state=42)\n",
        "# model_IF.fit(df[anomaly_inputs])\n",
        "# df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "# df['anomaly'] = model_IF.predict(df[anomaly_inputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8338481"
      },
      "outputs": [],
      "source": [
        "# code along"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Feature Selection:**\n",
        "\n",
        "    ```python\n",
        "    anomaly_inputs = ['NPHI', 'RHOB', 'GR', 'CALI', 'PEF', 'DTC']\n",
        "    ```\n",
        "\n",
        "    * A list named `anomaly_inputs` is created, containing the names of six columns: 'NPHI', 'RHOB', 'GR', 'CALI', 'PEF', and 'DTC'. These column names likely represent different well log measurements. This means the Isolation Forest model will now consider these six variables to identify anomalies, instead of just 'NPHI' and 'RHOB' as in previous examples.\n",
        "\n",
        "2.  **Model Initialization and Training:**\n",
        "\n",
        "    ```python\n",
        "    model_IF = IsolationForest(contamination=0.1, random_state=42)\n",
        "    model_IF.fit(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * An Isolation Forest model (`model_IF`) is created.\n",
        "    * `contamination=0.1`: The model is configured to expect 10% of the data points to be anomalies.\n",
        "    * `random_state=42`: A random seed is set for reproducibility.\n",
        "    * `model_IF.fit(df[anomaly_inputs])`: The model is trained using the specified columns from the DataFrame `df`.\n",
        "\n",
        "3.  **Anomaly Score and Label Assignment:**\n",
        "\n",
        "    ```python\n",
        "    df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "    df['anomaly'] = model_IF.predict(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * `df['anomaly_scores'] = ...`: Anomaly scores are calculated for each data point using the trained model. Lower scores indicate a higher likelihood of being an anomaly. These scores are stored in the `anomaly_scores` column.\n",
        "    * `df['anomaly'] = ...`: Anomaly labels (1 for normal, -1 for anomaly) are assigned to each data point and stored in the `anomaly` column.\n",
        "\n",
        "4.  **Visualization (Implied):**\n",
        "\n",
        "    ```python\n",
        "    outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);\n",
        "    ```\n",
        "\n",
        "    * The `outlier_plot` function is called to visualize the results.\n",
        "    * It's passed the DataFrame, the method name, 'NPHI' and 'RHOB' as the variables to plot, and the x and y axis limits.\n",
        "\n",
        "5.  **Output Statistics:**\n",
        "\n",
        "    ```\n",
        "    Outlier Method: Isolation Forest\n",
        "    Number of anomalous values 1329\n",
        "    Number of non anomalous values 11961\n",
        "    Total Number of Values: 13290\n",
        "    ```\n",
        "\n",
        "    * The output provides:\n",
        "        * The name of the method: \"Isolation Forest.\"\n",
        "        * The number of anomalous values: 1329.\n",
        "        * The number of non-anomalous values: 11961.\n",
        "        * The total number of values: 13290.\n",
        "\n",
        "**Key Differences from Previous Examples:**\n",
        "\n",
        "The main difference here is the **input features** used for anomaly detection. In this case, the Isolation Forest model is trained and makes predictions based on all six columns: 'NPHI', 'RHOB', 'GR', 'CALI', 'PEF', and 'DTC'. Previous examples might have used only 'NPHI' and 'RHOB'.\n",
        "\n",
        "This change in input features can significantly affect the results of the anomaly detection. The model now considers more information to determine what is \"normal\" and what is \"anomalous.\""
      ],
      "metadata": {
        "id": "cyf6KrZSQ09C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52d1fea4"
      },
      "outputs": [],
      "source": [
        "# palette = ['#ff7f0e', '#1f77b4']\n",
        "# sns.pairplot(df, vars=anomaly_inputs, hue='anomaly', palette=palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose of a Pairplot**\n",
        "\n",
        "A pairplot is a powerful visualization tool that helps you understand the relationships between multiple variables in your dataset. It creates a matrix of plots:\n",
        "\n",
        "* **Diagonal Subplots:** On the diagonal, you'll typically find the univariate distribution of each variable (e.g., histograms or kernel density estimates). This shows you the shape and spread of individual variables.\n",
        "* **Off-Diagonal Subplots:** The off-diagonal subplots are scatter plots. Each scatter plot shows the relationship between two different variables.\n",
        "\n",
        "**Why Pairplots Are Useful in Anomaly Detection**\n",
        "\n",
        "1.  **Visualizing Relationships:**\n",
        "    * Anomaly detection often relies on the assumption that anomalies deviate from the normal relationships between variables. Pairplots help you visually identify these deviations.\n",
        "    * For example, if two variables are normally strongly correlated, anomalies might appear as points that fall far off the general trend.\n",
        "\n",
        "2.  **Identifying Potential Separability:**\n",
        "    * Pairplots can reveal if anomalies tend to cluster in certain regions of the data space. This can help you understand if the anomaly detection algorithm is effectively separating anomalies from normal data.\n",
        "\n",
        "3.  **Feature Analysis:**\n",
        "    * Pairplots can give you insights into which features might be most useful for distinguishing anomalies. Variables that show clear separation between normal and anomalous points in the scatter plots are likely to be important.\n",
        "\n",
        "4.  **Data Exploration:**\n",
        "    * Before or after anomaly detection, pairplots are helpful for general data exploration. They can reveal patterns, correlations, and potential issues (e.g., outliers) in the data itself, which might be relevant to the anomaly detection process.\n",
        "\n",
        "**In the context of the image, the pairplot is likely used to:**\n",
        "\n",
        "* Visually explore the relationships between the features used in the anomaly detection.\n",
        "* See how the anomalies (likely colored differently, as indicated by the legend) are distributed with respect to these relationships.\n",
        "* Gain a better understanding of the characteristics that make the anomalies different from the normal data.\n",
        "\n",
        "**Left Plot (Vertical Pattern):**\n",
        "\n",
        "* **X-axis:** This axis has a very limited range, with most data points clustered at the leftmost side. The scale suggests it might represent a variable with low values.\n",
        "* **Y-axis:** This axis has a wider range of values.\n",
        "* **Relationship:** There's a strong vertical pattern.\n",
        "    * The \"normal\" data (blue) is concentrated in a narrow band on the left.\n",
        "    * The \"anomalies\" (orange) are scattered more widely along the vertical axis, but mostly on the right side of the blue cluster.\n",
        "\n",
        "    **Interpretation:** This suggests that the variable on the X-axis is a strong discriminator between normal and anomalous data. The normal data has a very specific, low range of values for this variable. The anomalous data has a wider range, including higher values, indicating that deviations in this variable are key to identifying outliers.\n",
        "\n",
        "**Right Plot (Diagonal/Curved Pattern):**\n",
        "\n",
        "* **X-axis:** This axis has a wider range of values.\n",
        "* **Y-axis:** This axis also has a wider range of values.\n",
        "* **Relationship:** There's a more complex, diagonal or curved relationship.\n",
        "    * The \"normal\" data (blue) forms a fairly dense, elongated cluster with a noticeable curve.\n",
        "    * The \"anomalies\" (orange) are scattered around this cluster, often forming a boundary or a less dense outer ring.\n",
        "\n",
        "    **Interpretation:** This indicates that both variables are important for distinguishing anomalies. The normal data exhibits a specific, non-linear correlation. The anomalous data deviates from this relationship, suggesting that outliers have unusual combinations of values for these two variables.\n",
        "\n",
        "**Overall Implications for Anomaly Detection:**\n",
        "\n",
        "* These plots show that the anomaly detection algorithm is likely identifying outliers based on deviations from the typical relationships between the variables.\n",
        "* The left plot suggests that one variable (X-axis) might be a stronger indicator of anomalies when it has higher values.\n",
        "* The right plot implies that a combination of both variables, specifically deviations from their curved relationship, is important for spotting outliers.\n",
        "\n",
        "To give you a more specific interpretation, I would need to know what the X and Y axes represent (the names of the variables)."
      ],
      "metadata": {
        "id": "2BiX3e7rR_YY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare / Contrast Random Forest and Isolation Forest\n",
        "Random Forest and Isolation Forest are both powerful ensemble learning algorithms that utilize decision trees. However, they are designed for fundamentally different tasks: **classification/regression** (Random Forest) and **anomaly detection** (Isolation Forest). Here's a comparison and contrast of the two:\n",
        "\n",
        "**Random Forest:**\n",
        "\n",
        "* **Primary Task:** Supervised learning for **classification** (predicting a categorical label) and **regression** (predicting a continuous value).\n",
        "* **Goal:** To build a robust and accurate predictive model by learning the relationships between features and the target variable from labeled data.\n",
        "* **Mechanism:**\n",
        "    * Constructs multiple decision trees (a \"forest\") on different subsets of the training data (bootstrapping).\n",
        "    * When building each tree, it considers only a random subset of features at each split.\n",
        "    * The final prediction is made by aggregating the predictions of all the trees (e.g., majority vote for classification, average for regression).\n",
        "* **Key Characteristics:**\n",
        "    * **Supervised Learning:** Requires labeled training data (features and corresponding target variable).\n",
        "    * **Prediction-Focused:** Aims to predict the value of a target variable for new, unseen data.\n",
        "    * **Ensemble of Deep Trees:** Individual trees can grow relatively deep to capture complex relationships.\n",
        "    * **Feature Importance:** Can provide insights into which features are most important for the prediction task.\n",
        "    * **Handles High Dimensionality:** Performs well with a large number of features.\n",
        "    * **Robust to Overfitting:** The ensemble nature and random feature selection help prevent overfitting to the training data.\n",
        "* **Output:** A predicted class label (classification) or a predicted numerical value (regression).\n",
        "\n",
        "**Isolation Forest:**\n",
        "\n",
        "* **Primary Task:** Unsupervised learning for **anomaly detection** (identifying data points that deviate significantly from the normal data).\n",
        "* **Goal:** To isolate anomalous data points more easily than normal data points.\n",
        "* **Mechanism:**\n",
        "    * Builds multiple isolation trees (iTrees) by randomly partitioning the data.\n",
        "    * For each tree, it randomly selects a feature and then randomly selects a split value within the range of that feature.\n",
        "    * Anomalies, being rare and different, tend to be isolated in fewer splits (shorter path length in the trees).\n",
        "    * A score is calculated for each data point based on its average path length across all the iTrees. Shorter average path lengths indicate a higher likelihood of being an anomaly.\n",
        "* **Key Characteristics:**\n",
        "    * **Unsupervised Learning:** Does not require labeled anomaly data. It learns the \"normal\" data distribution implicitly.\n",
        "    * **Isolation-Focused:** Explicitly tries to isolate anomalies rather than modeling the normal data.\n",
        "    * **Ensemble of Shallow Trees:** Individual trees are typically kept shallow to isolate anomalies quickly.\n",
        "    * **Anomaly Score:** Outputs an anomaly score for each data point, indicating its degree of abnormality.\n",
        "    * **Effective for High-Dimensional Data:** Can handle datasets with many features.\n",
        "    * **Efficient:** Generally has lower computational complexity compared to some other anomaly detection methods.\n",
        "* **Output:** An anomaly score (typically between -1 and 1, where values closer to -1 indicate a higher probability of being an anomaly).\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature           | Random Forest                       | Isolation Forest                     |\n",
        "| :---------------- | :---------------------------------- | :----------------------------------- |\n",
        "| **Primary Task** | Classification, Regression          | Anomaly Detection                    |\n",
        "| **Learning Type** | Supervised                          | Unsupervised                         |\n",
        "| **Goal** | Prediction of a target variable     | Isolation of anomalies               |\n",
        "| **Data Labels** | Requires labeled data               | Does not require labeled anomalies   |\n",
        "| **Tree Depth** | Typically deep                      | Typically shallow                    |\n",
        "| **Splitting** | Based on maximizing information gain (or minimizing error) w.r.t. target | Based on random feature and split value |\n",
        "| **Output** | Class label or numerical value      | Anomaly score                        |\n",
        "| **Anomaly Handling** | Can be used for anomaly detection if anomalies are labeled as a separate class, but not its primary purpose. | Specifically designed for anomaly detection. |\n",
        "\n",
        "**Similarities:**\n",
        "\n",
        "* **Ensemble Methods:** Both algorithms rely on creating an ensemble (forest) of decision trees.\n",
        "* **Tree-Based:** Both utilize decision tree structures as their base learners.\n",
        "* **Randomness:** Both introduce randomness in the tree building process (subsampling of data and/or features) to improve robustness and generalization.\n",
        "* **Handle High Dimensionality:** Both can effectively handle datasets with a large number of features.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "* **Random Forest learns the relationship between features and a known outcome.** It's like learning what a \"cat\" looks like from labeled images of cats and non-cats.\n",
        "* **Isolation Forest learns what \"normal\" data looks like by seeing how easily data points can be isolated.** Anomalies are those points that are easily separated because they are different from the majority. It's like finding the unusual-looking object in a collection of similar items without explicitly defining what \"unusual\" means beforehand.\n",
        "\n",
        "Understanding these fundamental differences is crucial for choosing the appropriate algorithm for a given machine learning task. If you have labeled data and want to predict a specific outcome, Random Forest is likely the better choice. If you have unlabeled data and want to identify rare and unusual instances, Isolation Forest is a more suitable algorithm."
      ],
      "metadata": {
        "id": "QgtJURtdnje5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNSW_NB15 Isolation Forest"
      ],
      "metadata": {
        "id": "zyapEr3u96yt"
      }
    }
  ]
}