{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/INFO5737/blob/main/Anomaly_Detection_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection\n",
        "\n",
        "Your Name"
      ],
      "metadata": {
        "id": "fXCvi2LefKtQ"
      },
      "id": "fXCvi2LefKtQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff INFO5737 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "ojyIDptwB7Iy"
      },
      "id": "ojyIDptwB7Iy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Anomaly Detection?\n",
        "\n",
        "Anomaly detection, also known as outlier detection, is the process of identifying data points, events, or observations that deviate significantly from the normal or expected patterns within a dataset. These anomalies are often rare and can indicate unusual behavior, errors, fraud, or other significant events.\n",
        "\n",
        "Here's a breakdown of the key aspects of anomaly detection:\n",
        "\n",
        "**Core Idea:** To find the \"unusual\" in data by comparing it to what is considered \"normal.\"\n",
        "\n",
        "**Why is it Important?**\n",
        "\n",
        "* **Identifying Problems:** Anomalies can signal critical issues like system failures, security breaches, or fraudulent activities.\n",
        "* **Preventing Losses:** Early detection of anomalies can help organizations mitigate potential financial losses or damages.\n",
        "* **Improving Efficiency:** Identifying and addressing anomalies in processes can lead to optimization and better performance.\n",
        "* **Ensuring Data Quality:** Anomalies can highlight errors or inconsistencies in data collection or processing.\n",
        "\n",
        "**Types of Anomalies:**\n",
        "\n",
        "Anomalies can be broadly categorized into three main types:\n",
        "\n",
        "* **Point Anomalies (Outliers):** Individual data points that are significantly different from the rest of the data. For example, a single unusually high transaction in a series of credit card purchases.\n",
        "* **Contextual Anomalies (Conditional Outliers):** Data points that are unusual within a specific context but might be normal in another. For example, a very high temperature reading for a city in winter would be anomalous, but the same reading in summer might be normal. These are often found in time-series data.\n",
        "* **Collective Anomalies (Group Anomalies):** A group of related data points that, when considered together, deviate from the expected pattern, even if individual points within the group might not be anomalous on their own. For example, a series of small, frequent transactions from the same account to multiple new accounts could indicate fraudulent activity, even if each transaction individually is not large enough to be flagged.\n",
        "\n",
        "**Approaches to Anomaly Detection:**\n",
        "\n",
        "Anomaly detection techniques can be broadly classified based on the availability of labeled data:\n",
        "\n",
        "* **Supervised Anomaly Detection:** Requires a labeled dataset containing both normal and anomalous instances. Classification algorithms are trained to distinguish between the two classes. This approach is often challenging because labeled anomaly data is typically scarce and imbalanced.\n",
        "* **Semi-Supervised Anomaly Detection:** Uses a dataset containing only normal instances to train a model. The model then identifies instances that do not fit the learned normal pattern as anomalies.\n",
        "* **Unsupervised Anomaly Detection:** Does not rely on any labeled data. It assumes that normal data points are far more frequent than anomalies and tries to find patterns or instances that deviate from the majority of the data. This is the most common approach as labeled anomaly data is often unavailable.\n",
        "\n",
        "**Common Unsupervised Anomaly Detection Algorithms (relevant to your interest in machine learning):**\n",
        "\n",
        "* **Statistical Methods:** These methods assume a distribution for the normal data and identify data points that fall outside a defined statistical range (e.g., based on mean and standard deviation, or percentiles).\n",
        "* **Clustering-Based Methods:** These methods group similar data points into clusters. Anomalies are often points that do not belong to any cluster or belong to small, isolated clusters. (e.g., K-Means)\n",
        "* **Density-Based Methods:** These methods estimate the density of data points. Anomalies are points in low-density regions. (e.g., Local Outlier Factor - LOF, DBSCAN)\n",
        "* **Distance-Based Methods:** These methods calculate the distance between data points. Anomalies are points that are far from their nearest neighbors. (e.g., k-Nearest Neighbors - kNN)\n",
        "* **Tree-Based Methods:** These methods build decision trees to isolate anomalies. Anomalies tend to be isolated in fewer splits. (e.g., Isolation Forest, Robust Random Cut Forest)\n",
        "* **Autoencoders (Neural Networks):** These neural networks are trained to reconstruct normal data. Anomalies, being different, result in higher reconstruction errors.\n",
        "* **Principal Component Analysis (PCA):** This dimensionality reduction technique can identify anomalies as data points that have a large reconstruction error when projected onto the principal components of the normal data.\n",
        "\n",
        "**Applications of Anomaly Detection:**\n",
        "\n",
        "Anomaly detection is used in a wide range of domains, including:\n",
        "\n",
        "* **Cybersecurity:** Intrusion detection, fraud detection, malware analysis, identifying unusual user behavior.\n",
        "* **Finance:** Credit card fraud detection, detecting unusual trading activities.\n",
        "* **Manufacturing:** Quality control, predictive maintenance of equipment.\n",
        "* **Healthcare:** Detecting abnormal patient conditions, identifying anomalies in medical images.\n",
        "* **Retail and E-commerce:** Identifying fraudulent transactions, detecting unusual purchasing patterns, supply chain optimization.\n",
        "* **IoT and Sensor Data:** Monitoring sensor readings for unusual patterns indicating failures or anomalies.\n",
        "* **Environmental Monitoring:** Detecting unusual climate patterns or pollution levels."
      ],
      "metadata": {
        "id": "CPjSAkqMfjCU"
      },
      "id": "CPjSAkqMfjCU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://www.sans.org/cyber-security-courses/applied-data-science-machine-learning/\n",
        "* https://github.com/SecurityNik/Data-Science-and-ML/blob/main/Beginning%20Machine%20and%20Deep%20Learning%20with%20Zeek%20logs/08%20-%20beginning%20Machine%20Learning%20Anomaly%20Detection%20-%20isolation%20forest%20and%20local%20outlier%20factor.ipynb"
      ],
      "metadata": {
        "id": "fqnsLQ_78pml"
      },
      "id": "fqnsLQ_78pml"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Traffic\n",
        "\n",
        "Network traffic refers to the **flow of data across a computer network** at any given point in time. Think of it like cars on a highway – the more cars moving, the higher the traffic. In computer networks, this \"traffic\" consists of **data packets**, which are small segments of data that contain the information being transmitted, along with addressing and control information.\n",
        "\n",
        "Here's a breakdown of what that means:\n",
        "\n",
        "* **Data:** This is the actual information being exchanged, such as emails, web pages, videos, files, voice conversations, and commands.\n",
        "* **Packets:** Data is broken down into these smaller units for efficient transmission across the network. Each packet has a header containing source and destination addresses, protocol information, and sequencing details, and a payload containing a piece of the actual data.\n",
        "* **Flow:** The movement of these packets from a source device to a destination device across the network infrastructure (cables, routers, switches, wireless signals, etc.).\n",
        "* **Time:** Network traffic is a dynamic measure, constantly changing based on network activity.\n",
        "\n",
        "**Key Aspects of Network Traffic:**\n",
        "\n",
        "* **Volume:** The amount of data being transferred, usually measured in bits per second (bps) or its multiples (Kbps, Mbps, Gbps).\n",
        "* **Rate:** The speed at which data is being transferred.\n",
        "* **Types:** Different kinds of data generate different types of traffic (e.g., web browsing (HTTP/HTTPS), email (SMTP/POP3/IMAP), file transfer (FTP/SFTP), streaming (RTP), etc.).\n",
        "* **Direction:** Traffic can be inbound (coming into your network) or outbound (leaving your network). It can also be north-south (between a client and a server) or east-west (within a data center).\n",
        "* **Protocols:** Network traffic relies on various communication protocols (like TCP/IP, UDP, DNS, HTTP) that define the rules for how data is formatted, transmitted, and received.\n",
        "\n",
        "**Why is Understanding Network Traffic Important?**\n",
        "\n",
        "* **Network Performance:** High traffic can lead to congestion, causing slowdowns and impacting user experience. Monitoring traffic helps in capacity planning and identifying bottlenecks.\n",
        "* **Network Security:** Analyzing traffic patterns is crucial for detecting anomalies that might indicate security threats like intrusions, malware activity, or data exfiltration.\n",
        "* **Troubleshooting:** Understanding traffic flow can help diagnose network connectivity issues and identify the source of problems.\n",
        "* **Quality of Service (QoS):** Network traffic analysis allows for prioritizing certain types of traffic (e.g., voice or video) to ensure a better user experience for real-time applications.\n",
        "* **Cost Management:** Monitoring bandwidth usage can help organizations manage internet costs and identify potential overages."
      ],
      "metadata": {
        "id": "EIzV5-9bhn0P"
      },
      "id": "EIzV5-9bhn0P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TCP/IP Review"
      ],
      "metadata": {
        "id": "4C9NjUZn_uvi"
      },
      "id": "4C9NjUZn_uvi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OSI Framework\n",
        "\n",
        "The Open Systems Interconnection (OSI) framework is a conceptual model created by the International Organization for Standardization (ISO) in 1984. It provides a standardized way to understand how different hardware and software components in a network communicate. The OSI model consists of seven distinct layers, each with specific responsibilities that ensure seamless data transmission across diverse network technologies.\n",
        "\n",
        "Here are the seven layers of the OSI framework, starting from the top:\n",
        "\n",
        "**1. Application Layer (Layer 7):**\n",
        "This is the layer that directly interacts with end-user applications. It provides the interface between the application and the network, enabling users to access network services. Examples of protocols at this layer include HTTP (for web browsing), SMTP (for email), and FTP (for file transfer).\n",
        "\n",
        "**2. Presentation Layer (Layer 6):**\n",
        "The Presentation Layer is responsible for the formatting, encryption, and compression of data. It ensures that the information sent by one application is readable by another application on a different system, handling differences in data representation.\n",
        "\n",
        "**3. Session Layer (Layer 5):**\n",
        "This layer manages the establishment, maintenance, and termination of connections (sessions) between applications. It handles authentication and authorization and ensures that data streams are properly managed and synchronized.\n",
        "\n",
        "**4. Transport Layer (Layer 4):**\n",
        "The Transport Layer provides reliable or unreliable end-to-end delivery of data between hosts. Key protocols at this layer include TCP (Transmission Control Protocol), which offers reliable, connection-oriented communication, and UDP (User Datagram Protocol), which provides faster, connectionless communication. This layer handles segmentation, reassembly, flow control, and error control.\n",
        "\n",
        "**5. Network Layer (Layer 3):**\n",
        "The Network Layer is responsible for addressing and routing data packets across networks. It determines the best path for data to travel from source to destination using logical addresses (IP addresses). Routers operate at this layer.\n",
        "\n",
        "**6. Data Link Layer (Layer 2):**\n",
        "This layer handles the transfer of data between two directly connected nodes on a local network segment. It deals with physical addressing (MAC addresses), framing of data, and error detection within the local link. Protocols like Ethernet and Wi-Fi operate at this layer.\n",
        "\n",
        "**7. Physical Layer (Layer 1):**\n",
        "The Physical Layer is the lowest layer and deals with the physical transmission of raw bit streams over a physical medium (cables, wireless signals, etc.). It defines the electrical, mechanical, procedural, and functional specifications for activating, maintaining, and deactivating the physical link.\n",
        "\n",
        "In the OSI model, data moves down through the layers on the sending device, with each layer adding its header information (encapsulation). Once the data reaches the receiving device, it moves up through the layers, with each layer interpreting and removing its corresponding header (decapsulation) until it reaches the Application Layer.\n",
        "\n",
        "The OSI framework is a crucial tool for understanding network communication, troubleshooting issues, and developing network technologies. By dividing the complex process of networking into these seven distinct layers, it provides a clear and standardized way to analyze and manage network operations."
      ],
      "metadata": {
        "id": "WMCH3wfDi9M_"
      },
      "id": "WMCH3wfDi9M_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TCP/IP 5 Layer Framework\n",
        "\n",
        "The TCP/IP 5-layer framework is a conceptual model that organizes the suite of communication protocols used for the internet and most modern networks into five distinct layers. This layering helps to understand the different levels of abstraction involved in network communication.\n",
        "\n",
        "Here's an overview of each layer:\n",
        "\n",
        "**1. Application Layer:**\n",
        "\n",
        "This is the top layer of the TCP/IP model, closest to the end-user and the applications they interact with. It provides the interface for these applications to utilize network services. The Application Layer defines the protocols that applications use to exchange data. Examples of protocols at this layer include:\n",
        "\n",
        "* **HTTP (Hypertext Transfer Protocol):** Used for web browsing.\n",
        "* **HTTPS (HTTP Secure):** Secure version of HTTP.\n",
        "* **FTP (File Transfer Protocol):** Used for transferring files.\n",
        "* **SFTP (SSH File Transfer Protocol):** Secure file transfer.\n",
        "* **SMTP (Simple Mail Transfer Protocol):** Used for sending email.\n",
        "* **POP3 (Post Office Protocol version 3):** Used for retrieving email.\n",
        "* **IMAP (Internet Message Access Protocol):** Another protocol for retrieving email.\n",
        "* **DNS (Domain Name System):** Translates domain names to IP addresses.\n",
        "* **SSH (Secure Shell):** Provides secure remote access to systems.\n",
        "* **Telnet:** Provides unsecure remote access to systems.\n",
        "* **SNMP (Simple Network Management Protocol):** Used for network management.\n",
        "\n",
        "The data unit at this layer is simply referred to as **data**.\n",
        "\n",
        "**2. Transport Layer:**\n",
        "\n",
        "The Transport Layer is responsible for providing end-to-end communication between applications running on different hosts. It manages the reliable or unreliable delivery of data segments. The key protocols at this layer are:\n",
        "\n",
        "* **TCP (Transmission Control Protocol):** Offers a connection-oriented, reliable, and ordered delivery of data. It handles segmentation of data from the Application Layer, reassembly at the destination, flow control to prevent overwhelming the receiver, and error recovery through retransmission.\n",
        "* **UDP (User Datagram Protocol):** Provides a connectionless and unreliable delivery of data. It offers faster communication with minimal overhead, suitable for applications where speed is more critical than guaranteed delivery (e.g., streaming, online gaming).\n",
        "\n",
        "The data units at this layer are **segments** (for TCP) and **datagrams** (for UDP).\n",
        "\n",
        "**3. Network Layer (or Internet Layer):**\n",
        "\n",
        "This layer is responsible for addressing and routing data packets across the network. Its primary function is to enable communication between different networks. The main protocols at this layer are:\n",
        "\n",
        "* **IP (Internet Protocol):** Provides logical addressing (IP addresses) for hosts and handles the routing of packets across networks. It is an unreliable, connectionless protocol, meaning it doesn't guarantee delivery or the order of packets.\n",
        "* **ICMP (Internet Control Message Protocol):** Used for error reporting and network diagnostic functions (e.g., ping, traceroute).\n",
        "* **ARP (Address Resolution Protocol):** Used to map IP addresses to physical MAC addresses within a local network segment.\n",
        "\n",
        "The data unit at this layer is called a **packet**.\n",
        "\n",
        "**4. Data Link Layer:**\n",
        "\n",
        "The Data Link Layer focuses on the reliable transfer of data between two directly connected nodes on a local network segment. It deals with the physical addressing of devices (MAC addresses), framing of data for transmission over the physical medium, and basic error detection within the local link. Protocols at this layer include:\n",
        "\n",
        "* **Ethernet (IEEE 802.3):** A common standard for wired local area networks.\n",
        "* **Wi-Fi (IEEE 802.11):** A common standard for wireless local area networks.\n",
        "* **PPP (Point-to-Point Protocol):** Used for establishing direct connections between two nodes, often used for dial-up or VPN connections.\n",
        "\n",
        "The data unit at this layer is called a **frame**.\n",
        "\n",
        "**5. Physical Layer:**\n",
        "\n",
        "This is the lowest layer of the TCP/IP model, concerned with the physical transmission of data bits over a communication medium. It defines the physical and electrical specifications of the network hardware and transmission media. This layer deals with:\n",
        "\n",
        "* **Physical cables and connectors.**\n",
        "* **Radio frequencies for wireless transmission.**\n",
        "* **Voltage levels and signaling rates.**\n",
        "* **Synchronization of bits.**\n",
        "\n",
        "This layer doesn't have high-level protocols in the same way as the upper layers. Instead, it defines the characteristics of the physical connection. The data unit at this layer is a **bit**.\n",
        "\n",
        "Data travels down the TCP/IP stack through a process called **encapsulation**, where each layer adds its own header (and sometimes trailer) to the data it receives from the layer above. At the destination, this process is reversed through **decapsulation**, with each layer removing its header as the data moves up the stack to the receiving application."
      ],
      "metadata": {
        "id": "x2oV-gayiQc9"
      },
      "id": "x2oV-gayiQc9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Packet and Encapsulation\n",
        "\n",
        "**What is a Packet?**\n",
        "\n",
        "At its core, a **packet** is a fundamental unit of data transmission on a packet-switched network, like the internet. When data needs to be sent from one device to another, it's broken down into these smaller, manageable chunks called packets. Each packet contains:\n",
        "\n",
        "* **Payload:** This is the actual data being transmitted – a piece of an email, a fragment of a web page, a segment of a video stream, etc.\n",
        "* **Header:** This is control information added to the beginning of the payload. The header contains crucial details that help network devices route the packet to its destination and ensure proper reassembly. Information in the header can include source and destination addresses (both logical and physical), protocol identifiers, sequence numbers, error-checking information, and flags.\n",
        "* **(Sometimes) Trailer:** Some protocols, particularly at lower layers, add a trailer to the end of the payload. Trailers often contain error detection mechanisms like Cyclic Redundancy Checks (CRC) to verify the integrity of the transmitted data.\n",
        "\n",
        "**Packet as a Protocol Data Unit (PDU)**\n",
        "\n",
        "A **Protocol Data Unit (PDU)** is a more formal term used to refer to the unit of data exchanged between peer layers in a networking model like TCP/IP or OSI. The term \"packet\" is most commonly associated with the Network Layer (Layer 3) in the TCP/IP model. However, each layer in the TCP/IP model has its own specific name for the PDU it handles:\n",
        "\n",
        "* **Application Layer (Layer 1): Data**\n",
        "* **Transport Layer (Layer 2): Segment (TCP), Datagram (UDP)**\n",
        "* **Network Layer (Layer 3): Packet**\n",
        "* **Data Link Layer (Layer 4): Frame**\n",
        "* **Physical Layer (Layer 5): Bit**\n",
        "\n",
        "So, while \"packet\" specifically refers to the PDU at the Network Layer, in a broader sense, people often use it informally to refer to the encapsulated data units being transmitted across the network.\n",
        "\n",
        "**Encapsulation**\n",
        "\n",
        "**Encapsulation** is the process by which each layer in the TCP/IP model adds its own header (and sometimes trailer) to the data it receives from the layer above. This creates the layered structure of the PDU as it travels down the protocol stack on the sending device.\n",
        "\n",
        "Here's how encapsulation works within the TCP/IP 5-layer model:\n",
        "\n",
        "1.  **Application Layer:** The application creates data that needs to be sent. This data is passed down to the Transport Layer.\n",
        "\n",
        "2.  **Transport Layer:**\n",
        "    * **TCP:** If TCP is used, the Application Layer data is segmented into smaller units. A TCP header is added to each segment. This header includes source and destination port numbers, sequence numbers, acknowledgment numbers, and control flags. The PDU at this layer is called a **segment**.\n",
        "    * **UDP:** If UDP is used, a UDP header is added to the Application Layer data. The UDP header is simpler, containing source and destination port numbers and a checksum. The PDU at this layer is called a **datagram**.\n",
        "\n",
        "3.  **Network Layer (Internet Layer):** The segment (from TCP) or datagram (from UDP) is passed down to the Network Layer. An IP header is added. This header contains source and destination IP addresses, routing information, time-to-live (TTL), and protocol information. The PDU at this layer is the **packet**.\n",
        "\n",
        "4.  **Data Link Layer:** The IP packet is passed down to the Data Link Layer. A Data Link Layer header is added, which typically includes the physical (MAC) addresses of the source and destination network interface cards (NICs) on the local network segment. Some Data Link Layer protocols also add a trailer for error detection (e.g., CRC). The PDU at this layer is the **frame**.\n",
        "\n",
        "5.  **Physical Layer:** The frame is passed down to the Physical Layer. This layer doesn't add headers or trailers in the same way. Instead, it converts the frame into a stream of bits and transmits these bits over the physical medium (e.g., copper wire, fiber optic cable, radio waves). The PDU at this layer is the **bit**.\n",
        "\n",
        "**Summary of PDUs and Encapsulation in the TCP/IP 5-Layer Model:**\n",
        "\n",
        "| Layer           | PDU Name     | Encapsulation Process                                                                | Key Header Information                                                                                                                                                              |\n",
        "| :-------------- | :----------- | :------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| Application     | Data         | Application-specific headers might be added by the application itself.                 | Varies depending on the application protocol (e.g., HTTP headers, SMTP commands).                                                                                                   |\n",
        "| Transport       | Segment/Datagram | TCP header or UDP header is added to the Application Layer data.                      | Source and destination port numbers, sequence numbers, acknowledgment numbers, flags (TCP); source and destination port numbers, checksum (UDP).                                   |\n",
        "| Network (IP)    | Packet       | IP header is added to the TCP segment or UDP datagram.                                 | Source and destination IP addresses, protocol type, time-to-live (TTL).                                                                                                               |\n",
        "| Data Link       | Frame        | Data Link Layer header (and sometimes trailer) is added to the IP packet.              | Source and destination MAC addresses, error detection information (e.g., CRC).                                                                                                        |\n",
        "| Physical        | Bit          | Frame is converted into a stream of bits for transmission over the physical medium. | Physical layer specifications (e.g., voltage levels, timing).                                                                                                                            |\n",
        "\n",
        "At the receiving device, this encapsulation process is reversed (decapsulation). Each layer reads its header information, performs its designated function, and then removes the header (and trailer) before passing the remaining data up to the next layer in the stack. This layered approach ensures that each part of the communication process is handled by the appropriate layer, contributing to the overall functionality and reliability of network communication."
      ],
      "metadata": {
        "id": "h6sSXB1KjdpU"
      },
      "id": "h6sSXB1KjdpU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Images\n",
        "\n",
        "* Packet Tracer - https://www.computerhope.com/jargon/p/packet.\n",
        "* Wireshark - https://www.wireshark.org/docs/wsug_html_chunked/ChUseMainWindowSection.html\n",
        "* Encapsulation - http://www.tcpipguide.com/free/t_DataEncapsulationProtocolDataUnitsPDUsandServiceDa-2.htm#google_vignette"
      ],
      "metadata": {
        "id": "PaF33-vwlzX-"
      },
      "id": "PaF33-vwlzX-"
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scapy -q"
      ],
      "metadata": {
        "id": "-Rudw4NboZWl"
      },
      "id": "-Rudw4NboZWl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TCP\n",
        "\n",
        "**TCP (Transmission Control Protocol)** is a connection-oriented, reliable, and ordered transport layer protocol. It establishes a connection before data transfer, ensures all data arrives at the destination correctly and in the order it was sent, and provides mechanisms for error recovery and flow control.\n",
        "\n",
        "**Applications that use TCP:**\n",
        "\n",
        "* **World Wide Web (HTTP/HTTPS):** For browsing websites.\n",
        "* **Email (SMTP, POP3, IMAP):** For sending and receiving emails.\n",
        "* **File Transfer (FTP, SFTP):** For transferring files between systems.\n",
        "* **Secure Shell (SSH):** For secure remote command-line access.\n",
        "* **Telnet:** For unsecure remote command-line access.\n",
        "* **Databases (e.g., MySQL, PostgreSQL):** For client-server communication."
      ],
      "metadata": {
        "id": "KjxQsOzhpZkU"
      },
      "id": "KjxQsOzhpZkU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "metadata": {
        "id": "9Tya7YlPpezN"
      },
      "id": "9Tya7YlPpezN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. `###[ IP ]###`**\n",
        "\n",
        "The IP header is crucial for **routing packets across networks**. It contains the necessary information to get a packet from its source to its destination. Here's an explanation of each field:\n",
        "\n",
        "* **`version = 4`**:\n",
        "    * Specifies the **IP version** being used. In this case, `4` indicates **IPv4**, which is the most widely used version of the Internet Protocol. The next generation is IPv6, which would have a `version` of `6`.\n",
        "\n",
        "* **`ihl = None`**:\n",
        "    * **Internet Header Length**. This field indicates the size of the IP header in **32-bit words**. A standard IPv4 header without options is 20 bytes (5 x 32-bit words).\n",
        "    * The `None` value here usually means that Scapy will automatically calculate the correct IHL when the packet is sent or when a full representation of the packet is needed. If there were IP options present, the IHL would be greater than 5.\n",
        "\n",
        "* **`tos = 0x0`**:\n",
        "    * **Type of Service** (now more accurately referred to as the **Differentiated Services Field - DSCP** and Explicit Congestion Notification - ECN).\n",
        "    * This byte is used to prioritize or classify packets, allowing for different treatment based on their importance. `0x0` indicates the default or best-effort service with no specific prioritization.\n",
        "\n",
        "* **`len = None`**:\n",
        "    * **Total Length**. This field specifies the total size of the IP packet in bytes, including both the header and the data (payload) it carries.\n",
        "    * `None` here means Scapy will calculate the total length based on the size of the IP header and the encapsulated data (in this case, the TCP segment).\n",
        "\n",
        "* **`id = 1`**:\n",
        "    * **Identification**. This is a 16-bit integer that uniquely identifies each IP packet sent by a host. If an IP packet is fragmented (broken into smaller pieces) to traverse a network with a smaller Maximum Transmission Unit (MTU), all fragments of the original packet will have the same `id`. This allows the receiving host to reassemble the fragments correctly.\n",
        "\n",
        "* **`flags = `**:\n",
        "    * **IP Flags**. These are control bits that relate to fragmentation. The flags are:\n",
        "        * **Bit 0:** Reserved (must be zero).\n",
        "        * **Bit 1 (DF - Don't Fragment):** If set, this flag indicates that the packet should not be fragmented. If a router encounters a link with an MTU smaller than the packet size, the router will drop the packet and typically send an ICMP \"Fragmentation Needed\" message back to the sender.\n",
        "        * **Bit 2 (MF - More Fragments):** If set, this flag indicates that this packet is a fragment and more fragments of the original packet will follow. The last fragment will have this flag set to zero.\n",
        "    * An empty value here likely means that no fragmentation-related flags are set (both DF and MF are 0).\n",
        "\n",
        "* **`frag = 0`**:\n",
        "    * **Fragment Offset**. This 13-bit field indicates the position of the fragment's data relative to the beginning of the original, unfragmented packet. The offset is measured in units of 8 bytes. A value of `0` indicates that this is either an unfragmented packet or the first fragment.\n",
        "\n",
        "* **`ttl = 64`**:\n",
        "    * **Time To Live**. This 8-bit field limits the number of hops (routers) an IP packet can traverse on the network. Each router that processes the packet decrements the TTL value by at least one. When the TTL reaches zero, the packet is discarded to prevent routing loops from causing packets to circulate indefinitely. The initial TTL value is set by the sending host (often 64 or 128).\n",
        "\n",
        "* **`proto = tcp`**:\n",
        "    * **Protocol**. This 8-bit field indicates the next-level protocol encapsulated within the IP packet's data payload. Common values include:\n",
        "        * `tcp` (or `6`): Transmission Control Protocol\n",
        "        * `udp` (or `17`): User Datagram Protocol\n",
        "        * `icmp` (or `1`): Internet Control Message Protocol\n",
        "        * In this case, `tcp` indicates that the data being carried by this IP packet is a TCP segment (from the Transport Layer).\n",
        "\n",
        "* **`chksum = None`**:\n",
        "    * **Header Checksum**. This 16-bit field contains a checksum value calculated over the IP header. It's used for basic error detection to ensure the integrity of the header during transmission. If a router detects an error in the header using the checksum, it will discard the packet.\n",
        "    * `None` here means Scapy will calculate the correct checksum before sending the packet.\n",
        "\n",
        "* **`src = 192.168.1.100`**:\n",
        "    * **Source IP Address**. This 32-bit field contains the IP address of the sender of the packet.\n",
        "\n",
        "* **`dst = 192.168.1.101`**:\n",
        "    * **Destination IP Address**. This 32-bit field contains the IP address of the intended recipient of the packet.\n",
        "\n",
        "* **`\\options\\`**:\n",
        "    * **IP Options**. This is a variable-length field that can contain optional features for the IP protocol, such as record route, timestamp, or security options. In this case, the backslash indicates that there are no IP options present.\n",
        "\n",
        "In summary, the IP header provides the fundamental addressing and control information necessary for routing a packet from its source to its destination across an IP network. It identifies the source and destination, specifies how the packet should be handled (e.g., fragmentation, priority), and indicates the type of data being carried in its payload.\n",
        "\n",
        "**2. `###[ TCP ]###` Section:**\n",
        "\n",
        "* **`sport = 12345`**: Source port number. The application initiating the connection is using port 12345.\n",
        "* **`dport = 80`**: Destination port number. The service being requested (likely HTTP) is listening on port 80.\n",
        "* **`seq = 0`**: Sequence number. This is the initial sequence number (ISN) for the TCP connection establishment. It's used to track the order of data segments.\n",
        "* **`ack = 0`**: Acknowledgment number. In the SYN packet, the acknowledgment number is usually 0 as no data has been received yet. It will be used in subsequent packets to acknowledge received data.\n",
        "* **`dataofs = None`**: Data offset (number of 32-bit words in the TCP header). `None` means Scapy will calculate this.\n",
        "* **`reserved = 0`**: Reserved bits, which are usually set to 0.\n",
        "* **`flags = S`**: TCP flags. 'S' stands for SYN (Synchronize). This flag is set in the first packet of the TCP three-way handshake to initiate a connection. Other flags include 'A' (ACKnowledgment), 'F' (FINish), 'R' (ReSeT), 'P' (PUSH), and 'U' (URGent).\n",
        "* **`window = 8192`**: Window size. This indicates the amount of data the sender is willing to receive without an acknowledgment.\n",
        "* **`chksum = None`**: Checksum. Used for error detection of the TCP header and data. `None` means Scapy will calculate this.\n",
        "* **`urgptr = 0`**: Urgent pointer. This field is used when the URG flag is set to indicate the offset of urgent data within the segment.\n",
        "* **`\\options\\`**: TCP options can be included here (e.g., Maximum Segment Size - MSS). It's empty in this basic example.\n",
        "\n",
        "In summary, a TCP packet, as represented by Scapy, will have an IP layer for addressing and routing, and a TCP layer containing header fields specific to TCP's connection-oriented and reliable nature, including sequence numbers, acknowledgment numbers, flags for connection control, and window size for flow control. The raw data payload would follow the TCP header."
      ],
      "metadata": {
        "id": "Vjxr6wJBpnr-"
      },
      "id": "Vjxr6wJBpnr-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UDP\n",
        "\n",
        "**UDP (User Datagram Protocol)** is a connectionless and unreliable transport layer protocol. It sends data packets (datagrams) without establishing a connection beforehand and doesn't guarantee delivery, order, or error checking. This makes it faster and with lower overhead than TCP.\n",
        "\n",
        "**Applications that use UDP:**\n",
        "\n",
        "* **Domain Name System (DNS):** For looking up IP addresses from domain names.\n",
        "* **Streaming Media (e.g., video conferencing, online gaming):** Where speed is often prioritized over guaranteed delivery.\n",
        "* **Voice over IP (VoIP):** For real-time voice communication.\n",
        "* **Online Gaming:** For fast-paced data exchange where occasional packet loss is acceptable.\n",
        "* **Simple Network Management Protocol (SNMP):** For network management tasks.\n",
        "* **Trivial File Transfer Protocol (TFTP):** A simpler file transfer protocol."
      ],
      "metadata": {
        "id": "Pq4oXZrUpV1E"
      },
      "id": "Pq4oXZrUpV1E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "metadata": {
        "id": "g4wpufDYoT3Q"
      },
      "id": "g4wpufDYoT3Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. `###[ IP ]###`**\n",
        "\n",
        "This section defines the **Internet Protocol (IP) layer** of the packet. It contains the header fields for IPv4:\n",
        "\n",
        "* **`version = 4`**: Indicates that this is an IPv4 packet.\n",
        "* **`ihl = None`**: Internet Header Length. This field specifies the size of the IP header in 32-bit words. `None` usually means Scapy will calculate this automatically when the packet is sent or displayed in full detail.\n",
        "* **`tos = 0x0`**: Type of Service (now Differentiated Services Field - DSCP). This field is used to prioritize or classify packets. `0x0` indicates no specific prioritization.\n",
        "* **`len = None`**: Total length of the IP packet (header + data). `None` means Scapy will calculate this.\n",
        "* **`id = 1`**: Identification field. This is a sequence number that helps in reassembling fragmented IP packets at the destination.\n",
        "* **`flags = `**: IP flags. These flags control fragmentation. The empty value likely means no flags are set.\n",
        "* **`frag = 0`**: Fragment offset. If the packet is fragmented, this field indicates the position of the fragment within the original packet. `0` means it's either not fragmented or it's the first fragment.\n",
        "* **`ttl = 64`**: Time To Live. This field limits the number of hops a packet can take through routers to prevent routing loops. It's initialized to a value (often 64) and decremented by each router.\n",
        "* **`proto = 17`**: Protocol. This field indicates the next-level protocol encapsulated within the IP packet. `17` corresponds to **UDP (User Datagram Protocol)**.\n",
        "* **`chksum = None`**: Header checksum. This field is used for error detection of the IP header. `None` means Scapy will calculate this.\n",
        "* **`src = 192.168.1.100`**: Source IP address of the packet.\n",
        "* **`dst = 192.168.1.102`**: Destination IP address of the packet.\n",
        "* **`\\options \\`**: This section would contain any IP options, but it's empty here.\n",
        "\n",
        "**2. `###[ UDP ]###`**\n",
        "\n",
        "This section defines the **User Datagram Protocol (UDP) layer**, which is encapsulated within the IP packet (as indicated by `proto = 17` in the IP header).\n",
        "\n",
        "* **`sport = 5000`**: Source port number. The application on the source host that sent this UDP datagram is using port 5000.\n",
        "* **`dport = 53`**: Destination port number. The application on the destination host that should receive this UDP datagram is listening on port 53, which is the standard port for **DNS (Domain Name System)** queries.\n",
        "* **`len = None`**: Length of the UDP datagram (header + data). `None` means Scapy will calculate this.\n",
        "* **`chksum = None`**: Checksum. This field is used for error detection of the UDP header and data. `None` means Scapy will calculate this.\n",
        "\n",
        "**3. `###[ Raw ]###`**\n",
        "\n",
        "This section represents the **raw payload** of the UDP datagram. It's the actual data being transmitted by the application.\n",
        "\n",
        "* **`load = b'This is a simulated UDP packet.'`**: This is the raw byte string payload. The `b'` prefix indicates a byte literal in Python. This is the data that the DNS application (listening on port 53 at the destination IP) would receive.\n",
        "\n",
        "**4. `'IP / UDP 192.168.1.100:5000 > 192.168.1.102:53 / Raw'`**\n",
        "\n",
        "This is a concise **summary** of the created packet, also provided by Scapy. It shows the layering and key information:\n",
        "\n",
        "* **`IP`**: Indicates the outermost layer is IP.\n",
        "* **`/`**: Separator indicating encapsulation.\n",
        "* **`UDP`**: Indicates the next layer encapsulated within IP is UDP.\n",
        "* **`192.168.1.100:5000`**: Source IP address and source port number.\n",
        "* **`>`**: Indicates the direction of the packet (from source to destination).\n",
        "* **`192.168.1.102:53`**: Destination IP address and destination port number.\n",
        "* **`/`**: Separator.\n",
        "* **`Raw`**: Indicates the raw payload being carried by the UDP datagram.\n",
        "\n",
        "**In essence, this Scapy output describes a simulated network packet that:**\n",
        "\n",
        "* Originates from IP address `192.168.1.100` and UDP port `5000`.\n",
        "* Is destined for IP address `192.168.1.102` and UDP port `53` (likely a DNS server).\n",
        "* Contains a raw payload of the text \"This is a simulated UDP packet.\"\n",
        "\n",
        "Scapy allows you to construct and manipulate packets at this level, making it a powerful tool for network analysis, testing, and security exploration. When you would send this packet using Scapy's `send()` function, the library would typically fill in the `None` values (like `ihl`, `len`, and `chksum`) before transmission."
      ],
      "metadata": {
        "id": "apc5wSaepI-k"
      },
      "id": "apc5wSaepI-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TCP/IP Network Anomalies\n",
        "\n",
        "TCP/IP network anomalies are unusual patterns or behaviors in network traffic that deviate significantly from the expected or normal baseline of communication using the TCP/IP protocol suite. These anomalies can be caused by various factors, including:\n",
        "\n",
        "* **Security Threats:** Intrusions, malware activity, denial-of-service (DoS) attacks, port scanning, and unauthorized access attempts.\n",
        "* **Network Issues:** Equipment malfunction, misconfigurations, congestion, routing problems, and broadcast storms.\n",
        "* **Policy Violations:** Unauthorized use of bandwidth, forbidden protocols, or access to restricted resources.\n",
        "* **Operational Errors:** Mistakes in configuration or maintenance leading to unusual traffic patterns.\n",
        "\n",
        "**Here are some examples of TCP/IP network anomalies:**\n",
        "\n",
        "**Traffic Volume Anomalies:**\n",
        "\n",
        "* **Sudden Spike in Traffic:** An unexpected and significant increase in network traffic to a specific host or across the entire network, potentially indicating a DoS attack or a compromised host participating in a botnet.\n",
        "* **Unusual Drop in Traffic:** A sudden and significant decrease in traffic, which could indicate a network outage, a failing server, or a targeted attack disrupting communication.\n",
        "* **Excessive Bandwidth Usage:** A host consuming far more bandwidth than its typical usage, potentially due to malware uploading data or a user engaging in unauthorized file sharing.\n",
        "\n",
        "**Packet Characteristic Anomalies:**\n",
        "\n",
        "* **Unusual Packet Sizes:** Packets that are significantly larger or smaller than the typical packet size for a particular type of communication. This could indicate fragmentation attacks or tunneling.\n",
        "* **Irregular Packet Rate:** A sudden increase or decrease in the number of packets being transmitted, which might signal a DoS attack or network congestion.\n",
        "* **Abnormal TCP Flags:** Unexpected or illogical combinations of TCP flags (SYN, ACK, FIN, RST, URG, PSH) in packets. For example, multiple SYN packets without corresponding ACK responses (SYN flood attack) or RST packets for established connections without a clear reason.\n",
        "* **Out-of-Order Packets:** An unusually high number of TCP packets arriving out of sequence, potentially indicating network issues or an attacker trying to reorder packets.\n",
        "* **Retransmission Anomalies:** An excessive number of retransmitted TCP segments, suggesting network congestion or unreliable connections.\n",
        "\n",
        "**Connection and Flow Anomalies:**\n",
        "\n",
        "* **High Number of Connections:** A host establishing an unusually large number of connections to different destinations, which could be a sign of malware spreading or a port scan.\n",
        "* **Connections to Unusual Ports:** Communication attempts to ports that are not typically used by standard services, potentially indicating malware communication or reconnaissance activities.\n",
        "* **Connections from Unusual Source IPs:** Traffic originating from IP addresses that are not normally associated with the internal network or trusted external partners.\n",
        "* **Long-Duration Connections with Little Data Transfer:** Connections that remain open for extended periods without significant data exchange, which could be indicative of idle botnet connections.\n",
        "* **Failed Connection Attempts:** A high number of unsuccessful connection attempts to a specific host or service, potentially indicating a DoS attack or a misconfigured service.\n",
        "\n",
        "**Protocol-Specific Anomalies:**\n",
        "\n",
        "* **HTTP Anomalies:** Unusual user-agent strings, abnormal request methods, or excessive requests for specific resources.\n",
        "* **DNS Anomalies:** Sudden spikes in DNS queries, queries for unusual domain names, or a high number of failed DNS resolutions.\n",
        "* **SMTP Anomalies:** Large volumes of outgoing emails, emails sent to unusual recipients, or emails originating from compromised internal hosts.\n",
        "\n",
        "Detecting these TCP/IP network anomalies is crucial for maintaining network security, performance, and reliability. Machine learning techniques, as you are teaching, can be very effective in identifying these deviations from normal behavior by learning patterns in network traffic data."
      ],
      "metadata": {
        "id": "PdG-7xqB_xwA"
      },
      "id": "PdG-7xqB_xwA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest vs Isolation Forest\n",
        "\n",
        "**Random Forest** is a **supervised** algorithm used for **classification and regression**. It learns relationships between features and a target variable from labeled data to make predictions. It builds an ensemble of deep decision trees.\n",
        "\n",
        "**Isolation Forest** is an **unsupervised** algorithm used for **anomaly detection**. It identifies outliers by isolating them in shallow decision trees built by random partitioning. It doesn't require labeled anomaly data.\n",
        "\n",
        "**Key Difference:** Random Forest predicts a known outcome, while Isolation Forest identifies the unusual without prior knowledge of what's anomalous."
      ],
      "metadata": {
        "id": "KVtLtthT907g"
      },
      "id": "KVtLtthT907g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised learning** uses **labeled data** (input features paired with correct output targets) to train models for prediction or classification. The model learns the mapping between inputs and outputs.\n",
        "\n",
        "**Unsupervised learning** uses **unlabeled data** to find hidden patterns, structures, or groupings within the data itself, without any predefined target variables."
      ],
      "metadata": {
        "id": "JFtsFVglrgNl"
      },
      "id": "JFtsFVglrgNl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Decision Tree\n",
        "\n",
        "A **decision tree** is a tree-like supervised learning model used for both classification and regression. It makes decisions by recursively splitting the data based on the values of input features, aiming to create homogeneous subsets with respect to the target variable. Each internal node represents a feature test, each branch represents the outcome of the test, and each leaf node represents a prediction (class label or numerical value)."
      ],
      "metadata": {
        "id": "okItANjDn4Jm"
      },
      "id": "okItANjDn4Jm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                      Chillin with some grillin?\n",
        "                                         Is it cloudy today?\n",
        "                                            (Root Node)\n",
        "                                                /\\\n",
        "                                           yes /  \\ no\n",
        "                                              /    \\\n",
        "                                         chance    grillin\n",
        "                                        of rain?      \n",
        "                                            /\\\n",
        "                                       yes /  \\ no\n",
        "                                          /    \\\n",
        "                                  no grillin     grillin"
      ],
      "metadata": {
        "id": "VTDfoHRYvmgy"
      },
      "id": "VTDfoHRYvmgy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy\n",
        "\n",
        "$H(S) = -p_1 \\log_2(p_1) - p_0 \\log_2(p_0)$\n",
        "\n",
        "The entropy of a set S, denoted as H(S), is calculated by taking the negative of the probability of the first class (p₁) multiplied by the base-2 logarithm of that probability, and then subtracting the probability of the second class (p₀) multiplied by the base-2 logarithm of that probability."
      ],
      "metadata": {
        "id": "HSIt1tmJCSoN"
      },
      "id": "HSIt1tmJCSoN"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # We'll need numpy for the logarithm base 2\n",
        "\n",
        "def entropy(target_col):\n",
        "    \"\"\"Calculates the entropy of a target variable with binary classes.\"\"\"\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    probabilities = counts / len(target_col) # Calculates p1 and p0 (probabilities of each class)\n",
        "    entropy_value = 0\n",
        "    for prob in probabilities:\n",
        "        if prob > 0:  # Avoid log(0) error\n",
        "            entropy_value -= prob * np.log2(prob) # Calculates -p * log2(p) for each class\n",
        "\n",
        "    return entropy_value # Returns H(S)"
      ],
      "metadata": {
        "id": "vJyu4w3wCG5_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vJyu4w3wCG5_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Gain\n",
        "\n",
        "$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$\n",
        "\n",
        "The information gain of splitting a dataset S on an attribute A, denoted as IG(S, A), is determined by taking the initial entropy of the dataset S, represented as H(S), and subtracting the sum of the entropies of each subset Sv created by splitting on the different values (v) of attribute A. Each of these subset entropies, H(Sv), is weighted by the proportion of the number of elements in that subset (|Sv|) relative to the total number of elements in the original dataset (|S|)."
      ],
      "metadata": {
        "id": "bPE0s4iiCfAb"
      },
      "id": "bPE0s4iiCfAb"
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(data, split_attribute, target_attribute):\n",
        "    \"\"\"Calculates the information gain of splitting on an attribute.\"\"\"\n",
        "    # Total entropy of the target variable\n",
        "    total_entropy = entropy(data[target_attribute]) # H(S)\n",
        "\n",
        "    # Unique values in the split attribute\n",
        "    values = data[split_attribute].unique()\n",
        "    weighted_entropy = 0\n",
        "\n",
        "    for value in values:\n",
        "        subset = data[data[split_attribute] == value]\n",
        "        subset_size = len(subset)\n",
        "        if subset_size > 0:\n",
        "            subset_entropy = entropy(subset[target_attribute]) # H(Sv)\n",
        "            weighted_entropy += (subset_size / len(data)) * subset_entropy # (|Sv| / |S|) * H(Sv)\n",
        "\n",
        "    # Information Gain\n",
        "    gain = total_entropy - weighted_entropy # IG(S, A) = H(S) - Σ (|Sv| / |S|) * H(Sv)\n",
        "    return gain"
      ],
      "metadata": {
        "id": "Xc5Mlb3hCQcG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Xc5Mlb3hCQcG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gini Impurity\n",
        "\n",
        "$Gini(S) = 1 - (p_1^2 + p_0^2)$\n",
        "\n",
        "The Gini impurity of a set S, denoted as Gini(S), is calculated by taking one and subtracting the sum of the squared probabilities of each class. For a binary classification problem, this means subtracting the square of the probability of the first class (p₁) and the square of the probability of the second class (p₀) from one."
      ],
      "metadata": {
        "id": "uZC9mekzCwr1"
      },
      "id": "uZC9mekzCwr1"
    },
    {
      "cell_type": "code",
      "source": [
        "def gini(target_col):\n",
        "    \"\"\"Calculates the Gini impurity of a target variable with binary classes.\"\"\"\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    probabilities = counts / len(target_col) # Calculates p1 and p0 (probabilities of each class)\n",
        "    gini_value = 1 - np.sum(probabilities**2) # Calculates 1 - (p1^2 + p0^2)\n",
        "    return gini_value # Returns Gini(S)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hxmIsLgSCwr2"
      },
      "id": "hxmIsLgSCwr2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gini Gain (Reduction in Gini Impurity)\n",
        "\n",
        "$Gini\\_Gain(S, A) = Gini(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Gini(S_v)$\n",
        "\n",
        "The Gini gain achieved by splitting a dataset S on an attribute A, denoted as Gini\\_Gain(S, A), is calculated by taking the initial Gini impurity of the dataset S, represented as Gini(S), and subtracting the weighted sum of the Gini impurities of each subset Sv created by splitting on the different values (v) of attribute A. Each subset's Gini impurity, Gini(Sv), is weighted by the proportion of the number of elements in that subset (|Sv|) relative to the total number of elements in the original dataset (|S|)."
      ],
      "metadata": {
        "id": "f7Z-QMMhDHk9"
      },
      "id": "f7Z-QMMhDHk9"
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_gain(data, split_attribute, target_attribute):\n",
        "    \"\"\"Calculates the Gini gain of splitting on an attribute.\"\"\"\n",
        "    # Total Gini impurity of the target variable\n",
        "    total_gini = gini(data[target_attribute]) # Gini(S)\n",
        "\n",
        "    # Unique values in the split attribute\n",
        "    values = data[split_attribute].unique()\n",
        "    weighted_gini = 0\n",
        "\n",
        "    for value in values:\n",
        "        subset = data[data[split_attribute] == value]\n",
        "        subset_size = len(subset)\n",
        "        if subset_size > 0:\n",
        "            subset_gini = gini(subset[target_attribute]) # Gini(Sv)\n",
        "            weighted_gini += (subset_size / len(data)) * subset_gini # (|Sv| / |S|) * Gini(Sv)\n",
        "\n",
        "    # Gini Gain\n",
        "    gain = total_gini - weighted_gini # Gini_Gain(S, A) = Gini(S) - Σ (|Sv| / |S|) * Gini(Sv)\n",
        "    return gain"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-6-SYI3TDHk-"
      },
      "id": "-6-SYI3TDHk-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "**Our Goal:** To understand how Information Gain, Gini Impurity, and Entropy guide the splitting of a decision tree when the target variable has three distinct classes.\n",
        "\n",
        "**Scenario:** Imagine we have a dataset about different types of fruits based on their color, size, shape, and texture, and our target variable is the fruit type (Apple, Banana, Orange)."
      ],
      "metadata": {
        "id": "EJaMs7jLxF-g"
      },
      "id": "EJaMs7jLxF-g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting up the Environment and Initial Data"
      ],
      "metadata": {
        "id": "_VfAMuWxBVn9"
      },
      "id": "_VfAMuWxBVn9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "data = {\n",
        "    'Fur': np.random.choice(['Furry', 'Not Furry'], size=12),\n",
        "    'Legs': np.random.choice([4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2], size=12), # Bias towards 4 legs for cats/dogs\n",
        "    'Sound': np.random.choice(['Meow', 'Woof', 'Hiss', 'Meow', 'Woof', 'Hiss', 'Meow', 'Woof', 'Hiss', 'Meow', 'Woof', 'Hiss'], size=12),\n",
        "    'Tail': np.random.choice(['Long', 'Short', 'None', 'Long', 'Short', 'None', 'Long', 'Short', 'None', 'Long', 'Short', 'None'], size=12),\n",
        "    'Animal': ['Cat', 'Dog', 'Turtle', 'Cat', 'Dog', 'Turtle', 'Cat', 'Dog', 'Turtle', 'Cat', 'Dog', 'Turtle']\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "nIfL1K8kxF-i"
      },
      "id": "nIfL1K8kxF-i"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "metadata": {
        "id": "wk6n-wyyBwmR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wk6n-wyyBwmR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Entropy\n",
        "\n",
        "**What is Entropy?**\n",
        "\n",
        "Entropy is a measure of the impurity, disorder, or randomness in a set of data. In the context of information theory, it quantifies the average amount of information needed to describe the outcome of a random event. In machine learning, particularly with decision trees, entropy is used to measure the impurity of a dataset with respect to the target variable.\n",
        "\n",
        "A pure dataset (where all data points belong to the same class) has an entropy of 0. A dataset with an equal distribution of classes has maximum entropy.\n",
        "\n",
        "**Why Use Entropy with the Target Variable?**\n",
        "\n",
        "In decision trees, we use entropy to determine the \"best\" way to split the data at each node. The goal is to create splits that result in subsets of data that are as pure as possible (with respect to the target variable).\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1.  **Initial Entropy:** We first calculate the entropy of the entire dataset's target variable. This gives us a baseline measure of the impurity before any splits are made.\n",
        "\n",
        "2.  **Splitting and Information Gain:**\n",
        "    * We then consider different features and potential split points within those features.\n",
        "    * For each potential split, we calculate the entropy of the resulting subsets (after the split).\n",
        "    * We compare the entropy of the original dataset to the weighted average of the entropies of the subsets. The reduction in entropy achieved by the split is called **information gain**.\n",
        "\n",
        "3.  **Best Split Selection:** The feature and split point that yield the highest information gain (i.e., the greatest reduction in entropy) are chosen for that node in the decision tree.\n",
        "\n",
        "4.  **Recursion:** This process is repeated recursively for each subset, building the tree until a stopping criterion is met (e.g., a subset is pure, or a maximum tree depth is reached).\n",
        "\n",
        "**In essence, we use entropy to guide the tree construction process, favoring splits that create more homogeneous subsets, making the tree more effective at classifying or predicting the target variable.**\n",
        "\n",
        "**Entropy Formula:**\n",
        "\n",
        "The formula for entropy (H) for a set S is:\n",
        "\n",
        "$$H(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* \\(H(S)\\) is the entropy of the dataset S.\n",
        "* \\(c\\) is the number of distinct classes in the target variable.\n",
        "* \\(p_i\\) is the proportion (probability) of data points belonging to class i in the dataset S.\n",
        "\n",
        "The logarithm is base 2 because we often deal with bits of information. By minimizing entropy, we maximize the information gained by each split, leading to a more efficient and accurate decision tree.\n"
      ],
      "metadata": {
        "id": "j8Yb2AGnBgSX"
      },
      "id": "j8Yb2AGnBgSX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rySLWAo2xF-k"
      },
      "id": "rySLWAo2xF-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Interpret the entropy value in the context of three classes. A higher value still indicates more impurity or a more even distribution of the fruit types.)**"
      ],
      "metadata": {
        "id": "wL3mBsfQxF-k"
      },
      "id": "wL3mBsfQxF-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Information Gain\n",
        "\n",
        "Okay, let's explain information gain and how it's used to select the best feature for the first split in a decision tree.\n",
        "\n",
        "**What is Information Gain?**\n",
        "\n",
        "Information gain measures the reduction in entropy achieved by partitioning the dataset based on a particular feature. In simpler terms, it tells us how much \"cleaner\" or more organized the data becomes after we split it according to that feature. A higher information gain indicates a more effective split, as it means the resulting subsets are more homogeneous with respect to the target variable.\n",
        "\n",
        "**How We Use Information Gain to Find the Best Feature for the First Split**\n",
        "\n",
        "1.  **Calculate Initial Entropy:** First, as you've already done, we calculate the entropy of the target variable for the entire dataset. This is the entropy before any splitting is performed.\n",
        "\n",
        "2.  **Calculate Entropy for Each Feature:**\n",
        "    * For each feature in the dataset:\n",
        "        * We determine the unique values of that feature.\n",
        "        * For each unique value of the feature, we create a subset of the data containing only the rows where the feature has that value.\n",
        "        * We calculate the entropy of the target variable for each of these subsets.\n",
        "\n",
        "3.  **Calculate Weighted Average Entropy for Each Feature:**\n",
        "    * For each feature, we calculate the weighted average entropy of its subsets. The weight for each subset's entropy is the proportion of data points that belong to that subset.\n",
        "\n",
        "4.  **Calculate Information Gain for Each Feature:**\n",
        "    * For each feature, we calculate the information gain by subtracting the weighted average entropy of that feature's subsets from the initial entropy of the entire dataset.\n",
        "\n",
        "5.  **Select the Feature with the Highest Information Gain:**\n",
        "    * The feature with the highest information gain is chosen as the feature for the first split. This is because it provides the greatest reduction in impurity and thus the most informative split.\n",
        "\n",
        "**Information Gain Equation:**\n",
        "\n",
        "The formula for information gain (IG) is:\n",
        "\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* \\(IG(S, A)\\) is the information gain of splitting the dataset \\(S\\) on feature \\(A\\).\n",
        "* \\(H(S)\\) is the entropy of the original dataset \\(S\\).\n",
        "* \\(Values(A)\\) is the set of all possible values of feature \\(A\\).\n",
        "* \\(|S_v|\\) is the number of data points in the subset \\(S_v\\) where feature \\(A\\) has value \\(v\\).\n",
        "* \\(|S|\\) is the total number of data points in the dataset \\(S\\).\n",
        "* \\(H(S_v)\\) is the entropy of the subset \\(S_v\\).\n",
        "\n",
        "1.  **`information_gain(data, split_feature, target_feature)` function:**\n",
        "    * Takes the DataFrame (`data`), the feature to split on (`split_feature`), and the target feature (`target_feature`) as input.\n",
        "    * Calculates the `total_entropy` (entropy of the target variable in the whole dataset).\n",
        "    * Finds the unique values of the `split_feature` and their counts.\n",
        "    * Iterates through each unique value of the `split_feature`:\n",
        "        * Creates a `subset` of the data where the `split_feature` equals the current value.\n",
        "        * Calculates the probability (`subset_prob`) of that subset.\n",
        "        * Calculates the entropy of the `target_feature` in the `subset`.\n",
        "        * Adds the weighted entropy of the subset to `weighted_entropy`.\n",
        "    * Returns the information gain by subtracting the `weighted_entropy` from the `total_entropy`.\n",
        "\n",
        "2.  **Calculate Information Gain for Each Feature:**\n",
        "    * We iterate through all the features in the DataFrame (except the target variable 'Animal').\n",
        "    * We call the `information_gain` function for each feature to calculate its information gain.\n",
        "    * We store the information gains in a dictionary `information_gains`.\n",
        "    * We print the information gain for each feature.\n",
        "\n",
        "3.  **Find the Feature with the Highest Information Gain:**\n",
        "    * We use the `max()` function with the `key` argument to find the feature with the maximum information gain from the `information_gains` dictionary.\n",
        "    * We print the name of the `best_feature`.\n",
        "\n",
        "This code will calculate the information gain for each feature and tell you which feature would be the best choice for the first split in a decision tree based on maximizing information gain."
      ],
      "metadata": {
        "id": "8sjyoM5JEBrM"
      },
      "id": "8sjyoM5JEBrM"
    },
    {
      "cell_type": "code",
      "source": [
        "# information_gains = {}\n",
        "# for feature in df.columns:\n",
        "#     if feature != 'Animal':  # Don't calculate gain for the target variable itself\n",
        "#         information_gains[feature] = information_gain(df, feature, 'Animal')\n",
        "#         print(f\"Information Gain for '{feature}': {information_gains[feature]:.4f}\")\n",
        "\n",
        "# # Find the Feature with the Highest Information Gain\n",
        "# best_feature = max(information_gains, key=information_gains.get)\n",
        "# print(f\"\\nBest feature to split on: '{best_feature}'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KGcy7zkAxF-l"
      },
      "id": "KGcy7zkAxF-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Which feature seems most promising for the first split based on these values.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We iterate through each of our features.\n",
        "* For each feature, we use our `information_gain` function (which we defined earlier and works for multi-class targets) to calculate how much the entropy of 'Fruit' is reduced by splitting on that feature.\n",
        "* The output shows the Information Gain for each potential first split. The feature with the highest Information Gain would be chosen as the root node."
      ],
      "metadata": {
        "id": "XvdXES0FxF-l"
      },
      "id": "XvdXES0FxF-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Recursion"
      ],
      "metadata": {
        "id": "qbj8PeqKpBKn"
      },
      "id": "qbj8PeqKpBKn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three common methods for generating the Fibonacci sequence in Python are using a loop (iteration), recursion, and memoization (dynamic programming).\n",
        "\n",
        "**1. Iteration:**\n",
        "\n",
        "   - **Approach:** This method uses a `for` or `while` loop to compute the Fibonacci numbers sequentially.\n",
        "   - **Process:** Initialize the first two Fibonacci numbers (0 and 1), and then iteratively calculate the next number by adding the previous two.\n",
        "   - **Example:**"
      ],
      "metadata": {
        "id": "4jaq3LpDmn3G"
      },
      "id": "4jaq3LpDmn3G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "TwVDWNvrmn3H"
      },
      "id": "TwVDWNvrmn3H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Recursion:**\n",
        "\n",
        "   - **Approach:** This method defines a function that calls itself to compute the Fibonacci numbers.\n",
        "   - **Process:** Define base cases for the first two Fibonacci numbers (0 and 1), and then recursively define the function for the next number by adding the previous two.\n",
        "   - **Example:**"
      ],
      "metadata": {
        "id": "X0KgRCHkmn3I"
      },
      "id": "X0KgRCHkmn3I"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9DKpGN3pmn3I"
      },
      "id": "9DKpGN3pmn3I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Memoization:**\n",
        "\n",
        "Memoization is an optimization technique that speeds up code by storing the results of expensive function calls and reusing them when the same inputs occur again. It essentially creates a cache of function results to avoid redundant computations.\n",
        "\n",
        "   - **Approach:** This method combines recursion with dynamic programming (memoization) to optimize the calculation.\n",
        "   - **Process:** Store the results of previous Fibonacci number calculations in a cache (dictionary or list) to avoid recomputing them.\n",
        "   - **Example:**"
      ],
      "metadata": {
        "id": "57fNck2xmn3J"
      },
      "id": "57fNck2xmn3J"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "metadata": {
        "id": "qN0ABsYLl7Tk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qN0ABsYLl7Tk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Understanding Gini Impurity\n",
        "\n",
        "For a multi-class target with probabilities $p_1, p_2, ..., p_n$, the Gini Impurity is:\n",
        "\n",
        "$$Gini(S) = 1 - \\sum_{i=1}^{n} p_i^2$$\n",
        "\n",
        "\n",
        "**Step 1: Splitting on \"Sound\"**\n",
        "\n",
        "We've (hypothetically) determined that \"Sound\" is the best feature to split on (using information gain across all features). Now let's see how Gini impurity is used to evaluate this split.\n",
        "\n",
        "1.  **Unique Values of \"Sound\":** The unique values in the 'Sound' column are 'Meow', 'Woof', and 'Hiss'.\n",
        "\n",
        "2.  **Create Subsets:** We split the DataFrame into three subsets based on these values:\n",
        "\n"
      ],
      "metadata": {
        "id": "4fkp-lHIEYDP"
      },
      "id": "4fkp-lHIEYDP"
    },
    {
      "cell_type": "code",
      "source": [
        "# subset_meow = df[df['Sound'] == 'Meow']\n",
        "# subset_woof = df[df['Sound'] == 'Woof']\n",
        "# subset_hiss = df[df['Sound'] == 'Hiss']"
      ],
      "metadata": {
        "id": "t5tSTC--4Hza"
      },
      "id": "t5tSTC--4Hza",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Calculate Gini Impurity of Subsets:** We calculate the Gini impurity of the 'Animal' column in each subset:"
      ],
      "metadata": {
        "id": "4VwjwJH64DqT"
      },
      "id": "4VwjwJH64DqT"
    },
    {
      "cell_type": "code",
      "source": [
        "# gini_meow = gini(subset_meow['Animal'])\n",
        "# gini_woof = gini(subset_woof['Animal'])\n",
        "# gini_hiss = gini(subset_hiss['Animal'])\n",
        "\n",
        "# print(f\"Gini Impurity for 'Meow' subset: {gini_meow:.4f}\")\n",
        "# print(f\"Gini Impurity for 'Woof' subset: {gini_woof:.4f}\")\n",
        "# print(f\"Gini Impurity for 'Hiss' subset: {gini_hiss:.4f}\")"
      ],
      "metadata": {
        "id": "E5mNpTug4VvV"
      },
      "id": "E5mNpTug4VvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  **Calculate Weighted Average Gini Impurity:** We calculate the weighted average Gini impurity of the split:"
      ],
      "metadata": {
        "id": "slqB0X9M4REn"
      },
      "id": "slqB0X9M4REn"
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted_gini_sound = (\n",
        "#     (len(subset_meow) / len(df)) * gini_meow +\n",
        "#     (len(subset_woof) / len(df)) * gini_woof +\n",
        "#     (len(subset_hiss) / len(df)) * gini_hiss\n",
        "# )\n",
        "# print(f\"Weighted Average Gini Impurity for 'Sound' Split: {weighted_gini_sound:.4f}\")"
      ],
      "metadata": {
        "id": "dd7G1LuN4lW9"
      },
      "id": "dd7G1LuN4lW9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Do We Calculate the Weighted Gini of \"Sound\"?**\n",
        "\n",
        "   * We calculate the weighted Gini impurity of \"Sound\" (and any other potential splitting feature) to evaluate the **quality** of that specific split. We're not looking for a single \"total Gini score\" for the whole process. Instead, we're comparing the Gini impurity *before* the split to the *expected Gini impurity after* the split.\n",
        "   * The goal is to see how much the split reduces the impurity in the data. A greater reduction is better.\n",
        "   * It's a Gini impurity value *specific to a particular split based on a particular feature*.\n",
        "   * We have:\n",
        "        * `initial_gini`: Gini impurity of the original dataset (before any split).\n",
        "        * `weighted_gini_sound`: The *expected* Gini impurity *after* we split the dataset based on the \"Sound\" feature.\n",
        "\n",
        "**What Does \"Weighted\" Mean?**\n",
        "\n",
        "   * \"Weighted\" in \"weighted Gini impurity\" means that the Gini impurity of each resulting subset is given a weight based on the proportion of data points that fall into that subset.\n",
        "   * **Why weight?** Because subsets can have different sizes. We want to give more importance to the Gini impurity of larger subsets, as they represent a larger portion of the data.\n",
        "\n",
        "**Illustrative Example to Explain \"Weighted\"**\n",
        "\n",
        "   Imagine we have 100 data points, and we're considering a split that creates two subsets:\n",
        "\n",
        "   * Subset A: 90 data points, Gini impurity = 0.1\n",
        "   * Subset B: 10 data points, Gini impurity = 0.5\n",
        "\n",
        "   If we simply averaged the Gini impurities (0.1 + 0.5) / 2 = 0.3, it wouldn't accurately reflect the overall impurity after the split. Subset A is much larger and purer, so it should contribute more to our overall measure of impurity.\n",
        "\n",
        "   The weighted average Gini impurity would be:\n",
        "\n",
        "   (90/100) \\* 0.1 + (10/100) \\* 0.5 = 0.09 + 0.05 = 0.14\n",
        "\n",
        "   This shows that the split gives us a relatively pure result overall because most of the data (90%) is in a pure subset.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "   * We calculate the weighted Gini impurity for each *potential split* to compare the effectiveness of different splits.\n",
        "   * It's a *split-specific* score.\n",
        "   * \"Weighted\" means we give more importance to larger subsets, which is crucial for accurately assessing the overall impurity after a split."
      ],
      "metadata": {
        "id": "Zb89bdu15CsP"
      },
      "id": "Zb89bdu15CsP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  **Lowest weighted Gini Impurity:** We're aiming for the smallest amount of impurity *after* the split.\n",
        "\n",
        "   * **Purity = Better Predictions:** Our goal is to create subsets of data that are \"pure\" with respect to the target variable. A pure subset contains mostly or only data points belonging to a single class. Pure subsets make it easier to make accurate predictions.\n",
        "   * **Impurity = Uncertainty:** High impurity means the data is mixed, and it's hard to predict the class of a data point in that subset. Low impurity means the data is organized, and prediction is more certain.\n",
        "   * **Reducing Uncertainty:** By choosing the split that results in the greatest reduction in impurity (or the lowest weighted average impurity), we are effectively reducing the uncertainty about the target variable. This leads to a decision tree that can make more accurate classifications or predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "shkYsMHk4mZn"
      },
      "id": "shkYsMHk4mZn"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# def calculate_weighted_gini(data, split_feature, target_feature):\n",
        "#     values = data[split_feature].unique()\n",
        "#     weighted_gini = 0\n",
        "#     for value in values:\n",
        "#         subset = data[data[split_feature] == value]\n",
        "#         subset_prob = len(subset) / len(data)\n",
        "#         weighted_gini += subset_prob * gini(subset[target_feature])\n",
        "#         print(f\"  Gini Impurity for '{value}': {gini(subset[target_feature]):.4f}\")\n",
        "#     return weighted_gini\n",
        "\n",
        "# def find_best_split(data, features, target):\n",
        "#     best_feature = None\n",
        "#     min_weighted_gini = 1.0  # Initialize with maximum impurity\n",
        "\n",
        "#     for feature in features:\n",
        "#         weighted_gini = calculate_weighted_gini(data, feature, target)\n",
        "#         print(f\"Weighted Gini for '{feature}': {weighted_gini:.4f}\\n\")\n",
        "#         if weighted_gini < min_weighted_gini:\n",
        "#             min_weighted_gini = weighted_gini\n",
        "#             best_feature = feature\n",
        "\n",
        "#     return best_feature\n",
        "\n",
        "# def build_tree_recursive(data, features, target, depth=0):\n",
        "#     \"\"\"Simplified recursive tree builder (for illustration).\"\"\"\n",
        "\n",
        "#     print(f\"\\n\\n--- Depth: {depth} ---\")\n",
        "#     print(f\"Data:\\n{data}\")\n",
        "#     print(f\"Features: {features}\\n\\n\")\n",
        "\n",
        "#     # Stopping condition (simplified)\n",
        "#     if len(np.unique(data[target])) == 1 or depth > 2 or not features:\n",
        "#         print(f\"  --> Leaf Node: {np.unique(data[target])[0] if len(np.unique(data[target])) == 1 else 'Mixed'}\")\n",
        "#         return\n",
        "\n",
        "#     best_feature = find_best_split(data, features, target)\n",
        "#     print(f\"  Best Feature: {best_feature}\")\n",
        "\n",
        "#     unique_values = data[best_feature].unique()\n",
        "#     print(f\"  Unique Values: {unique_values}\")\n",
        "#     remaining_features = [f for f in features if f != best_feature]\n",
        "\n",
        "#     for value in unique_values:\n",
        "#         subset = data[data[best_feature] == value]\n",
        "#         print(f\"  Branch: {best_feature} = {value}\")\n",
        "#         build_tree_recursive(subset, remaining_features, target, depth + 1)  # Recursive call!\n",
        "\n",
        "# # Initial call to start the tree building\n",
        "# build_tree_recursive(df, df.columns.drop('Animal').tolist(), 'Animal')"
      ],
      "metadata": {
        "id": "0lbibalQ-9vi"
      },
      "id": "0lbibalQ-9vi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Features: ['Fur', 'Legs', 'Sound', 'Tail']\n",
        "\n",
        "\n",
        "  Gini Impurity for 'Furry': 0.6667\n",
        "  Gini Impurity for 'Not Furry': 0.6667\n",
        "Weighted Gini for 'Fur': 0.6667\n",
        "\n",
        "  Gini Impurity for '2': 0.5600\n",
        "  Gini Impurity for '4': 0.6122\n",
        "Weighted Gini for 'Legs': 0.5905\n",
        "\n",
        "  Gini Impurity for 'Hiss': 0.5714\n",
        "  Gini Impurity for 'Woof': 0.5000\n",
        "  Gini Impurity for 'Meow': 0.4444\n",
        "Weighted Gini for 'Sound': 0.5278\n",
        "\n",
        "  Gini Impurity for 'Short': 0.6667\n",
        "  Gini Impurity for 'Long': 0.4444\n",
        "  Gini Impurity for 'None': 0.6111\n",
        "Weighted Gini for 'Tail': 0.5833\n",
        "\n",
        "  Best Feature: Sound\n",
        "  Unique Values: ['Hiss' 'Woof' 'Meow']\n",
        "  Branch: Sound = Hiss\n",
        "```\n",
        "* **Gini Impurity Calculations:**\n",
        "    * For each feature ('Fur', 'Legs', 'Sound', 'Tail'), the Gini impurity is calculated for each unique value of that feature.\n",
        "    * For each feature, a \"Weighted Gini\" is also calculated.\n",
        "\n",
        "* **Feature-Specific Gini Impurity:**\n",
        "    * For 'Fur', both 'Furry' and 'Not Furry' have a Gini impurity of 0.6667, and the weighted Gini is also 0.6667.\n",
        "    * For 'Legs', '2' has a Gini impurity of 0.5600, '4' has 0.6122, and the weighted Gini is 0.5905.\n",
        "    * For 'Sound', 'Hiss' has a Gini impurity of 0.5714, 'Woof' has 0.5000, 'Meow' has 0.4444, and the weighted Gini is 0.5278.\n",
        "    * For 'Tail', 'Short' has a Gini impurity of 0.6667, 'Long' has 0.4444, 'None' has 0.6111, and the weighted Gini is 0.5833.\n",
        "\n",
        "* **Decision:**\n",
        "    * \"Best Feature\" is identified as 'Sound'.\n",
        "    * \"Unique Values\" for 'Sound' are listed as `['Hiss' 'Woof' 'Meow']`.\n",
        "    * The image states \"Branch: Sound\".\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "The decision tree algorithm selected 'Sound' as the first feature to split on. This selection was based on comparing the \"Weighted Gini\" values for all features. The feature 'Sound' has the lowest weighted Gini (0.5278) compared to 'Fur' (0.6667), 'Legs' (0.5905), and 'Tail' (0.5833).\n",
        "\n",
        "The algorithm aims to minimize the impurity after the split, so it chooses the feature that results in the lowest weighted average impurity.\n",
        "\n",
        "The unique values of 'Sound' indicate that the first split will create branches for 'Hiss', 'Woof', and 'Meow'."
      ],
      "metadata": {
        "id": "dbUUqfKLFmrp"
      },
      "id": "dbUUqfKLFmrp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2c8SLYkaoqu"
      },
      "source": [
        "## Iris"
      ],
      "id": "A2c8SLYkaoqu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Data"
      ],
      "metadata": {
        "id": "DX6wuQOKGwjV"
      },
      "id": "DX6wuQOKGwjV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp0Dmkxtaoqu"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# df['species'] = y\n",
        "# df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('species', axis=1),\n",
        "#                                                     df['species'],\n",
        "#                                                     test_size=0.20,\n",
        "#                                                     random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_train.head())"
      ],
      "id": "Tp0Dmkxtaoqu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier Model and Tree"
      ],
      "metadata": {
        "id": "IwO3nKeyGrVr"
      },
      "id": "IwO3nKeyGrVr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_SKH8naaoqv"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# print(X_train.head())\n",
        "\n",
        "# model = DecisionTreeClassifier(criterion='gini', random_state=42).fit(X_train, y_train)\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "id": "7_SKH8naaoqv"
    },
    {
      "cell_type": "code",
      "source": [
        "# # example gini score on petal length\n",
        "# 1 - np.sum(np.square(y_train.value_counts(normalize=True)))"
      ],
      "metadata": {
        "id": "mqFuzxxwBxzD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mqFuzxxwBxzD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Splits"
      ],
      "metadata": {
        "id": "_reav3APNZhU"
      },
      "id": "_reav3APNZhU"
    },
    {
      "cell_type": "code",
      "source": [
        "# example = X_train.copy()\n",
        "# example['species'] = y_train\n",
        "\n",
        "# sns.pairplot(example, hue='species', palette=['red', 'blue', 'green']);"
      ],
      "metadata": {
        "id": "0qJ02zV6I5y3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0qJ02zV6I5y3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pair Plot**\n",
        "\n",
        "* **Petal Length vs. Other Features:** In every scatter plot where petal length is involved (first row, third column), *setosa* (red) is clearly separated along the petal length axis.\n",
        "* **Petal Width vs. Other Features:** The same is true for petal width (fourth column, second row); *setosa* is well-separated.\n",
        "* **Petal Length vs. Petal Width:** This plot shows *setosa* forming a distinct cluster in the lower-left corner, reinforcing that both features together provide excellent separation."
      ],
      "metadata": {
        "id": "WjQndZ0DN0g2"
      },
      "id": "WjQndZ0DN0g2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCZMsXr2aoqv"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train.hist()\n",
        "# plt.tight_layout();"
      ],
      "id": "xCZMsXr2aoqv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histograms**\n",
        "\n",
        "* **Petal Length Histogram:** The histogram for petal length shows a distinct group on the far left (around 1-2 cm) that corresponds to *setosa* (red in the pair plot). There's a clear separation from the other two species, which have petal lengths mostly greater than 3 cm.\n",
        "* **Petal Width Histogram:** Similarly, the petal width histogram shows *setosa* grouped on the far left (around 0-0.5 cm), well-separated from the *versicolor* and *virginica* distributions.\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "The visual separation in both the histograms and pair plots indicates that *setosa* has consistently smaller petal lengths and widths compared to *versicolor* and *virginica*. This \"natural\" separation means a decision tree can easily use these features to create simple rules (like \"petal length <= a threshold\") to isolate *setosa* with high accuracy, as seen in the decision tree where the first split is on petal length."
      ],
      "metadata": {
        "id": "RudmXpxlMjX4"
      },
      "id": "RudmXpxlMjX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partitioning Examples"
      ],
      "metadata": {
        "id": "lhoZjoBEHBkv"
      },
      "id": "lhoZjoBEHBkv"
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "metadata": {
        "id": "9YbGtuU1OnX4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9YbGtuU1OnX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "where does the decision tree model get the specific values it uses for splitting (like `petal length (cm) <= 2.45`). Let's clarify this:\n",
        "\n",
        "**The Model Learns the Split Values from the Training Data**\n",
        "\n",
        "The decision tree algorithm *learns* these threshold values (e.g., 2.45 for petal length, 1.65 or 1.75 for petal width) directly from the **training data** you provide it.  It doesn't pull them out of thin air or use some pre-defined list.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "1.  **Training Data:**\n",
        "    * You start with a dataset (like the Iris dataset) that contains measurements of features (petal length, petal width, sepal length, sepal width) for a set of samples, along with the known class or category (species) for each sample.\n",
        "    * This dataset is typically split into two parts:\n",
        "        * **Training set:** This is the portion of the data the model uses to learn the relationships between features and the target variable (species).\n",
        "        * **Testing set:** This is a separate portion used to evaluate how well the model generalizes to new, unseen data.\n",
        "\n",
        "2.  **Algorithm's Search for Optimal Splits:**\n",
        "    * The decision tree algorithm (like CART, ID3, or C4.5) examines the training data to find the best way to split the data at each node.\n",
        "    * \"Best\" is defined based on a criterion like:\n",
        "        * **Information Gain (for ID3, C4.5):** How much does this split reduce the entropy (uncertainty) about the target variable?\n",
        "        * **Gini Impurity (for CART):** How much does this split reduce the \"impurity\" (mixing of classes) in the resulting subsets?\n",
        "    * To find the best split, the algorithm essentially tries out different possible threshold values for each feature.\n",
        "    * For example, for 'petal length', it might try splitting at 2.0 cm, 2.1 cm, 2.2 cm, 2.3 cm, 2.4 cm, 2.45 cm, 2.5 cm, and so on, evaluating the Information Gain or Gini Impurity for each potential split.\n",
        "    * The algorithm then selects the threshold value that results in the greatest improvement in the chosen criterion (highest Information Gain or lowest Gini Impurity).\n",
        "\n",
        "3.  **Example with Petal Length <= 2.45:**\n",
        "    * In the tree you provided, the algorithm found that splitting the data at `petal length (cm) <= 2.45` produced the most significant reduction in impurity (or increase in Information Gain) at that stage of the tree building process.\n",
        "    * This means that, among all the possible petal length values, 2.45 cm was the best at separating the Iris species in the training data at that point.\n",
        "\n",
        "4.  **No Predefined Values:**\n",
        "    * It's crucial to understand that the algorithm is *calculating* and *selecting* these values based on the data. It's not using any pre-set list of petal length or width values.\n",
        "    * If you trained the same model on a slightly different training set (a different random sample of the Iris data), the exact split values might be slightly different, although the overall structure of the tree would likely be similar.\n",
        "\n",
        "**In essence:** The decision tree model learns the rules (including the threshold values for splitting) by analyzing patterns in the training data and finding the splits that best organize the data points according to the target variable."
      ],
      "metadata": {
        "id": "HoDRCtXyQt5_"
      },
      "id": "HoDRCtXyQt5_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg1bovfZaoqv"
      },
      "outputs": [],
      "source": [
        "# # plot using hue to show different classes\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.scatterplot(x=example['petal length (cm)'],\n",
        "#                 y=example['petal width (cm)'],\n",
        "#                 hue=example['species'],\n",
        "#                 palette=['red', 'blue', 'green'])\n",
        "# plt.axvline(x=2.45, color='black')\n",
        "# plt.axvline(x=4.75, color='black')\n",
        "# plt.hlines(y=1.75, xmin=2.45, xmax=8, color='black')\n",
        "\n",
        "# plt.xlim(0, 8)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "id": "Sg1bovfZaoqv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS63Jj0maoqw"
      },
      "outputs": [],
      "source": [
        "# # plot using hue to show different classes\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.scatterplot(x=example['petal length (cm)'][example['petal length (cm)']>2.45],\n",
        "#                 y=example['petal width (cm)'],\n",
        "#                 hue=example['species'],\n",
        "#                 palette=['red', 'blue', 'green'])\n",
        "# plt.axhline(y=1.75, color='black')\n",
        "# plt.vlines(x=4.95, ymin=0, ymax=1.75, color='black')\n",
        "# plt.vlines(x=5.45, ymin=0, ymax=1.75, color='black')\n",
        "# plt.axhline(y=1.55, color='black')\n",
        "\n",
        "# plt.xlim(2.45, 8)\n",
        "# plt.ylim(0, )\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "id": "YS63Jj0maoqw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Explanation**\n",
        "\n",
        "The decision tree algorithm aims to partition the Iris dataset by making a series of decisions based on the feature values, ultimately creating subsets that are as pure as possible with respect to the Iris species (setosa, versicolor, and virginica). It prioritizes the features that best separate the species.\n",
        "\n",
        "**Step-by-Step Breakdown with \"I\" Perspective**\n",
        "\n",
        "* **Initial State:** I start with the entire Iris dataset, which, as shown in the histograms (Image 2), has some overlap between species, particularly in petal length and width. This means the dataset isn't perfectly separated from the beginning.\n",
        "* **First Split (Root Node):**\n",
        "    * I observe in the tree (Image 1) that the first split occurs on `petal length (cm) <= 2.45`.\n",
        "    * Looking at the scatter plot (Image 3), I can see that this split effectively isolates the *setosa* species (red points) on the left.\n",
        "    * So, I'm essentially saying: \"If a flower has a petal length of 2.45 cm or less, I confidently classify it as *setosa*.\" This creates a pure *setosa* leaf node.\n",
        "* **Second Split (Right Branch):**\n",
        "    * For the remaining data points (which are *versicolor* and *virginica*), I see that the tree splits on `petal length (cm) <= 4.75`.\n",
        "    * In the scatter plot, this is the vertical line at around 4.75 cm.  This split further divides the dataset, though there's still some mixing of *versicolor* and *virginica*.\n",
        "    * I'm now saying: \"Of the flowers that are *not* *setosa*, if their petal length is 4.75 cm or less, they are *likely* *versicolor*.\"\n",
        "* **Third Split (Left Branch from Second):**\n",
        "    * The tree then splits the *likely versicolor* branch based on `petal width (cm) <= 1.65`.\n",
        "    * This is the horizontal line at 1.65 cm in the scatter plot (Image 3).\n",
        "    * I refine my classification: \"Of the non-*setosa* flowers with petal lengths less than or equal to 4.75 cm, if their petal width is also less than or equal to 1.65 cm, I classify them as *versicolor*.\"  There's still a tiny bit of misclassification here, as seen in the tree's Gini impurity.\n",
        "* **Fourth Split (Right Branch from Second):**\n",
        "    * Finally, the tree splits the other branch (petal length > 4.75 cm) based on `petal width (cm) <= 1.75`.\n",
        "    * This is the horizontal line at 1.75 cm in the scatter plot.\n",
        "    * I conclude: \"The remaining flowers (non-*setosa* with petal lengths greater than 4.75 cm) are classified based on petal width. If the petal width is greater than 1.75 cm, they are classified as *virginica*; otherwise, *versicolor*.\" Again, there's a small amount of misclassification.\n",
        "\n",
        "**Intuitive Summary**\n",
        "\n",
        "The tree essentially creates a series of if-else rules that carve up the data space. It starts with the most important feature (petal length) to make the biggest distinction (separating *setosa*). Then, it refines the classifications using other features (petal width) to separate the remaining species as cleanly as possible. The scatter plots visually show how these rules correspond to dividing the data points into rectangular regions."
      ],
      "metadata": {
        "id": "R2fDPwJsMpGh"
      },
      "id": "R2fDPwJsMpGh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests\n",
        "\n",
        "**Random Forest** is an ensemble supervised learning algorithm that builds multiple decision trees on different subsets of the data and using random subsets of features. For prediction (classification or regression), it aggregates the predictions of all the individual trees, typically through majority voting (classification) or averaging (regression), to improve accuracy and reduce overfitting."
      ],
      "metadata": {
        "id": "i1DFJji1-sf5"
      },
      "id": "i1DFJji1-sf5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Learning\n",
        "\n",
        "In statistics and machine learning, **ensemble methods** use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Ensemble_learning\n",
        "\n",
        "See Ensemble Learning Notebook\n",
        "\n",
        "The traditional way of performing hyperparameter optimization has been **grid search**, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search"
      ],
      "metadata": {
        "id": "YzxoYTH_DMLe"
      },
      "id": "YzxoYTH_DMLe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging and Boosting\n",
        "\n",
        "* Out of bag...\n",
        "* Bagging parallel, boosting sequential"
      ],
      "metadata": {
        "id": "L_8-zzBDn4MI"
      },
      "id": "L_8-zzBDn4MI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfXyPaw4aoq4"
      },
      "source": [
        "### Pruning - Hyperparameters\n",
        "\n",
        "A parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "* https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\n",
        "\n",
        "Here are some default parameters:\n",
        "\n",
        "<pre>\n",
        "hyperparameters = {\n",
        "            'n_estimators': 100,\n",
        "            'criterion': 'gini',\n",
        "            'max_depth': None,\n",
        "            'max_leaf_nodes': None,\n",
        "            'bootstrap': True\n",
        "            }\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "</pre>\n",
        "\n",
        "**Parameters vs Hyperparameters**:\n",
        "* Parameter: Usually estimated or learned from data\n",
        "* Hyperparameter: Values that are tuned by the data scientist"
      ],
      "id": "LfXyPaw4aoq4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54PGK4F5aoq4"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data = load_iris()\n",
        "# X = data.data\n",
        "# y = data.target\n",
        "\n",
        "# iris = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# iris['species'] = y\n",
        "# X_train, X_test, y_train, y_test = train_test_split(iris.drop('species', axis=1),\n",
        "#                                                     iris['species'],\n",
        "#                                                     test_size=0.20,\n",
        "#                                                     random_state=42)"
      ],
      "id": "54PGK4F5aoq4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "995HXwlUaoq5"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# hyperparameters = {\n",
        "#             'criterion': 'entropy'\n",
        "#             }\n",
        "\n",
        "# model = DecisionTreeClassifier(random_state=42).set_params(**hyperparameters)\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "id": "995HXwlUaoq5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9PJck9Yaoq6"
      },
      "source": [
        "The above tree keeps splitting till all the nodes are pure and can lead to overfitting. The next cell introduces some (hyper)parameters that help avoid overfitting."
      ],
      "id": "v9PJck9Yaoq6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa_H_MLvaoq6"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# hyperparameters = {\n",
        "#             'criterion': 'entropy',\n",
        "#             'max_depth': 3,\n",
        "#             'max_leaf_nodes': 4\n",
        "#             }\n",
        "\n",
        "# model = DecisionTreeClassifier(random_state=42).set_params(**hyperparameters)\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "id": "Sa_H_MLvaoq6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqHLJYAaoq6"
      },
      "source": [
        "### Tuning Random Forest (Hyper)Parameters\n",
        "\n",
        "**Focusing on Tree Structure and Complexity:**\n",
        "\n",
        "* **`min_samples_split`**: This controls the minimum number of samples required to split an internal node.\n",
        "    * **Suggested Values:** `[2, 5]`\n",
        "    * **Justification:** A smaller value (like 2) allows for more complex trees that might overfit, while a larger value (like 5) can help prevent overfitting by ensuring nodes only split if they contain a reasonable number of data points. Limiting to two values keeps the search space manageable.\n",
        "* **`min_samples_leaf`**: This controls the minimum number of samples required to be at a leaf node.\n",
        "    * **Suggested Values:** `[1, 3]`\n",
        "    * **Justification:** Similar to `min_samples_split`, a smaller value (like 1) can lead to more complex trees, while a larger value (like 3) can promote more generalized models by requiring a minimum number of samples in the final predictions.\n",
        "* **`max_features`**: This determines the number of features to consider when looking for the best split at each node.\n",
        "    * **Suggested Values:** `['sqrt', 0.5]`\n",
        "    * **Justification:**\n",
        "        * `'sqrt'` (or 'log2') considers the square root of the total number of features. This is a common and often effective default.\n",
        "        * `0.5` considers half of the total number of features. This provides a different level of randomness in feature selection compared to 'sqrt'. Limiting to two options keeps the search efficient.\n",
        "\n",
        "**Hyperparameter Related to Randomness:**\n",
        "\n",
        "* **`random_state`**: While not directly a tuning parameter for the model's complexity, it's crucial for reproducibility.\n",
        "    * **Suggested Value:** `[42]` (or any single integer)\n",
        "    * **Justification:** Setting a `random_state` ensures that the random processes within the Random Forest (like bootstrapping and feature selection) produce the same results each time the code is run. This is important for consistent evaluation and comparison of different hyperparameter settings. You might not include this in the *tuning* grid but should emphasize its importance for good practice.\n",
        "\n",
        "**Considerations for Keeping Execution Time Down:**\n",
        "\n",
        "* **Number of Hyperparameters:** The example you provided has 5 hyperparameters. Adding 3 more (`min_samples_split`, `min_samples_leaf`, `max_features`) will increase the size of the grid significantly (2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 = 256 combinations). Be mindful of this. You might suggest that students initially try a smaller grid and then potentially expand if time allows.\n",
        "* **Range of Values:** Keeping the number of values for each hyperparameter limited is key, as you've already done.\n",
        "\n",
        "```python\n",
        "hyperparameters = {\n",
        "    'n_estimators': [50, 150],  # Slightly reduced max for faster execution\n",
        "    'criterion': ['entropy', 'gini'],\n",
        "    'max_depth': [3, 5],      # Slightly increased max depth\n",
        "    'max_leaf_nodes': [6, 10], # Adjusted range\n",
        "    'bootstrap': [True, False],\n",
        "}\n",
        "```\n",
        "\n",
        "**Important Considerations**\n",
        "\n",
        "* **Justification is Key:** Explain *why* you chose the specific values for each hyperparameter. Your reasoning should be based on your understanding of how these parameters affect the model's bias-variance trade-off, complexity, and potential for overfitting or underfitting.\n",
        "* **Cross-Validation Strategy:** Use an appropriate cross-validation strategy (e.g., Stratified K-Fold since it's likely a classification task with potentially imbalanced classes) to get a reliable estimate of the model's performance for each hyperparameter combination.\n",
        "* **Computational Limits:** Keep in mind the constraints on their hyperparameter choices and grid size and computational time. You might need to start with a smaller grid and iterate if time permits.\n",
        "\n",
        "Random forests create many decision trees that sample data. The bootstrap hyperparameter sets sampling with or without replacement.\n",
        "\n",
        "**Rule**: Never make adjustments to your model based on test set results.\n",
        "            "
      ],
      "id": "wgqHLJYAaoq6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60uMb6Eqaoq7"
      },
      "source": [
        "### Random Forest Model with Grid Search"
      ],
      "id": "60uMb6Eqaoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "920nHbfOaoq7"
      },
      "outputs": [],
      "source": [
        "# # create dataframe from sklearn iris dataset; print shape, info, and head\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import load_iris\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# df['species'] = y\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "id": "920nHbfOaoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WjRu5NLaoq7"
      },
      "outputs": [],
      "source": [
        "# # train test split using 25% for test size; print X_train shape and head\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('species', axis=1),\n",
        "#                                                     df['species'],\n",
        "#                                                     test_size=0.25,\n",
        "#                                                     random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_train.head())"
      ],
      "id": "0WjRu5NLaoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "298UVX_Taoq7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# hyperparameters = {\n",
        "#     'n_estimators': [50, 150],\n",
        "#     'criterion': ['entropy', 'gini'],\n",
        "#     'max_depth': [3, 5],\n",
        "#     'max_leaf_nodes': [6, 10],\n",
        "#     'bootstrap': [True, False],\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(estimator = RandomForestClassifier(),\n",
        "#                            param_grid = hyperparameters,\n",
        "#                            scoring = 'accuracy',\n",
        "#                            cv = 10)\n",
        "\n",
        "# grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# best_accuracy = grid_search.best_score_\n",
        "# best_parameters = grid_search.best_params_\n",
        "\n",
        "# print('best accuracy', best_accuracy)\n",
        "# print('best parameters', best_parameters)"
      ],
      "id": "298UVX_Taoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrmA7QA2aoq7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # model = RandomForestClassifier(bootstrap = False,\n",
        "# #                                criterion = 'entropy',\n",
        "# #                                max_depth = 3,\n",
        "# #                                min_samples_leaf = 5,\n",
        "# #                                min_samples_split = 4,\n",
        "# #                                random_state = 42)\n",
        "# model = RandomForestClassifier(random_state = 42).set_params(**best_parameters) # * args, ** kwargs\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))"
      ],
      "id": "qrmA7QA2aoq7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}