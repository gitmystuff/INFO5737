{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/INFO5737/blob/main/Anomaly_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection\n",
        "\n",
        "Your Name"
      ],
      "metadata": {
        "id": "fXCvi2LefKtQ"
      },
      "id": "fXCvi2LefKtQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff INFO5737 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "ojyIDptwB7Iy"
      },
      "id": "ojyIDptwB7Iy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Anomaly Detection?\n",
        "\n",
        "Anomaly detection, also known as outlier detection, is the process of identifying data points, events, or observations that deviate significantly from the normal or expected patterns within a dataset. These anomalies are often rare and can indicate unusual behavior, errors, fraud, or other significant events.\n",
        "\n",
        "Here's a breakdown of the key aspects of anomaly detection:\n",
        "\n",
        "**Core Idea:** To find the \"unusual\" in data by comparing it to what is considered \"normal.\"\n",
        "\n",
        "**Why is it Important?**\n",
        "\n",
        "* **Identifying Problems:** Anomalies can signal critical issues like system failures, security breaches, or fraudulent activities.\n",
        "* **Preventing Losses:** Early detection of anomalies can help organizations mitigate potential financial losses or damages.\n",
        "* **Improving Efficiency:** Identifying and addressing anomalies in processes can lead to optimization and better performance.\n",
        "* **Ensuring Data Quality:** Anomalies can highlight errors or inconsistencies in data collection or processing.\n",
        "\n",
        "**Types of Anomalies:**\n",
        "\n",
        "Anomalies can be broadly categorized into three main types:\n",
        "\n",
        "* **Point Anomalies (Outliers):** Individual data points that are significantly different from the rest of the data. For example, a single unusually high transaction in a series of credit card purchases.\n",
        "* **Contextual Anomalies (Conditional Outliers):** Data points that are unusual within a specific context but might be normal in another. For example, a very high temperature reading for a city in winter would be anomalous, but the same reading in summer might be normal. These are often found in time-series data.\n",
        "* **Collective Anomalies (Group Anomalies):** A group of related data points that, when considered together, deviate from the expected pattern, even if individual points within the group might not be anomalous on their own. For example, a series of small, frequent transactions from the same account to multiple new accounts could indicate fraudulent activity, even if each transaction individually is not large enough to be flagged.\n",
        "\n",
        "**Approaches to Anomaly Detection:**\n",
        "\n",
        "Anomaly detection techniques can be broadly classified based on the availability of labeled data:\n",
        "\n",
        "* **Supervised Anomaly Detection:** Requires a labeled dataset containing both normal and anomalous instances. Classification algorithms are trained to distinguish between the two classes. This approach is often challenging because labeled anomaly data is typically scarce and imbalanced.\n",
        "* **Semi-Supervised Anomaly Detection:** Uses a dataset containing only normal instances to train a model. The model then identifies instances that do not fit the learned normal pattern as anomalies.\n",
        "* **Unsupervised Anomaly Detection:** Does not rely on any labeled data. It assumes that normal data points are far more frequent than anomalies and tries to find patterns or instances that deviate from the majority of the data. This is the most common approach as labeled anomaly data is often unavailable.\n",
        "\n",
        "**Common Unsupervised Anomaly Detection Algorithms (relevant to your interest in machine learning):**\n",
        "\n",
        "* **Statistical Methods:** These methods assume a distribution for the normal data and identify data points that fall outside a defined statistical range (e.g., based on mean and standard deviation, or percentiles).\n",
        "* **Clustering-Based Methods:** These methods group similar data points into clusters. Anomalies are often points that do not belong to any cluster or belong to small, isolated clusters. (e.g., K-Means)\n",
        "* **Density-Based Methods:** These methods estimate the density of data points. Anomalies are points in low-density regions. (e.g., Local Outlier Factor - LOF, DBSCAN)\n",
        "* **Distance-Based Methods:** These methods calculate the distance between data points. Anomalies are points that are far from their nearest neighbors. (e.g., k-Nearest Neighbors - kNN)\n",
        "* **Tree-Based Methods:** These methods build decision trees to isolate anomalies. Anomalies tend to be isolated in fewer splits. (e.g., Isolation Forest, Robust Random Cut Forest)\n",
        "* **Autoencoders (Neural Networks):** These neural networks are trained to reconstruct normal data. Anomalies, being different, result in higher reconstruction errors.\n",
        "* **Principal Component Analysis (PCA):** This dimensionality reduction technique can identify anomalies as data points that have a large reconstruction error when projected onto the principal components of the normal data.\n",
        "\n",
        "**Applications of Anomaly Detection:**\n",
        "\n",
        "Anomaly detection is used in a wide range of domains, including:\n",
        "\n",
        "* **Cybersecurity:** Intrusion detection, fraud detection, malware analysis, identifying unusual user behavior.\n",
        "* **Finance:** Credit card fraud detection, detecting unusual trading activities.\n",
        "* **Manufacturing:** Quality control, predictive maintenance of equipment.\n",
        "* **Healthcare:** Detecting abnormal patient conditions, identifying anomalies in medical images.\n",
        "* **Retail and E-commerce:** Identifying fraudulent transactions, detecting unusual purchasing patterns, supply chain optimization.\n",
        "* **IoT and Sensor Data:** Monitoring sensor readings for unusual patterns indicating failures or anomalies.\n",
        "* **Environmental Monitoring:** Detecting unusual climate patterns or pollution levels."
      ],
      "metadata": {
        "id": "CPjSAkqMfjCU"
      },
      "id": "CPjSAkqMfjCU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://www.sans.org/cyber-security-courses/applied-data-science-machine-learning/\n",
        "* https://github.com/SecurityNik/Data-Science-and-ML/blob/main/Beginning%20Machine%20and%20Deep%20Learning%20with%20Zeek%20logs/08%20-%20beginning%20Machine%20Learning%20Anomaly%20Detection%20-%20isolation%20forest%20and%20local%20outlier%20factor.ipynb"
      ],
      "metadata": {
        "id": "fqnsLQ_78pml"
      },
      "id": "fqnsLQ_78pml"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Traffic\n",
        "\n",
        "Network traffic refers to the **flow of data across a computer network** at any given point in time. Think of it like cars on a highway – the more cars moving, the higher the traffic. In computer networks, this \"traffic\" consists of **data packets**, which are small segments of data that contain the information being transmitted, along with addressing and control information.\n",
        "\n",
        "Here's a breakdown of what that means:\n",
        "\n",
        "* **Data:** This is the actual information being exchanged, such as emails, web pages, videos, files, voice conversations, and commands.\n",
        "* **Packets:** Data is broken down into these smaller units for efficient transmission across the network. Each packet has a header containing source and destination addresses, protocol information, and sequencing details, and a payload containing a piece of the actual data.\n",
        "* **Flow:** The movement of these packets from a source device to a destination device across the network infrastructure (cables, routers, switches, wireless signals, etc.).\n",
        "* **Time:** Network traffic is a dynamic measure, constantly changing based on network activity.\n",
        "\n",
        "**Key Aspects of Network Traffic:**\n",
        "\n",
        "* **Volume:** The amount of data being transferred, usually measured in bits per second (bps) or its multiples (Kbps, Mbps, Gbps).\n",
        "* **Rate:** The speed at which data is being transferred.\n",
        "* **Types:** Different kinds of data generate different types of traffic (e.g., web browsing (HTTP/HTTPS), email (SMTP/POP3/IMAP), file transfer (FTP/SFTP), streaming (RTP), etc.).\n",
        "* **Direction:** Traffic can be inbound (coming into your network) or outbound (leaving your network). It can also be north-south (between a client and a server) or east-west (within a data center).\n",
        "* **Protocols:** Network traffic relies on various communication protocols (like TCP/IP, UDP, DNS, HTTP) that define the rules for how data is formatted, transmitted, and received.\n",
        "\n",
        "**Why is Understanding Network Traffic Important?**\n",
        "\n",
        "* **Network Performance:** High traffic can lead to congestion, causing slowdowns and impacting user experience. Monitoring traffic helps in capacity planning and identifying bottlenecks.\n",
        "* **Network Security:** Analyzing traffic patterns is crucial for detecting anomalies that might indicate security threats like intrusions, malware activity, or data exfiltration.\n",
        "* **Troubleshooting:** Understanding traffic flow can help diagnose network connectivity issues and identify the source of problems.\n",
        "* **Quality of Service (QoS):** Network traffic analysis allows for prioritizing certain types of traffic (e.g., voice or video) to ensure a better user experience for real-time applications.\n",
        "* **Cost Management:** Monitoring bandwidth usage can help organizations manage internet costs and identify potential overages."
      ],
      "metadata": {
        "id": "EIzV5-9bhn0P"
      },
      "id": "EIzV5-9bhn0P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TCP/IP Review"
      ],
      "metadata": {
        "id": "4C9NjUZn_uvi"
      },
      "id": "4C9NjUZn_uvi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OSI Framework\n",
        "\n",
        "The Open Systems Interconnection (OSI) framework is a conceptual model created by the International Organization for Standardization (ISO) in 1984. It provides a standardized way to understand how different hardware and software components in a network communicate. The OSI model consists of seven distinct layers, each with specific responsibilities that ensure seamless data transmission across diverse network technologies.\n",
        "\n",
        "Here are the seven layers of the OSI framework, starting from the top:\n",
        "\n",
        "**1. Application Layer (Layer 7):**\n",
        "This is the layer that directly interacts with end-user applications. It provides the interface between the application and the network, enabling users to access network services. Examples of protocols at this layer include HTTP (for web browsing), SMTP (for email), and FTP (for file transfer).\n",
        "\n",
        "**2. Presentation Layer (Layer 6):**\n",
        "The Presentation Layer is responsible for the formatting, encryption, and compression of data. It ensures that the information sent by one application is readable by another application on a different system, handling differences in data representation.\n",
        "\n",
        "**3. Session Layer (Layer 5):**\n",
        "This layer manages the establishment, maintenance, and termination of connections (sessions) between applications. It handles authentication and authorization and ensures that data streams are properly managed and synchronized.\n",
        "\n",
        "**4. Transport Layer (Layer 4):**\n",
        "The Transport Layer provides reliable or unreliable end-to-end delivery of data between hosts. Key protocols at this layer include TCP (Transmission Control Protocol), which offers reliable, connection-oriented communication, and UDP (User Datagram Protocol), which provides faster, connectionless communication. This layer handles segmentation, reassembly, flow control, and error control.\n",
        "\n",
        "**5. Network Layer (Layer 3):**\n",
        "The Network Layer is responsible for addressing and routing data packets across networks. It determines the best path for data to travel from source to destination using logical addresses (IP addresses). Routers operate at this layer.\n",
        "\n",
        "**6. Data Link Layer (Layer 2):**\n",
        "This layer handles the transfer of data between two directly connected nodes on a local network segment. It deals with physical addressing (MAC addresses), framing of data, and error detection within the local link. Protocols like Ethernet and Wi-Fi operate at this layer.\n",
        "\n",
        "**7. Physical Layer (Layer 1):**\n",
        "The Physical Layer is the lowest layer and deals with the physical transmission of raw bit streams over a physical medium (cables, wireless signals, etc.). It defines the electrical, mechanical, procedural, and functional specifications for activating, maintaining, and deactivating the physical link.\n",
        "\n",
        "In the OSI model, data moves down through the layers on the sending device, with each layer adding its header information (encapsulation). Once the data reaches the receiving device, it moves up through the layers, with each layer interpreting and removing its corresponding header (decapsulation) until it reaches the Application Layer.\n",
        "\n",
        "The OSI framework is a crucial tool for understanding network communication, troubleshooting issues, and developing network technologies. By dividing the complex process of networking into these seven distinct layers, it provides a clear and standardized way to analyze and manage network operations."
      ],
      "metadata": {
        "id": "WMCH3wfDi9M_"
      },
      "id": "WMCH3wfDi9M_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TCP/IP 5 Layer Framework\n",
        "\n",
        "The TCP/IP 5-layer framework is a conceptual model that organizes the suite of communication protocols used for the internet and most modern networks into five distinct layers. This layering helps to understand the different levels of abstraction involved in network communication.\n",
        "\n",
        "Here's an overview of each layer:\n",
        "\n",
        "**1. Application Layer:**\n",
        "\n",
        "This is the top layer of the TCP/IP model, closest to the end-user and the applications they interact with. It provides the interface for these applications to utilize network services. The Application Layer defines the protocols that applications use to exchange data. Examples of protocols at this layer include:\n",
        "\n",
        "* **HTTP (Hypertext Transfer Protocol):** Used for web browsing.\n",
        "* **HTTPS (HTTP Secure):** Secure version of HTTP.\n",
        "* **FTP (File Transfer Protocol):** Used for transferring files.\n",
        "* **SFTP (SSH File Transfer Protocol):** Secure file transfer.\n",
        "* **SMTP (Simple Mail Transfer Protocol):** Used for sending email.\n",
        "* **POP3 (Post Office Protocol version 3):** Used for retrieving email.\n",
        "* **IMAP (Internet Message Access Protocol):** Another protocol for retrieving email.\n",
        "* **DNS (Domain Name System):** Translates domain names to IP addresses.\n",
        "* **SSH (Secure Shell):** Provides secure remote access to systems.\n",
        "* **Telnet:** Provides unsecure remote access to systems.\n",
        "* **SNMP (Simple Network Management Protocol):** Used for network management.\n",
        "\n",
        "The data unit at this layer is simply referred to as **data**.\n",
        "\n",
        "**2. Transport Layer:**\n",
        "\n",
        "The Transport Layer is responsible for providing end-to-end communication between applications running on different hosts. It manages the reliable or unreliable delivery of data segments. The key protocols at this layer are:\n",
        "\n",
        "* **TCP (Transmission Control Protocol):** Offers a connection-oriented, reliable, and ordered delivery of data. It handles segmentation of data from the Application Layer, reassembly at the destination, flow control to prevent overwhelming the receiver, and error recovery through retransmission.\n",
        "* **UDP (User Datagram Protocol):** Provides a connectionless and unreliable delivery of data. It offers faster communication with minimal overhead, suitable for applications where speed is more critical than guaranteed delivery (e.g., streaming, online gaming).\n",
        "\n",
        "The data units at this layer are **segments** (for TCP) and **datagrams** (for UDP).\n",
        "\n",
        "**3. Network Layer (or Internet Layer):**\n",
        "\n",
        "This layer is responsible for addressing and routing data packets across the network. Its primary function is to enable communication between different networks. The main protocols at this layer are:\n",
        "\n",
        "* **IP (Internet Protocol):** Provides logical addressing (IP addresses) for hosts and handles the routing of packets across networks. It is an unreliable, connectionless protocol, meaning it doesn't guarantee delivery or the order of packets.\n",
        "* **ICMP (Internet Control Message Protocol):** Used for error reporting and network diagnostic functions (e.g., ping, traceroute).\n",
        "* **ARP (Address Resolution Protocol):** Used to map IP addresses to physical MAC addresses within a local network segment.\n",
        "\n",
        "The data unit at this layer is called a **packet**.\n",
        "\n",
        "**4. Data Link Layer:**\n",
        "\n",
        "The Data Link Layer focuses on the reliable transfer of data between two directly connected nodes on a local network segment. It deals with the physical addressing of devices (MAC addresses), framing of data for transmission over the physical medium, and basic error detection within the local link. Protocols at this layer include:\n",
        "\n",
        "* **Ethernet (IEEE 802.3):** A common standard for wired local area networks.\n",
        "* **Wi-Fi (IEEE 802.11):** A common standard for wireless local area networks.\n",
        "* **PPP (Point-to-Point Protocol):** Used for establishing direct connections between two nodes, often used for dial-up or VPN connections.\n",
        "\n",
        "The data unit at this layer is called a **frame**.\n",
        "\n",
        "**5. Physical Layer:**\n",
        "\n",
        "This is the lowest layer of the TCP/IP model, concerned with the physical transmission of data bits over a communication medium. It defines the physical and electrical specifications of the network hardware and transmission media. This layer deals with:\n",
        "\n",
        "* **Physical cables and connectors.**\n",
        "* **Radio frequencies for wireless transmission.**\n",
        "* **Voltage levels and signaling rates.**\n",
        "* **Synchronization of bits.**\n",
        "\n",
        "This layer doesn't have high-level protocols in the same way as the upper layers. Instead, it defines the characteristics of the physical connection. The data unit at this layer is a **bit**.\n",
        "\n",
        "Data travels down the TCP/IP stack through a process called **encapsulation**, where each layer adds its own header (and sometimes trailer) to the data it receives from the layer above. At the destination, this process is reversed through **decapsulation**, with each layer removing its header as the data moves up the stack to the receiving application."
      ],
      "metadata": {
        "id": "x2oV-gayiQc9"
      },
      "id": "x2oV-gayiQc9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Packet and Encapsulation\n",
        "\n",
        "**What is a Packet?**\n",
        "\n",
        "At its core, a **packet** is a fundamental unit of data transmission on a packet-switched network, like the internet. When data needs to be sent from one device to another, it's broken down into these smaller, manageable chunks called packets. Each packet contains:\n",
        "\n",
        "* **Payload:** This is the actual data being transmitted – a piece of an email, a fragment of a web page, a segment of a video stream, etc.\n",
        "* **Header:** This is control information added to the beginning of the payload. The header contains crucial details that help network devices route the packet to its destination and ensure proper reassembly. Information in the header can include source and destination addresses (both logical and physical), protocol identifiers, sequence numbers, error-checking information, and flags.\n",
        "* **(Sometimes) Trailer:** Some protocols, particularly at lower layers, add a trailer to the end of the payload. Trailers often contain error detection mechanisms like Cyclic Redundancy Checks (CRC) to verify the integrity of the transmitted data.\n",
        "\n",
        "**Packet as a Protocol Data Unit (PDU)**\n",
        "\n",
        "A **Protocol Data Unit (PDU)** is a more formal term used to refer to the unit of data exchanged between peer layers in a networking model like TCP/IP or OSI. The term \"packet\" is most commonly associated with the Network Layer (Layer 3) in the TCP/IP model. However, each layer in the TCP/IP model has its own specific name for the PDU it handles:\n",
        "\n",
        "* **Application Layer (Layer 1): Data**\n",
        "* **Transport Layer (Layer 2): Segment (TCP), Datagram (UDP)**\n",
        "* **Network Layer (Layer 3): Packet**\n",
        "* **Data Link Layer (Layer 4): Frame**\n",
        "* **Physical Layer (Layer 5): Bit**\n",
        "\n",
        "So, while \"packet\" specifically refers to the PDU at the Network Layer, in a broader sense, people often use it informally to refer to the encapsulated data units being transmitted across the network.\n",
        "\n",
        "**Encapsulation**\n",
        "\n",
        "**Encapsulation** is the process by which each layer in the TCP/IP model adds its own header (and sometimes trailer) to the data it receives from the layer above. This creates the layered structure of the PDU as it travels down the protocol stack on the sending device.\n",
        "\n",
        "Here's how encapsulation works within the TCP/IP 5-layer model:\n",
        "\n",
        "1.  **Application Layer:** The application creates data that needs to be sent. This data is passed down to the Transport Layer.\n",
        "\n",
        "2.  **Transport Layer:**\n",
        "    * **TCP:** If TCP is used, the Application Layer data is segmented into smaller units. A TCP header is added to each segment. This header includes source and destination port numbers, sequence numbers, acknowledgment numbers, and control flags. The PDU at this layer is called a **segment**.\n",
        "    * **UDP:** If UDP is used, a UDP header is added to the Application Layer data. The UDP header is simpler, containing source and destination port numbers and a checksum. The PDU at this layer is called a **datagram**.\n",
        "\n",
        "3.  **Network Layer (Internet Layer):** The segment (from TCP) or datagram (from UDP) is passed down to the Network Layer. An IP header is added. This header contains source and destination IP addresses, routing information, time-to-live (TTL), and protocol information. The PDU at this layer is the **packet**.\n",
        "\n",
        "4.  **Data Link Layer:** The IP packet is passed down to the Data Link Layer. A Data Link Layer header is added, which typically includes the physical (MAC) addresses of the source and destination network interface cards (NICs) on the local network segment. Some Data Link Layer protocols also add a trailer for error detection (e.g., CRC). The PDU at this layer is the **frame**.\n",
        "\n",
        "5.  **Physical Layer:** The frame is passed down to the Physical Layer. This layer doesn't add headers or trailers in the same way. Instead, it converts the frame into a stream of bits and transmits these bits over the physical medium (e.g., copper wire, fiber optic cable, radio waves). The PDU at this layer is the **bit**.\n",
        "\n",
        "**Summary of PDUs and Encapsulation in the TCP/IP 5-Layer Model:**\n",
        "\n",
        "| Layer           | PDU Name     | Encapsulation Process                                                                | Key Header Information                                                                                                                                                              |\n",
        "| :-------------- | :----------- | :------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| Application     | Data         | Application-specific headers might be added by the application itself.                 | Varies depending on the application protocol (e.g., HTTP headers, SMTP commands).                                                                                                   |\n",
        "| Transport       | Segment/Datagram | TCP header or UDP header is added to the Application Layer data.                      | Source and destination port numbers, sequence numbers, acknowledgment numbers, flags (TCP); source and destination port numbers, checksum (UDP).                                   |\n",
        "| Network (IP)    | Packet       | IP header is added to the TCP segment or UDP datagram.                                 | Source and destination IP addresses, protocol type, time-to-live (TTL).                                                                                                               |\n",
        "| Data Link       | Frame        | Data Link Layer header (and sometimes trailer) is added to the IP packet.              | Source and destination MAC addresses, error detection information (e.g., CRC).                                                                                                        |\n",
        "| Physical        | Bit          | Frame is converted into a stream of bits for transmission over the physical medium. | Physical layer specifications (e.g., voltage levels, timing).                                                                                                                            |\n",
        "\n",
        "At the receiving device, this encapsulation process is reversed (decapsulation). Each layer reads its header information, performs its designated function, and then removes the header (and trailer) before passing the remaining data up to the next layer in the stack. This layered approach ensures that each part of the communication process is handled by the appropriate layer, contributing to the overall functionality and reliability of network communication."
      ],
      "metadata": {
        "id": "h6sSXB1KjdpU"
      },
      "id": "h6sSXB1KjdpU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Images\n",
        "\n",
        "* Packet Tracer - https://www.computerhope.com/jargon/p/packet.\n",
        "* Wireshark - https://www.wireshark.org/docs/wsug_html_chunked/ChUseMainWindowSection.html\n",
        "* Encapsulation - http://www.tcpipguide.com/free/t_DataEncapsulationProtocolDataUnitsPDUsandServiceDa-2.htm#google_vignette"
      ],
      "metadata": {
        "id": "PaF33-vwlzX-"
      },
      "id": "PaF33-vwlzX-"
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scapy -q"
      ],
      "metadata": {
        "id": "-Rudw4NboZWl"
      },
      "id": "-Rudw4NboZWl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TCP\n",
        "\n",
        "**TCP (Transmission Control Protocol)** is a connection-oriented, reliable, and ordered transport layer protocol. It establishes a connection before data transfer, ensures all data arrives at the destination correctly and in the order it was sent, and provides mechanisms for error recovery and flow control.\n",
        "\n",
        "**Applications that use TCP:**\n",
        "\n",
        "* **World Wide Web (HTTP/HTTPS):** For browsing websites.\n",
        "* **Email (SMTP, POP3, IMAP):** For sending and receiving emails.\n",
        "* **File Transfer (FTP, SFTP):** For transferring files between systems.\n",
        "* **Secure Shell (SSH):** For secure remote command-line access.\n",
        "* **Telnet:** For unsecure remote command-line access.\n",
        "* **Databases (e.g., MySQL, PostgreSQL):** For client-server communication."
      ],
      "metadata": {
        "id": "KjxQsOzhpZkU"
      },
      "id": "KjxQsOzhpZkU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "metadata": {
        "id": "9Tya7YlPpezN"
      },
      "id": "9Tya7YlPpezN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. `###[ IP ]###`**\n",
        "\n",
        "The IP header is crucial for **routing packets across networks**. It contains the necessary information to get a packet from its source to its destination. Here's an explanation of each field:\n",
        "\n",
        "* **`version = 4`**:\n",
        "    * Specifies the **IP version** being used. In this case, `4` indicates **IPv4**, which is the most widely used version of the Internet Protocol. The next generation is IPv6, which would have a `version` of `6`.\n",
        "\n",
        "* **`ihl = None`**:\n",
        "    * **Internet Header Length**. This field indicates the size of the IP header in **32-bit words**. A standard IPv4 header without options is 20 bytes (5 x 32-bit words).\n",
        "    * The `None` value here usually means that Scapy will automatically calculate the correct IHL when the packet is sent or when a full representation of the packet is needed. If there were IP options present, the IHL would be greater than 5.\n",
        "\n",
        "* **`tos = 0x0`**:\n",
        "    * **Type of Service** (now more accurately referred to as the **Differentiated Services Field - DSCP** and Explicit Congestion Notification - ECN).\n",
        "    * This byte is used to prioritize or classify packets, allowing for different treatment based on their importance. `0x0` indicates the default or best-effort service with no specific prioritization.\n",
        "\n",
        "* **`len = None`**:\n",
        "    * **Total Length**. This field specifies the total size of the IP packet in bytes, including both the header and the data (payload) it carries.\n",
        "    * `None` here means Scapy will calculate the total length based on the size of the IP header and the encapsulated data (in this case, the TCP segment).\n",
        "\n",
        "* **`id = 1`**:\n",
        "    * **Identification**. This is a 16-bit integer that uniquely identifies each IP packet sent by a host. If an IP packet is fragmented (broken into smaller pieces) to traverse a network with a smaller Maximum Transmission Unit (MTU), all fragments of the original packet will have the same `id`. This allows the receiving host to reassemble the fragments correctly.\n",
        "\n",
        "* **`flags = `**:\n",
        "    * **IP Flags**. These are control bits that relate to fragmentation. The flags are:\n",
        "        * **Bit 0:** Reserved (must be zero).\n",
        "        * **Bit 1 (DF - Don't Fragment):** If set, this flag indicates that the packet should not be fragmented. If a router encounters a link with an MTU smaller than the packet size, the router will drop the packet and typically send an ICMP \"Fragmentation Needed\" message back to the sender.\n",
        "        * **Bit 2 (MF - More Fragments):** If set, this flag indicates that this packet is a fragment and more fragments of the original packet will follow. The last fragment will have this flag set to zero.\n",
        "    * An empty value here likely means that no fragmentation-related flags are set (both DF and MF are 0).\n",
        "\n",
        "* **`frag = 0`**:\n",
        "    * **Fragment Offset**. This 13-bit field indicates the position of the fragment's data relative to the beginning of the original, unfragmented packet. The offset is measured in units of 8 bytes. A value of `0` indicates that this is either an unfragmented packet or the first fragment.\n",
        "\n",
        "* **`ttl = 64`**:\n",
        "    * **Time To Live**. This 8-bit field limits the number of hops (routers) an IP packet can traverse on the network. Each router that processes the packet decrements the TTL value by at least one. When the TTL reaches zero, the packet is discarded to prevent routing loops from causing packets to circulate indefinitely. The initial TTL value is set by the sending host (often 64 or 128).\n",
        "\n",
        "* **`proto = tcp`**:\n",
        "    * **Protocol**. This 8-bit field indicates the next-level protocol encapsulated within the IP packet's data payload. Common values include:\n",
        "        * `tcp` (or `6`): Transmission Control Protocol\n",
        "        * `udp` (or `17`): User Datagram Protocol\n",
        "        * `icmp` (or `1`): Internet Control Message Protocol\n",
        "        * In this case, `tcp` indicates that the data being carried by this IP packet is a TCP segment (from the Transport Layer).\n",
        "\n",
        "* **`chksum = None`**:\n",
        "    * **Header Checksum**. This 16-bit field contains a checksum value calculated over the IP header. It's used for basic error detection to ensure the integrity of the header during transmission. If a router detects an error in the header using the checksum, it will discard the packet.\n",
        "    * `None` here means Scapy will calculate the correct checksum before sending the packet.\n",
        "\n",
        "* **`src = 192.168.1.100`**:\n",
        "    * **Source IP Address**. This 32-bit field contains the IP address of the sender of the packet.\n",
        "\n",
        "* **`dst = 192.168.1.101`**:\n",
        "    * **Destination IP Address**. This 32-bit field contains the IP address of the intended recipient of the packet.\n",
        "\n",
        "* **`\\options\\`**:\n",
        "    * **IP Options**. This is a variable-length field that can contain optional features for the IP protocol, such as record route, timestamp, or security options. In this case, the backslash indicates that there are no IP options present.\n",
        "\n",
        "In summary, the IP header provides the fundamental addressing and control information necessary for routing a packet from its source to its destination across an IP network. It identifies the source and destination, specifies how the packet should be handled (e.g., fragmentation, priority), and indicates the type of data being carried in its payload.\n",
        "\n",
        "**2. `###[ TCP ]###` Section:**\n",
        "\n",
        "* **`sport = 12345`**: Source port number. The application initiating the connection is using port 12345.\n",
        "* **`dport = 80`**: Destination port number. The service being requested (likely HTTP) is listening on port 80.\n",
        "* **`seq = 0`**: Sequence number. This is the initial sequence number (ISN) for the TCP connection establishment. It's used to track the order of data segments.\n",
        "* **`ack = 0`**: Acknowledgment number. In the SYN packet, the acknowledgment number is usually 0 as no data has been received yet. It will be used in subsequent packets to acknowledge received data.\n",
        "* **`dataofs = None`**: Data offset (number of 32-bit words in the TCP header). `None` means Scapy will calculate this.\n",
        "* **`reserved = 0`**: Reserved bits, which are usually set to 0.\n",
        "* **`flags = S`**: TCP flags. 'S' stands for SYN (Synchronize). This flag is set in the first packet of the TCP three-way handshake to initiate a connection. Other flags include 'A' (ACKnowledgment), 'F' (FINish), 'R' (ReSeT), 'P' (PUSH), and 'U' (URGent).\n",
        "* **`window = 8192`**: Window size. This indicates the amount of data the sender is willing to receive without an acknowledgment.\n",
        "* **`chksum = None`**: Checksum. Used for error detection of the TCP header and data. `None` means Scapy will calculate this.\n",
        "* **`urgptr = 0`**: Urgent pointer. This field is used when the URG flag is set to indicate the offset of urgent data within the segment.\n",
        "* **`\\options\\`**: TCP options can be included here (e.g., Maximum Segment Size - MSS). It's empty in this basic example.\n",
        "\n",
        "In summary, a TCP packet, as represented by Scapy, will have an IP layer for addressing and routing, and a TCP layer containing header fields specific to TCP's connection-oriented and reliable nature, including sequence numbers, acknowledgment numbers, flags for connection control, and window size for flow control. The raw data payload would follow the TCP header."
      ],
      "metadata": {
        "id": "Vjxr6wJBpnr-"
      },
      "id": "Vjxr6wJBpnr-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UDP\n",
        "\n",
        "**UDP (User Datagram Protocol)** is a connectionless and unreliable transport layer protocol. It sends data packets (datagrams) without establishing a connection beforehand and doesn't guarantee delivery, order, or error checking. This makes it faster and with lower overhead than TCP.\n",
        "\n",
        "**Applications that use UDP:**\n",
        "\n",
        "* **Domain Name System (DNS):** For looking up IP addresses from domain names.\n",
        "* **Streaming Media (e.g., video conferencing, online gaming):** Where speed is often prioritized over guaranteed delivery.\n",
        "* **Voice over IP (VoIP):** For real-time voice communication.\n",
        "* **Online Gaming:** For fast-paced data exchange where occasional packet loss is acceptable.\n",
        "* **Simple Network Management Protocol (SNMP):** For network management tasks.\n",
        "* **Trivial File Transfer Protocol (TFTP):** A simpler file transfer protocol."
      ],
      "metadata": {
        "id": "Pq4oXZrUpV1E"
      },
      "id": "Pq4oXZrUpV1E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "metadata": {
        "id": "g4wpufDYoT3Q"
      },
      "id": "g4wpufDYoT3Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. `###[ IP ]###`**\n",
        "\n",
        "This section defines the **Internet Protocol (IP) layer** of the packet. It contains the header fields for IPv4:\n",
        "\n",
        "* **`version = 4`**: Indicates that this is an IPv4 packet.\n",
        "* **`ihl = None`**: Internet Header Length. This field specifies the size of the IP header in 32-bit words. `None` usually means Scapy will calculate this automatically when the packet is sent or displayed in full detail.\n",
        "* **`tos = 0x0`**: Type of Service (now Differentiated Services Field - DSCP). This field is used to prioritize or classify packets. `0x0` indicates no specific prioritization.\n",
        "* **`len = None`**: Total length of the IP packet (header + data). `None` means Scapy will calculate this.\n",
        "* **`id = 1`**: Identification field. This is a sequence number that helps in reassembling fragmented IP packets at the destination.\n",
        "* **`flags = `**: IP flags. These flags control fragmentation. The empty value likely means no flags are set.\n",
        "* **`frag = 0`**: Fragment offset. If the packet is fragmented, this field indicates the position of the fragment within the original packet. `0` means it's either not fragmented or it's the first fragment.\n",
        "* **`ttl = 64`**: Time To Live. This field limits the number of hops a packet can take through routers to prevent routing loops. It's initialized to a value (often 64) and decremented by each router.\n",
        "* **`proto = 17`**: Protocol. This field indicates the next-level protocol encapsulated within the IP packet. `17` corresponds to **UDP (User Datagram Protocol)**.\n",
        "* **`chksum = None`**: Header checksum. This field is used for error detection of the IP header. `None` means Scapy will calculate this.\n",
        "* **`src = 192.168.1.100`**: Source IP address of the packet.\n",
        "* **`dst = 192.168.1.102`**: Destination IP address of the packet.\n",
        "* **`\\options \\`**: This section would contain any IP options, but it's empty here.\n",
        "\n",
        "**2. `###[ UDP ]###`**\n",
        "\n",
        "This section defines the **User Datagram Protocol (UDP) layer**, which is encapsulated within the IP packet (as indicated by `proto = 17` in the IP header).\n",
        "\n",
        "* **`sport = 5000`**: Source port number. The application on the source host that sent this UDP datagram is using port 5000.\n",
        "* **`dport = 53`**: Destination port number. The application on the destination host that should receive this UDP datagram is listening on port 53, which is the standard port for **DNS (Domain Name System)** queries.\n",
        "* **`len = None`**: Length of the UDP datagram (header + data). `None` means Scapy will calculate this.\n",
        "* **`chksum = None`**: Checksum. This field is used for error detection of the UDP header and data. `None` means Scapy will calculate this.\n",
        "\n",
        "**3. `###[ Raw ]###`**\n",
        "\n",
        "This section represents the **raw payload** of the UDP datagram. It's the actual data being transmitted by the application.\n",
        "\n",
        "* **`load = b'This is a simulated UDP packet.'`**: This is the raw byte string payload. The `b'` prefix indicates a byte literal in Python. This is the data that the DNS application (listening on port 53 at the destination IP) would receive.\n",
        "\n",
        "**4. `'IP / UDP 192.168.1.100:5000 > 192.168.1.102:53 / Raw'`**\n",
        "\n",
        "This is a concise **summary** of the created packet, also provided by Scapy. It shows the layering and key information:\n",
        "\n",
        "* **`IP`**: Indicates the outermost layer is IP.\n",
        "* **`/`**: Separator indicating encapsulation.\n",
        "* **`UDP`**: Indicates the next layer encapsulated within IP is UDP.\n",
        "* **`192.168.1.100:5000`**: Source IP address and source port number.\n",
        "* **`>`**: Indicates the direction of the packet (from source to destination).\n",
        "* **`192.168.1.102:53`**: Destination IP address and destination port number.\n",
        "* **`/`**: Separator.\n",
        "* **`Raw`**: Indicates the raw payload being carried by the UDP datagram.\n",
        "\n",
        "**In essence, this Scapy output describes a simulated network packet that:**\n",
        "\n",
        "* Originates from IP address `192.168.1.100` and UDP port `5000`.\n",
        "* Is destined for IP address `192.168.1.102` and UDP port `53` (likely a DNS server).\n",
        "* Contains a raw payload of the text \"This is a simulated UDP packet.\"\n",
        "\n",
        "Scapy allows you to construct and manipulate packets at this level, making it a powerful tool for network analysis, testing, and security exploration. When you would send this packet using Scapy's `send()` function, the library would typically fill in the `None` values (like `ihl`, `len`, and `chksum`) before transmission."
      ],
      "metadata": {
        "id": "apc5wSaepI-k"
      },
      "id": "apc5wSaepI-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TCP/IP Network Anomalies\n",
        "\n",
        "TCP/IP network anomalies are unusual patterns or behaviors in network traffic that deviate significantly from the expected or normal baseline of communication using the TCP/IP protocol suite. These anomalies can be caused by various factors, including:\n",
        "\n",
        "* **Security Threats:** Intrusions, malware activity, denial-of-service (DoS) attacks, port scanning, and unauthorized access attempts.\n",
        "* **Network Issues:** Equipment malfunction, misconfigurations, congestion, routing problems, and broadcast storms.\n",
        "* **Policy Violations:** Unauthorized use of bandwidth, forbidden protocols, or access to restricted resources.\n",
        "* **Operational Errors:** Mistakes in configuration or maintenance leading to unusual traffic patterns.\n",
        "\n",
        "**Here are some examples of TCP/IP network anomalies:**\n",
        "\n",
        "**Traffic Volume Anomalies:**\n",
        "\n",
        "* **Sudden Spike in Traffic:** An unexpected and significant increase in network traffic to a specific host or across the entire network, potentially indicating a DoS attack or a compromised host participating in a botnet.\n",
        "* **Unusual Drop in Traffic:** A sudden and significant decrease in traffic, which could indicate a network outage, a failing server, or a targeted attack disrupting communication.\n",
        "* **Excessive Bandwidth Usage:** A host consuming far more bandwidth than its typical usage, potentially due to malware uploading data or a user engaging in unauthorized file sharing.\n",
        "\n",
        "**Packet Characteristic Anomalies:**\n",
        "\n",
        "* **Unusual Packet Sizes:** Packets that are significantly larger or smaller than the typical packet size for a particular type of communication. This could indicate fragmentation attacks or tunneling.\n",
        "* **Irregular Packet Rate:** A sudden increase or decrease in the number of packets being transmitted, which might signal a DoS attack or network congestion.\n",
        "* **Abnormal TCP Flags:** Unexpected or illogical combinations of TCP flags (SYN, ACK, FIN, RST, URG, PSH) in packets. For example, multiple SYN packets without corresponding ACK responses (SYN flood attack) or RST packets for established connections without a clear reason.\n",
        "* **Out-of-Order Packets:** An unusually high number of TCP packets arriving out of sequence, potentially indicating network issues or an attacker trying to reorder packets.\n",
        "* **Retransmission Anomalies:** An excessive number of retransmitted TCP segments, suggesting network congestion or unreliable connections.\n",
        "\n",
        "**Connection and Flow Anomalies:**\n",
        "\n",
        "* **High Number of Connections:** A host establishing an unusually large number of connections to different destinations, which could be a sign of malware spreading or a port scan.\n",
        "* **Connections to Unusual Ports:** Communication attempts to ports that are not typically used by standard services, potentially indicating malware communication or reconnaissance activities.\n",
        "* **Connections from Unusual Source IPs:** Traffic originating from IP addresses that are not normally associated with the internal network or trusted external partners.\n",
        "* **Long-Duration Connections with Little Data Transfer:** Connections that remain open for extended periods without significant data exchange, which could be indicative of idle botnet connections.\n",
        "* **Failed Connection Attempts:** A high number of unsuccessful connection attempts to a specific host or service, potentially indicating a DoS attack or a misconfigured service.\n",
        "\n",
        "**Protocol-Specific Anomalies:**\n",
        "\n",
        "* **HTTP Anomalies:** Unusual user-agent strings, abnormal request methods, or excessive requests for specific resources.\n",
        "* **DNS Anomalies:** Sudden spikes in DNS queries, queries for unusual domain names, or a high number of failed DNS resolutions.\n",
        "* **SMTP Anomalies:** Large volumes of outgoing emails, emails sent to unusual recipients, or emails originating from compromised internal hosts.\n",
        "\n",
        "Detecting these TCP/IP network anomalies is crucial for maintaining network security, performance, and reliability. Machine learning techniques, as you are teaching, can be very effective in identifying these deviations from normal behavior by learning patterns in network traffic data."
      ],
      "metadata": {
        "id": "PdG-7xqB_xwA"
      },
      "id": "PdG-7xqB_xwA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest vs Isolation Forest\n",
        "\n",
        "**Random Forest** is a **supervised** algorithm used for **classification and regression**. It learns relationships between features and a target variable from labeled data to make predictions. It builds an ensemble of deep decision trees.\n",
        "\n",
        "**Isolation Forest** is an **unsupervised** algorithm used for **anomaly detection**. It identifies outliers by isolating them in shallow decision trees built by random partitioning. It doesn't require labeled anomaly data.\n",
        "\n",
        "**Key Difference:** Random Forest predicts a known outcome, while Isolation Forest identifies the unusual without prior knowledge of what's anomalous."
      ],
      "metadata": {
        "id": "KVtLtthT907g"
      },
      "id": "KVtLtthT907g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised learning** uses **labeled data** (input features paired with correct output targets) to train models for prediction or classification. The model learns the mapping between inputs and outputs.\n",
        "\n",
        "**Unsupervised learning** uses **unlabeled data** to find hidden patterns, structures, or groupings within the data itself, without any predefined target variables."
      ],
      "metadata": {
        "id": "JFtsFVglrgNl"
      },
      "id": "JFtsFVglrgNl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Decision Tree\n",
        "\n",
        "A **decision tree** is a tree-like supervised learning model used for both classification and regression. It makes decisions by recursively splitting the data based on the values of input features, aiming to create homogeneous subsets with respect to the target variable. Each internal node represents a feature test, each branch represents the outcome of the test, and each leaf node represents a prediction (class label or numerical value)."
      ],
      "metadata": {
        "id": "okItANjDn4Jm"
      },
      "id": "okItANjDn4Jm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                      Chillin with some grillin?\n",
        "                                         Is it cloudy today?\n",
        "                                            (Root Node)\n",
        "                                                /\\\n",
        "                                           yes /  \\ no\n",
        "                                              /    \\\n",
        "                                         chance    grillin\n",
        "                                        of rain?      \n",
        "                                            /\\\n",
        "                                       yes /  \\ no\n",
        "                                          /    \\\n",
        "                                  no grillin     grillin"
      ],
      "metadata": {
        "id": "VTDfoHRYvmgy"
      },
      "id": "VTDfoHRYvmgy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy\n",
        "\n",
        "$H(S) = -p_1 \\log_2(p_1) - p_0 \\log_2(p_0)$\n",
        "\n",
        "The entropy of a set S, denoted as H(S), is calculated by taking the negative of the probability of the first class (p₁) multiplied by the base-2 logarithm of that probability, and then subtracting the probability of the second class (p₀) multiplied by the base-2 logarithm of that probability."
      ],
      "metadata": {
        "id": "HSIt1tmJCSoN"
      },
      "id": "HSIt1tmJCSoN"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # We'll need numpy for the logarithm base 2\n",
        "\n",
        "def entropy(target_col):\n",
        "    \"\"\"Calculates the entropy of a target variable with binary classes.\"\"\"\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    probabilities = counts / len(target_col) # Calculates p1 and p0 (probabilities of each class)\n",
        "    entropy_value = 0\n",
        "    for prob in probabilities:\n",
        "        if prob > 0:  # Avoid log(0) error\n",
        "            entropy_value -= prob * np.log2(prob) # Calculates -p * log2(p) for each class\n",
        "\n",
        "    return entropy_value # Returns H(S)"
      ],
      "metadata": {
        "id": "vJyu4w3wCG5_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vJyu4w3wCG5_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Gain\n",
        "\n",
        "$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$\n",
        "\n",
        "The information gain of splitting a dataset S on an attribute A, denoted as IG(S, A), is determined by taking the initial entropy of the dataset S, represented as H(S), and subtracting the sum of the entropies of each subset Sv created by splitting on the different values (v) of attribute A. Each of these subset entropies, H(Sv), is weighted by the proportion of the number of elements in that subset (|Sv|) relative to the total number of elements in the original dataset (|S|)."
      ],
      "metadata": {
        "id": "bPE0s4iiCfAb"
      },
      "id": "bPE0s4iiCfAb"
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(data, split_attribute, target_attribute):\n",
        "    \"\"\"Calculates the information gain of splitting on an attribute.\"\"\"\n",
        "    # Total entropy of the target variable\n",
        "    total_entropy = entropy(data[target_attribute]) # H(S)\n",
        "\n",
        "    # Unique values in the split attribute\n",
        "    values = data[split_attribute].unique()\n",
        "    weighted_entropy = 0\n",
        "\n",
        "    for value in values:\n",
        "        subset = data[data[split_attribute] == value]\n",
        "        subset_size = len(subset)\n",
        "        if subset_size > 0:\n",
        "            subset_entropy = entropy(subset[target_attribute]) # H(Sv)\n",
        "            weighted_entropy += (subset_size / len(data)) * subset_entropy # (|Sv| / |S|) * H(Sv)\n",
        "\n",
        "    # Information Gain\n",
        "    gain = total_entropy - weighted_entropy # IG(S, A) = H(S) - Σ (|Sv| / |S|) * H(Sv)\n",
        "    return gain"
      ],
      "metadata": {
        "id": "Xc5Mlb3hCQcG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Xc5Mlb3hCQcG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gini Impurity\n",
        "\n",
        "$Gini(S) = 1 - (p_1^2 + p_0^2)$\n",
        "\n",
        "The Gini impurity of a set S, denoted as Gini(S), is calculated by taking one and subtracting the sum of the squared probabilities of each class. For a binary classification problem, this means subtracting the square of the probability of the first class (p₁) and the square of the probability of the second class (p₀) from one."
      ],
      "metadata": {
        "id": "uZC9mekzCwr1"
      },
      "id": "uZC9mekzCwr1"
    },
    {
      "cell_type": "code",
      "source": [
        "def gini(target_col):\n",
        "    \"\"\"Calculates the Gini impurity of a target variable with binary classes.\"\"\"\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    probabilities = counts / len(target_col) # Calculates p1 and p0 (probabilities of each class)\n",
        "    gini_value = 1 - np.sum(probabilities**2) # Calculates 1 - (p1^2 + p0^2)\n",
        "    return gini_value # Returns Gini(S)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hxmIsLgSCwr2"
      },
      "id": "hxmIsLgSCwr2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gini Gain (Reduction in Gini Impurity)\n",
        "\n",
        "$Gini\\_Gain(S, A) = Gini(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Gini(S_v)$\n",
        "\n",
        "The Gini gain achieved by splitting a dataset S on an attribute A, denoted as Gini\\_Gain(S, A), is calculated by taking the initial Gini impurity of the dataset S, represented as Gini(S), and subtracting the weighted sum of the Gini impurities of each subset Sv created by splitting on the different values (v) of attribute A. Each subset's Gini impurity, Gini(Sv), is weighted by the proportion of the number of elements in that subset (|Sv|) relative to the total number of elements in the original dataset (|S|)."
      ],
      "metadata": {
        "id": "f7Z-QMMhDHk9"
      },
      "id": "f7Z-QMMhDHk9"
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_gain(data, split_attribute, target_attribute):\n",
        "    \"\"\"Calculates the Gini gain of splitting on an attribute.\"\"\"\n",
        "    # Total Gini impurity of the target variable\n",
        "    total_gini = gini(data[target_attribute]) # Gini(S)\n",
        "\n",
        "    # Unique values in the split attribute\n",
        "    values = data[split_attribute].unique()\n",
        "    weighted_gini = 0\n",
        "\n",
        "    for value in values:\n",
        "        subset = data[data[split_attribute] == value]\n",
        "        subset_size = len(subset)\n",
        "        if subset_size > 0:\n",
        "            subset_gini = gini(subset[target_attribute]) # Gini(Sv)\n",
        "            weighted_gini += (subset_size / len(data)) * subset_gini # (|Sv| / |S|) * Gini(Sv)\n",
        "\n",
        "    # Gini Gain\n",
        "    gain = total_gini - weighted_gini # Gini_Gain(S, A) = Gini(S) - Σ (|Sv| / |S|) * Gini(Sv)\n",
        "    return gain"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-6-SYI3TDHk-"
      },
      "id": "-6-SYI3TDHk-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "**Our Goal:** To understand how Information Gain, Gini Impurity, and Entropy guide the splitting of a decision tree when the target variable has three distinct classes.\n",
        "\n",
        "**Scenario:** Imagine we have a dataset about different types of fruits based on their color, size, shape, and texture, and our target variable is the fruit type (Apple, Banana, Orange)."
      ],
      "metadata": {
        "id": "EJaMs7jLxF-g"
      },
      "id": "EJaMs7jLxF-g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting up the Environment and Initial Data"
      ],
      "metadata": {
        "id": "_VfAMuWxBVn9"
      },
      "id": "_VfAMuWxBVn9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "data = {\n",
        "    'Fur': np.random.choice(['Furry', 'Not Furry'], size=12),\n",
        "    'Legs': np.random.choice([4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2], size=12), # Bias towards 4 legs for cats/dogs\n",
        "    'Sound': np.random.choice(['Meow', 'Woof', 'Hiss', 'Meow', 'Woof', 'Hiss', 'Meow', 'Woof', 'Hiss', 'Meow', 'Woof', 'Hiss'], size=12),\n",
        "    'Tail': np.random.choice(['Long', 'Short', 'None', 'Long', 'Short', 'None', 'Long', 'Short', 'None', 'Long', 'Short', 'None'], size=12),\n",
        "    'Animal': ['Cat', 'Dog', 'Turtle', 'Cat', 'Dog', 'Turtle', 'Cat', 'Dog', 'Turtle', 'Cat', 'Dog', 'Turtle']\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "nIfL1K8kxF-i"
      },
      "id": "nIfL1K8kxF-i"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "metadata": {
        "id": "wk6n-wyyBwmR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wk6n-wyyBwmR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Entropy\n",
        "\n",
        "**What is Entropy?**\n",
        "\n",
        "Entropy is a measure of the impurity, disorder, or randomness in a set of data. In the context of information theory, it quantifies the average amount of information needed to describe the outcome of a random event. In machine learning, particularly with decision trees, entropy is used to measure the impurity of a dataset with respect to the target variable.\n",
        "\n",
        "A pure dataset (where all data points belong to the same class) has an entropy of 0. A dataset with an equal distribution of classes has maximum entropy.\n",
        "\n",
        "**Why Use Entropy with the Target Variable?**\n",
        "\n",
        "In decision trees, we use entropy to determine the \"best\" way to split the data at each node. The goal is to create splits that result in subsets of data that are as pure as possible (with respect to the target variable).\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1.  **Initial Entropy:** We first calculate the entropy of the entire dataset's target variable. This gives us a baseline measure of the impurity before any splits are made.\n",
        "\n",
        "2.  **Splitting and Information Gain:**\n",
        "    * We then consider different features and potential split points within those features.\n",
        "    * For each potential split, we calculate the entropy of the resulting subsets (after the split).\n",
        "    * We compare the entropy of the original dataset to the weighted average of the entropies of the subsets. The reduction in entropy achieved by the split is called **information gain**.\n",
        "\n",
        "3.  **Best Split Selection:** The feature and split point that yield the highest information gain (i.e., the greatest reduction in entropy) are chosen for that node in the decision tree.\n",
        "\n",
        "4.  **Recursion:** This process is repeated recursively for each subset, building the tree until a stopping criterion is met (e.g., a subset is pure, or a maximum tree depth is reached).\n",
        "\n",
        "**In essence, we use entropy to guide the tree construction process, favoring splits that create more homogeneous subsets, making the tree more effective at classifying or predicting the target variable.**\n",
        "\n",
        "**Entropy Formula:**\n",
        "\n",
        "The formula for entropy (H) for a set S is:\n",
        "\n",
        "$$H(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* \\(H(S)\\) is the entropy of the dataset S.\n",
        "* \\(c\\) is the number of distinct classes in the target variable.\n",
        "* \\(p_i\\) is the proportion (probability) of data points belonging to class i in the dataset S.\n",
        "\n",
        "The logarithm is base 2 because we often deal with bits of information. By minimizing entropy, we maximize the information gained by each split, leading to a more efficient and accurate decision tree.\n"
      ],
      "metadata": {
        "id": "j8Yb2AGnBgSX"
      },
      "id": "j8Yb2AGnBgSX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rySLWAo2xF-k"
      },
      "id": "rySLWAo2xF-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Interpret the entropy value in the context of three classes. A higher value still indicates more impurity or a more even distribution of the fruit types.)**"
      ],
      "metadata": {
        "id": "wL3mBsfQxF-k"
      },
      "id": "wL3mBsfQxF-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Information Gain\n",
        "\n",
        "Okay, let's explain information gain and how it's used to select the best feature for the first split in a decision tree.\n",
        "\n",
        "**What is Information Gain?**\n",
        "\n",
        "Information gain measures the reduction in entropy achieved by partitioning the dataset based on a particular feature. In simpler terms, it tells us how much \"cleaner\" or more organized the data becomes after we split it according to that feature. A higher information gain indicates a more effective split, as it means the resulting subsets are more homogeneous with respect to the target variable.\n",
        "\n",
        "**How We Use Information Gain to Find the Best Feature for the First Split**\n",
        "\n",
        "1.  **Calculate Initial Entropy:** First, as you've already done, we calculate the entropy of the target variable for the entire dataset. This is the entropy before any splitting is performed.\n",
        "\n",
        "2.  **Calculate Entropy for Each Feature:**\n",
        "    * For each feature in the dataset:\n",
        "        * We determine the unique values of that feature.\n",
        "        * For each unique value of the feature, we create a subset of the data containing only the rows where the feature has that value.\n",
        "        * We calculate the entropy of the target variable for each of these subsets.\n",
        "\n",
        "3.  **Calculate Weighted Average Entropy for Each Feature:**\n",
        "    * For each feature, we calculate the weighted average entropy of its subsets. The weight for each subset's entropy is the proportion of data points that belong to that subset.\n",
        "\n",
        "4.  **Calculate Information Gain for Each Feature:**\n",
        "    * For each feature, we calculate the information gain by subtracting the weighted average entropy of that feature's subsets from the initial entropy of the entire dataset.\n",
        "\n",
        "5.  **Select the Feature with the Highest Information Gain:**\n",
        "    * The feature with the highest information gain is chosen as the feature for the first split. This is because it provides the greatest reduction in impurity and thus the most informative split.\n",
        "\n",
        "**Information Gain Equation:**\n",
        "\n",
        "The formula for information gain (IG) is:\n",
        "\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* \\(IG(S, A)\\) is the information gain of splitting the dataset \\(S\\) on feature \\(A\\).\n",
        "* \\(H(S)\\) is the entropy of the original dataset \\(S\\).\n",
        "* \\(Values(A)\\) is the set of all possible values of feature \\(A\\).\n",
        "* \\(|S_v|\\) is the number of data points in the subset \\(S_v\\) where feature \\(A\\) has value \\(v\\).\n",
        "* \\(|S|\\) is the total number of data points in the dataset \\(S\\).\n",
        "* \\(H(S_v)\\) is the entropy of the subset \\(S_v\\).\n",
        "\n",
        "1.  **`information_gain(data, split_feature, target_feature)` function:**\n",
        "    * Takes the DataFrame (`data`), the feature to split on (`split_feature`), and the target feature (`target_feature`) as input.\n",
        "    * Calculates the `total_entropy` (entropy of the target variable in the whole dataset).\n",
        "    * Finds the unique values of the `split_feature` and their counts.\n",
        "    * Iterates through each unique value of the `split_feature`:\n",
        "        * Creates a `subset` of the data where the `split_feature` equals the current value.\n",
        "        * Calculates the probability (`subset_prob`) of that subset.\n",
        "        * Calculates the entropy of the `target_feature` in the `subset`.\n",
        "        * Adds the weighted entropy of the subset to `weighted_entropy`.\n",
        "    * Returns the information gain by subtracting the `weighted_entropy` from the `total_entropy`.\n",
        "\n",
        "2.  **Calculate Information Gain for Each Feature:**\n",
        "    * We iterate through all the features in the DataFrame (except the target variable 'Animal').\n",
        "    * We call the `information_gain` function for each feature to calculate its information gain.\n",
        "    * We store the information gains in a dictionary `information_gains`.\n",
        "    * We print the information gain for each feature.\n",
        "\n",
        "3.  **Find the Feature with the Highest Information Gain:**\n",
        "    * We use the `max()` function with the `key` argument to find the feature with the maximum information gain from the `information_gains` dictionary.\n",
        "    * We print the name of the `best_feature`.\n",
        "\n",
        "This code will calculate the information gain for each feature and tell you which feature would be the best choice for the first split in a decision tree based on maximizing information gain."
      ],
      "metadata": {
        "id": "8sjyoM5JEBrM"
      },
      "id": "8sjyoM5JEBrM"
    },
    {
      "cell_type": "code",
      "source": [
        "# information_gains = {}\n",
        "# for feature in df.columns:\n",
        "#     if feature != 'Animal':  # Don't calculate gain for the target variable itself\n",
        "#         information_gains[feature] = information_gain(df, feature, 'Animal')\n",
        "#         print(f\"Information Gain for '{feature}': {information_gains[feature]:.4f}\")\n",
        "\n",
        "# # Find the Feature with the Highest Information Gain\n",
        "# best_feature = max(information_gains, key=information_gains.get)\n",
        "# print(f\"\\nBest feature to split on: '{best_feature}'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KGcy7zkAxF-l"
      },
      "id": "KGcy7zkAxF-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Which feature seems most promising for the first split based on these values.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We iterate through each of our features.\n",
        "* For each feature, we use our `information_gain` function (which we defined earlier and works for multi-class targets) to calculate how much the entropy of 'Fruit' is reduced by splitting on that feature.\n",
        "* The output shows the Information Gain for each potential first split. The feature with the highest Information Gain would be chosen as the root node."
      ],
      "metadata": {
        "id": "XvdXES0FxF-l"
      },
      "id": "XvdXES0FxF-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Recursion"
      ],
      "metadata": {
        "id": "qbj8PeqKpBKn"
      },
      "id": "qbj8PeqKpBKn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three common methods for generating the Fibonacci sequence in Python are using a loop (iteration), recursion, and memoization (dynamic programming).\n",
        "\n",
        "**1. Iteration:**\n",
        "\n",
        "   - **Approach:** This method uses a `for` or `while` loop to compute the Fibonacci numbers sequentially.\n",
        "   - **Process:** Initialize the first two Fibonacci numbers (0 and 1), and then iteratively calculate the next number by adding the previous two.\n",
        "   - **Example:**"
      ],
      "metadata": {
        "id": "4jaq3LpDmn3G"
      },
      "id": "4jaq3LpDmn3G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "TwVDWNvrmn3H"
      },
      "id": "TwVDWNvrmn3H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Recursion:**\n",
        "\n",
        "   - **Approach:** This method defines a function that calls itself to compute the Fibonacci numbers.\n",
        "   - **Process:** Define base cases for the first two Fibonacci numbers (0 and 1), and then recursively define the function for the next number by adding the previous two.\n",
        "   - **Example:**"
      ],
      "metadata": {
        "id": "X0KgRCHkmn3I"
      },
      "id": "X0KgRCHkmn3I"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9DKpGN3pmn3I"
      },
      "id": "9DKpGN3pmn3I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Memoization:**\n",
        "\n",
        "Memoization is an optimization technique that speeds up code by storing the results of expensive function calls and reusing them when the same inputs occur again. It essentially creates a cache of function results to avoid redundant computations.\n",
        "\n",
        "   - **Approach:** This method combines recursion with dynamic programming (memoization) to optimize the calculation.\n",
        "   - **Process:** Store the results of previous Fibonacci number calculations in a cache (dictionary or list) to avoid recomputing them.\n",
        "   - **Example:**"
      ],
      "metadata": {
        "id": "57fNck2xmn3J"
      },
      "id": "57fNck2xmn3J"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along"
      ],
      "metadata": {
        "id": "qN0ABsYLl7Tk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qN0ABsYLl7Tk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Understanding Gini Impurity\n",
        "\n",
        "For a multi-class target with probabilities $p_1, p_2, ..., p_n$, the Gini Impurity is:\n",
        "\n",
        "$$Gini(S) = 1 - \\sum_{i=1}^{n} p_i^2$$\n",
        "\n",
        "\n",
        "**Step 1: Splitting on \"Sound\"**\n",
        "\n",
        "We've (hypothetically) determined that \"Sound\" is the best feature to split on (using information gain across all features). Now let's see how Gini impurity is used to evaluate this split.\n",
        "\n",
        "1.  **Unique Values of \"Sound\":** The unique values in the 'Sound' column are 'Meow', 'Woof', and 'Hiss'.\n",
        "\n",
        "2.  **Create Subsets:** We split the DataFrame into three subsets based on these values:\n",
        "\n"
      ],
      "metadata": {
        "id": "4fkp-lHIEYDP"
      },
      "id": "4fkp-lHIEYDP"
    },
    {
      "cell_type": "code",
      "source": [
        "# subset_meow = df[df['Sound'] == 'Meow']\n",
        "# subset_woof = df[df['Sound'] == 'Woof']\n",
        "# subset_hiss = df[df['Sound'] == 'Hiss']"
      ],
      "metadata": {
        "id": "t5tSTC--4Hza"
      },
      "id": "t5tSTC--4Hza",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Calculate Gini Impurity of Subsets:** We calculate the Gini impurity of the 'Animal' column in each subset:"
      ],
      "metadata": {
        "id": "4VwjwJH64DqT"
      },
      "id": "4VwjwJH64DqT"
    },
    {
      "cell_type": "code",
      "source": [
        "# gini_meow = gini(subset_meow['Animal'])\n",
        "# gini_woof = gini(subset_woof['Animal'])\n",
        "# gini_hiss = gini(subset_hiss['Animal'])\n",
        "\n",
        "# print(f\"Gini Impurity for 'Meow' subset: {gini_meow:.4f}\")\n",
        "# print(f\"Gini Impurity for 'Woof' subset: {gini_woof:.4f}\")\n",
        "# print(f\"Gini Impurity for 'Hiss' subset: {gini_hiss:.4f}\")"
      ],
      "metadata": {
        "id": "E5mNpTug4VvV"
      },
      "id": "E5mNpTug4VvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  **Calculate Weighted Average Gini Impurity:** We calculate the weighted average Gini impurity of the split:"
      ],
      "metadata": {
        "id": "slqB0X9M4REn"
      },
      "id": "slqB0X9M4REn"
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted_gini_sound = (\n",
        "#     (len(subset_meow) / len(df)) * gini_meow +\n",
        "#     (len(subset_woof) / len(df)) * gini_woof +\n",
        "#     (len(subset_hiss) / len(df)) * gini_hiss\n",
        "# )\n",
        "# print(f\"Weighted Average Gini Impurity for 'Sound' Split: {weighted_gini_sound:.4f}\")"
      ],
      "metadata": {
        "id": "dd7G1LuN4lW9"
      },
      "id": "dd7G1LuN4lW9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Do We Calculate the Weighted Gini of \"Sound\"?**\n",
        "\n",
        "   * We calculate the weighted Gini impurity of \"Sound\" (and any other potential splitting feature) to evaluate the **quality** of that specific split. We're not looking for a single \"total Gini score\" for the whole process. Instead, we're comparing the Gini impurity *before* the split to the *expected Gini impurity after* the split.\n",
        "   * The goal is to see how much the split reduces the impurity in the data. A greater reduction is better.\n",
        "   * It's a Gini impurity value *specific to a particular split based on a particular feature*.\n",
        "   * We have:\n",
        "        * `initial_gini`: Gini impurity of the original dataset (before any split).\n",
        "        * `weighted_gini_sound`: The *expected* Gini impurity *after* we split the dataset based on the \"Sound\" feature.\n",
        "\n",
        "**What Does \"Weighted\" Mean?**\n",
        "\n",
        "   * \"Weighted\" in \"weighted Gini impurity\" means that the Gini impurity of each resulting subset is given a weight based on the proportion of data points that fall into that subset.\n",
        "   * **Why weight?** Because subsets can have different sizes. We want to give more importance to the Gini impurity of larger subsets, as they represent a larger portion of the data.\n",
        "\n",
        "**Illustrative Example to Explain \"Weighted\"**\n",
        "\n",
        "   Imagine we have 100 data points, and we're considering a split that creates two subsets:\n",
        "\n",
        "   * Subset A: 90 data points, Gini impurity = 0.1\n",
        "   * Subset B: 10 data points, Gini impurity = 0.5\n",
        "\n",
        "   If we simply averaged the Gini impurities (0.1 + 0.5) / 2 = 0.3, it wouldn't accurately reflect the overall impurity after the split. Subset A is much larger and purer, so it should contribute more to our overall measure of impurity.\n",
        "\n",
        "   The weighted average Gini impurity would be:\n",
        "\n",
        "   (90/100) \\* 0.1 + (10/100) \\* 0.5 = 0.09 + 0.05 = 0.14\n",
        "\n",
        "   This shows that the split gives us a relatively pure result overall because most of the data (90%) is in a pure subset.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "   * We calculate the weighted Gini impurity for each *potential split* to compare the effectiveness of different splits.\n",
        "   * It's a *split-specific* score.\n",
        "   * \"Weighted\" means we give more importance to larger subsets, which is crucial for accurately assessing the overall impurity after a split."
      ],
      "metadata": {
        "id": "Zb89bdu15CsP"
      },
      "id": "Zb89bdu15CsP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  **Lowest weighted Gini Impurity:** We're aiming for the smallest amount of impurity *after* the split.\n",
        "\n",
        "   * **Purity = Better Predictions:** Our goal is to create subsets of data that are \"pure\" with respect to the target variable. A pure subset contains mostly or only data points belonging to a single class. Pure subsets make it easier to make accurate predictions.\n",
        "   * **Impurity = Uncertainty:** High impurity means the data is mixed, and it's hard to predict the class of a data point in that subset. Low impurity means the data is organized, and prediction is more certain.\n",
        "   * **Reducing Uncertainty:** By choosing the split that results in the greatest reduction in impurity (or the lowest weighted average impurity), we are effectively reducing the uncertainty about the target variable. This leads to a decision tree that can make more accurate classifications or predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "shkYsMHk4mZn"
      },
      "id": "shkYsMHk4mZn"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# def calculate_weighted_gini(data, split_feature, target_feature):\n",
        "#     values = data[split_feature].unique()\n",
        "#     weighted_gini = 0\n",
        "#     for value in values:\n",
        "#         subset = data[data[split_feature] == value]\n",
        "#         subset_prob = len(subset) / len(data)\n",
        "#         weighted_gini += subset_prob * gini(subset[target_feature])\n",
        "#         print(f\"  Gini Impurity for '{value}': {gini(subset[target_feature]):.4f}\")\n",
        "#     return weighted_gini\n",
        "\n",
        "# def find_best_split(data, features, target):\n",
        "#     best_feature = None\n",
        "#     min_weighted_gini = 1.0  # Initialize with maximum impurity\n",
        "\n",
        "#     for feature in features:\n",
        "#         weighted_gini = calculate_weighted_gini(data, feature, target)\n",
        "#         print(f\"Weighted Gini for '{feature}': {weighted_gini:.4f}\\n\")\n",
        "#         if weighted_gini < min_weighted_gini:\n",
        "#             min_weighted_gini = weighted_gini\n",
        "#             best_feature = feature\n",
        "\n",
        "#     return best_feature\n",
        "\n",
        "# def build_tree_recursive(data, features, target, depth=0):\n",
        "#     \"\"\"Simplified recursive tree builder (for illustration).\"\"\"\n",
        "\n",
        "#     print(f\"\\n\\n--- Depth: {depth} ---\")\n",
        "#     print(f\"Data:\\n{data}\")\n",
        "#     print(f\"Features: {features}\\n\\n\")\n",
        "\n",
        "#     # Stopping condition (simplified)\n",
        "#     if len(np.unique(data[target])) == 1 or depth > 2 or not features:\n",
        "#         print(f\"  --> Leaf Node: {np.unique(data[target])[0] if len(np.unique(data[target])) == 1 else 'Mixed'}\")\n",
        "#         return\n",
        "\n",
        "#     best_feature = find_best_split(data, features, target)\n",
        "#     print(f\"  Best Feature: {best_feature}\")\n",
        "\n",
        "#     unique_values = data[best_feature].unique()\n",
        "#     print(f\"  Unique Values: {unique_values}\")\n",
        "#     remaining_features = [f for f in features if f != best_feature]\n",
        "\n",
        "#     for value in unique_values:\n",
        "#         subset = data[data[best_feature] == value]\n",
        "#         print(f\"  Branch: {best_feature} = {value}\")\n",
        "#         build_tree_recursive(subset, remaining_features, target, depth + 1)  # Recursive call!\n",
        "\n",
        "# # Initial call to start the tree building\n",
        "# build_tree_recursive(df, df.columns.drop('Animal').tolist(), 'Animal')"
      ],
      "metadata": {
        "id": "0lbibalQ-9vi"
      },
      "id": "0lbibalQ-9vi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Features: ['Fur', 'Legs', 'Sound', 'Tail']\n",
        "\n",
        "\n",
        "  Gini Impurity for 'Furry': 0.6667\n",
        "  Gini Impurity for 'Not Furry': 0.6667\n",
        "Weighted Gini for 'Fur': 0.6667\n",
        "\n",
        "  Gini Impurity for '2': 0.5600\n",
        "  Gini Impurity for '4': 0.6122\n",
        "Weighted Gini for 'Legs': 0.5905\n",
        "\n",
        "  Gini Impurity for 'Hiss': 0.5714\n",
        "  Gini Impurity for 'Woof': 0.5000\n",
        "  Gini Impurity for 'Meow': 0.4444\n",
        "Weighted Gini for 'Sound': 0.5278\n",
        "\n",
        "  Gini Impurity for 'Short': 0.6667\n",
        "  Gini Impurity for 'Long': 0.4444\n",
        "  Gini Impurity for 'None': 0.6111\n",
        "Weighted Gini for 'Tail': 0.5833\n",
        "\n",
        "  Best Feature: Sound\n",
        "  Unique Values: ['Hiss' 'Woof' 'Meow']\n",
        "  Branch: Sound = Hiss\n",
        "```\n",
        "* **Gini Impurity Calculations:**\n",
        "    * For each feature ('Fur', 'Legs', 'Sound', 'Tail'), the Gini impurity is calculated for each unique value of that feature.\n",
        "    * For each feature, a \"Weighted Gini\" is also calculated.\n",
        "\n",
        "* **Feature-Specific Gini Impurity:**\n",
        "    * For 'Fur', both 'Furry' and 'Not Furry' have a Gini impurity of 0.6667, and the weighted Gini is also 0.6667.\n",
        "    * For 'Legs', '2' has a Gini impurity of 0.5600, '4' has 0.6122, and the weighted Gini is 0.5905.\n",
        "    * For 'Sound', 'Hiss' has a Gini impurity of 0.5714, 'Woof' has 0.5000, 'Meow' has 0.4444, and the weighted Gini is 0.5278.\n",
        "    * For 'Tail', 'Short' has a Gini impurity of 0.6667, 'Long' has 0.4444, 'None' has 0.6111, and the weighted Gini is 0.5833.\n",
        "\n",
        "* **Decision:**\n",
        "    * \"Best Feature\" is identified as 'Sound'.\n",
        "    * \"Unique Values\" for 'Sound' are listed as `['Hiss' 'Woof' 'Meow']`.\n",
        "    * The image states \"Branch: Sound\".\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "The decision tree algorithm selected 'Sound' as the first feature to split on. This selection was based on comparing the \"Weighted Gini\" values for all features. The feature 'Sound' has the lowest weighted Gini (0.5278) compared to 'Fur' (0.6667), 'Legs' (0.5905), and 'Tail' (0.5833).\n",
        "\n",
        "The algorithm aims to minimize the impurity after the split, so it chooses the feature that results in the lowest weighted average impurity.\n",
        "\n",
        "The unique values of 'Sound' indicate that the first split will create branches for 'Hiss', 'Woof', and 'Meow'."
      ],
      "metadata": {
        "id": "dbUUqfKLFmrp"
      },
      "id": "dbUUqfKLFmrp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2c8SLYkaoqu"
      },
      "source": [
        "## Iris"
      ],
      "id": "A2c8SLYkaoqu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Data"
      ],
      "metadata": {
        "id": "DX6wuQOKGwjV"
      },
      "id": "DX6wuQOKGwjV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp0Dmkxtaoqu"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# df['species'] = y\n",
        "# df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('species', axis=1),\n",
        "#                                                     df['species'],\n",
        "#                                                     test_size=0.20,\n",
        "#                                                     random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_train.head())"
      ],
      "id": "Tp0Dmkxtaoqu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier Model and Tree"
      ],
      "metadata": {
        "id": "IwO3nKeyGrVr"
      },
      "id": "IwO3nKeyGrVr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_SKH8naaoqv"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# print(X_train.head())\n",
        "\n",
        "# model = DecisionTreeClassifier(criterion='gini', random_state=42).fit(X_train, y_train)\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "id": "7_SKH8naaoqv"
    },
    {
      "cell_type": "code",
      "source": [
        "# # example gini score on petal length\n",
        "# 1 - np.sum(np.square(y_train.value_counts(normalize=True)))"
      ],
      "metadata": {
        "id": "mqFuzxxwBxzD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mqFuzxxwBxzD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Splits"
      ],
      "metadata": {
        "id": "_reav3APNZhU"
      },
      "id": "_reav3APNZhU"
    },
    {
      "cell_type": "code",
      "source": [
        "# example = X_train.copy()\n",
        "# example['species'] = y_train\n",
        "\n",
        "# sns.pairplot(example, hue='species', palette=['red', 'blue', 'green']);"
      ],
      "metadata": {
        "id": "0qJ02zV6I5y3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0qJ02zV6I5y3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pair Plot**\n",
        "\n",
        "* **Petal Length vs. Other Features:** In every scatter plot where petal length is involved (first row, third column), *setosa* (red) is clearly separated along the petal length axis.\n",
        "* **Petal Width vs. Other Features:** The same is true for petal width (fourth column, second row); *setosa* is well-separated.\n",
        "* **Petal Length vs. Petal Width:** This plot shows *setosa* forming a distinct cluster in the lower-left corner, reinforcing that both features together provide excellent separation."
      ],
      "metadata": {
        "id": "WjQndZ0DN0g2"
      },
      "id": "WjQndZ0DN0g2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCZMsXr2aoqv"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train.hist()\n",
        "# plt.tight_layout();"
      ],
      "id": "xCZMsXr2aoqv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histograms**\n",
        "\n",
        "* **Petal Length Histogram:** The histogram for petal length shows a distinct group on the far left (around 1-2 cm) that corresponds to *setosa* (red in the pair plot). There's a clear separation from the other two species, which have petal lengths mostly greater than 3 cm.\n",
        "* **Petal Width Histogram:** Similarly, the petal width histogram shows *setosa* grouped on the far left (around 0-0.5 cm), well-separated from the *versicolor* and *virginica* distributions.\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "The visual separation in both the histograms and pair plots indicates that *setosa* has consistently smaller petal lengths and widths compared to *versicolor* and *virginica*. This \"natural\" separation means a decision tree can easily use these features to create simple rules (like \"petal length <= a threshold\") to isolate *setosa* with high accuracy, as seen in the decision tree where the first split is on petal length."
      ],
      "metadata": {
        "id": "RudmXpxlMjX4"
      },
      "id": "RudmXpxlMjX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partitioning Examples"
      ],
      "metadata": {
        "id": "lhoZjoBEHBkv"
      },
      "id": "lhoZjoBEHBkv"
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "metadata": {
        "id": "9YbGtuU1OnX4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9YbGtuU1OnX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "where does the decision tree model get the specific values it uses for splitting (like `petal length (cm) <= 2.45`). Let's clarify this:\n",
        "\n",
        "**The Model Learns the Split Values from the Training Data**\n",
        "\n",
        "The decision tree algorithm *learns* these threshold values (e.g., 2.45 for petal length, 1.65 or 1.75 for petal width) directly from the **training data** you provide it.  It doesn't pull them out of thin air or use some pre-defined list.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "1.  **Training Data:**\n",
        "    * You start with a dataset (like the Iris dataset) that contains measurements of features (petal length, petal width, sepal length, sepal width) for a set of samples, along with the known class or category (species) for each sample.\n",
        "    * This dataset is typically split into two parts:\n",
        "        * **Training set:** This is the portion of the data the model uses to learn the relationships between features and the target variable (species).\n",
        "        * **Testing set:** This is a separate portion used to evaluate how well the model generalizes to new, unseen data.\n",
        "\n",
        "2.  **Algorithm's Search for Optimal Splits:**\n",
        "    * The decision tree algorithm (like CART, ID3, or C4.5) examines the training data to find the best way to split the data at each node.\n",
        "    * \"Best\" is defined based on a criterion like:\n",
        "        * **Information Gain (for ID3, C4.5):** How much does this split reduce the entropy (uncertainty) about the target variable?\n",
        "        * **Gini Impurity (for CART):** How much does this split reduce the \"impurity\" (mixing of classes) in the resulting subsets?\n",
        "    * To find the best split, the algorithm essentially tries out different possible threshold values for each feature.\n",
        "    * For example, for 'petal length', it might try splitting at 2.0 cm, 2.1 cm, 2.2 cm, 2.3 cm, 2.4 cm, 2.45 cm, 2.5 cm, and so on, evaluating the Information Gain or Gini Impurity for each potential split.\n",
        "    * The algorithm then selects the threshold value that results in the greatest improvement in the chosen criterion (highest Information Gain or lowest Gini Impurity).\n",
        "\n",
        "3.  **Example with Petal Length <= 2.45:**\n",
        "    * In the tree you provided, the algorithm found that splitting the data at `petal length (cm) <= 2.45` produced the most significant reduction in impurity (or increase in Information Gain) at that stage of the tree building process.\n",
        "    * This means that, among all the possible petal length values, 2.45 cm was the best at separating the Iris species in the training data at that point.\n",
        "\n",
        "4.  **No Predefined Values:**\n",
        "    * It's crucial to understand that the algorithm is *calculating* and *selecting* these values based on the data. It's not using any pre-set list of petal length or width values.\n",
        "    * If you trained the same model on a slightly different training set (a different random sample of the Iris data), the exact split values might be slightly different, although the overall structure of the tree would likely be similar.\n",
        "\n",
        "**In essence:** The decision tree model learns the rules (including the threshold values for splitting) by analyzing patterns in the training data and finding the splits that best organize the data points according to the target variable."
      ],
      "metadata": {
        "id": "HoDRCtXyQt5_"
      },
      "id": "HoDRCtXyQt5_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg1bovfZaoqv"
      },
      "outputs": [],
      "source": [
        "# # plot using hue to show different classes\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.scatterplot(x=example['petal length (cm)'],\n",
        "#                 y=example['petal width (cm)'],\n",
        "#                 hue=example['species'],\n",
        "#                 palette=['red', 'blue', 'green'])\n",
        "# plt.axvline(x=2.45, color='black')\n",
        "# plt.axvline(x=4.75, color='black')\n",
        "# plt.hlines(y=1.75, xmin=2.45, xmax=8, color='black')\n",
        "\n",
        "# plt.xlim(0, 8)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "id": "Sg1bovfZaoqv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS63Jj0maoqw"
      },
      "outputs": [],
      "source": [
        "# # plot using hue to show different classes\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.scatterplot(x=example['petal length (cm)'][example['petal length (cm)']>2.45],\n",
        "#                 y=example['petal width (cm)'],\n",
        "#                 hue=example['species'],\n",
        "#                 palette=['red', 'blue', 'green'])\n",
        "# plt.axhline(y=1.75, color='black')\n",
        "# plt.vlines(x=4.95, ymin=0, ymax=1.75, color='black')\n",
        "# plt.vlines(x=5.45, ymin=0, ymax=1.75, color='black')\n",
        "# plt.axhline(y=1.55, color='black')\n",
        "\n",
        "# plt.xlim(2.45, 8)\n",
        "# plt.ylim(0, )\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "id": "YS63Jj0maoqw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Explanation**\n",
        "\n",
        "The decision tree algorithm aims to partition the Iris dataset by making a series of decisions based on the feature values, ultimately creating subsets that are as pure as possible with respect to the Iris species (setosa, versicolor, and virginica). It prioritizes the features that best separate the species.\n",
        "\n",
        "**Step-by-Step Breakdown with \"I\" Perspective**\n",
        "\n",
        "* **Initial State:** I start with the entire Iris dataset, which, as shown in the histograms (Image 2), has some overlap between species, particularly in petal length and width. This means the dataset isn't perfectly separated from the beginning.\n",
        "* **First Split (Root Node):**\n",
        "    * I observe in the tree (Image 1) that the first split occurs on `petal length (cm) <= 2.45`.\n",
        "    * Looking at the scatter plot (Image 3), I can see that this split effectively isolates the *setosa* species (red points) on the left.\n",
        "    * So, I'm essentially saying: \"If a flower has a petal length of 2.45 cm or less, I confidently classify it as *setosa*.\" This creates a pure *setosa* leaf node.\n",
        "* **Second Split (Right Branch):**\n",
        "    * For the remaining data points (which are *versicolor* and *virginica*), I see that the tree splits on `petal length (cm) <= 4.75`.\n",
        "    * In the scatter plot, this is the vertical line at around 4.75 cm.  This split further divides the dataset, though there's still some mixing of *versicolor* and *virginica*.\n",
        "    * I'm now saying: \"Of the flowers that are *not* *setosa*, if their petal length is 4.75 cm or less, they are *likely* *versicolor*.\"\n",
        "* **Third Split (Left Branch from Second):**\n",
        "    * The tree then splits the *likely versicolor* branch based on `petal width (cm) <= 1.65`.\n",
        "    * This is the horizontal line at 1.65 cm in the scatter plot (Image 3).\n",
        "    * I refine my classification: \"Of the non-*setosa* flowers with petal lengths less than or equal to 4.75 cm, if their petal width is also less than or equal to 1.65 cm, I classify them as *versicolor*.\"  There's still a tiny bit of misclassification here, as seen in the tree's Gini impurity.\n",
        "* **Fourth Split (Right Branch from Second):**\n",
        "    * Finally, the tree splits the other branch (petal length > 4.75 cm) based on `petal width (cm) <= 1.75`.\n",
        "    * This is the horizontal line at 1.75 cm in the scatter plot.\n",
        "    * I conclude: \"The remaining flowers (non-*setosa* with petal lengths greater than 4.75 cm) are classified based on petal width. If the petal width is greater than 1.75 cm, they are classified as *virginica*; otherwise, *versicolor*.\" Again, there's a small amount of misclassification.\n",
        "\n",
        "**Intuitive Summary**\n",
        "\n",
        "The tree essentially creates a series of if-else rules that carve up the data space. It starts with the most important feature (petal length) to make the biggest distinction (separating *setosa*). Then, it refines the classifications using other features (petal width) to separate the remaining species as cleanly as possible. The scatter plots visually show how these rules correspond to dividing the data points into rectangular regions."
      ],
      "metadata": {
        "id": "R2fDPwJsMpGh"
      },
      "id": "R2fDPwJsMpGh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests\n",
        "\n",
        "**Random Forest** is an ensemble supervised learning algorithm that builds multiple decision trees on different subsets of the data and using random subsets of features. For prediction (classification or regression), it aggregates the predictions of all the individual trees, typically through majority voting (classification) or averaging (regression), to improve accuracy and reduce overfitting."
      ],
      "metadata": {
        "id": "i1DFJji1-sf5"
      },
      "id": "i1DFJji1-sf5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Learning\n",
        "\n",
        "In statistics and machine learning, **ensemble methods** use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Ensemble_learning\n",
        "\n",
        "See Ensemble Learning Notebook\n",
        "\n",
        "The traditional way of performing hyperparameter optimization has been **grid search**, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search"
      ],
      "metadata": {
        "id": "YzxoYTH_DMLe"
      },
      "id": "YzxoYTH_DMLe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging and Boosting\n",
        "\n",
        "* Out of bag...\n",
        "* Bagging parallel, boosting sequential"
      ],
      "metadata": {
        "id": "L_8-zzBDn4MI"
      },
      "id": "L_8-zzBDn4MI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfXyPaw4aoq4"
      },
      "source": [
        "### Pruning - Hyperparameters\n",
        "\n",
        "A parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "* https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\n",
        "\n",
        "Here are some default parameters:\n",
        "\n",
        "<pre>\n",
        "hyperparameters = {\n",
        "            'n_estimators': 100,\n",
        "            'criterion': 'gini',\n",
        "            'max_depth': None,\n",
        "            'max_leaf_nodes': None,\n",
        "            'bootstrap': True\n",
        "            }\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "</pre>\n",
        "\n",
        "**Parameters vs Hyperparameters**:\n",
        "* Parameter: Usually estimated or learned from data\n",
        "* Hyperparameter: Values that are tuned by the data scientist"
      ],
      "id": "LfXyPaw4aoq4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54PGK4F5aoq4"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data = load_iris()\n",
        "# X = data.data\n",
        "# y = data.target\n",
        "\n",
        "# iris = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# iris['species'] = y\n",
        "# X_train, X_test, y_train, y_test = train_test_split(iris.drop('species', axis=1),\n",
        "#                                                     iris['species'],\n",
        "#                                                     test_size=0.20,\n",
        "#                                                     random_state=42)"
      ],
      "id": "54PGK4F5aoq4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "995HXwlUaoq5"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# hyperparameters = {\n",
        "#             'criterion': 'entropy'\n",
        "#             }\n",
        "\n",
        "# model = DecisionTreeClassifier(random_state=42).set_params(**hyperparameters)\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "id": "995HXwlUaoq5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9PJck9Yaoq6"
      },
      "source": [
        "The above tree keeps splitting till all the nodes are pure and can lead to overfitting. The next cell introduces some (hyper)parameters that help avoid overfitting."
      ],
      "id": "v9PJck9Yaoq6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa_H_MLvaoq6"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# hyperparameters = {\n",
        "#             'criterion': 'entropy',\n",
        "#             'max_depth': 3,\n",
        "#             'max_leaf_nodes': 4\n",
        "#             }\n",
        "\n",
        "# model = DecisionTreeClassifier(random_state=42).set_params(**hyperparameters)\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "id": "Sa_H_MLvaoq6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqHLJYAaoq6"
      },
      "source": [
        "### Tuning Random Forest (Hyper)Parameters\n",
        "\n",
        "**Focusing on Tree Structure and Complexity:**\n",
        "\n",
        "* **`min_samples_split`**: This controls the minimum number of samples required to split an internal node.\n",
        "    * **Suggested Values:** `[2, 5]`\n",
        "    * **Justification:** A smaller value (like 2) allows for more complex trees that might overfit, while a larger value (like 5) can help prevent overfitting by ensuring nodes only split if they contain a reasonable number of data points. Limiting to two values keeps the search space manageable.\n",
        "* **`min_samples_leaf`**: This controls the minimum number of samples required to be at a leaf node.\n",
        "    * **Suggested Values:** `[1, 3]`\n",
        "    * **Justification:** Similar to `min_samples_split`, a smaller value (like 1) can lead to more complex trees, while a larger value (like 3) can promote more generalized models by requiring a minimum number of samples in the final predictions.\n",
        "* **`max_features`**: This determines the number of features to consider when looking for the best split at each node.\n",
        "    * **Suggested Values:** `['sqrt', 0.5]`\n",
        "    * **Justification:**\n",
        "        * `'sqrt'` (or 'log2') considers the square root of the total number of features. This is a common and often effective default.\n",
        "        * `0.5` considers half of the total number of features. This provides a different level of randomness in feature selection compared to 'sqrt'. Limiting to two options keeps the search efficient.\n",
        "\n",
        "**Hyperparameter Related to Randomness:**\n",
        "\n",
        "* **`random_state`**: While not directly a tuning parameter for the model's complexity, it's crucial for reproducibility.\n",
        "    * **Suggested Value:** `[42]` (or any single integer)\n",
        "    * **Justification:** Setting a `random_state` ensures that the random processes within the Random Forest (like bootstrapping and feature selection) produce the same results each time the code is run. This is important for consistent evaluation and comparison of different hyperparameter settings. You might not include this in the *tuning* grid but should emphasize its importance for good practice.\n",
        "\n",
        "**Considerations for Keeping Execution Time Down:**\n",
        "\n",
        "* **Number of Hyperparameters:** The example you provided has 5 hyperparameters. Adding 3 more (`min_samples_split`, `min_samples_leaf`, `max_features`) will increase the size of the grid significantly (2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 = 256 combinations). Be mindful of this. You might suggest that students initially try a smaller grid and then potentially expand if time allows.\n",
        "* **Range of Values:** Keeping the number of values for each hyperparameter limited is key, as you've already done.\n",
        "\n",
        "```python\n",
        "hyperparameters = {\n",
        "    'n_estimators': [50, 150],  # Slightly reduced max for faster execution\n",
        "    'criterion': ['entropy', 'gini'],\n",
        "    'max_depth': [3, 5],      # Slightly increased max depth\n",
        "    'max_leaf_nodes': [6, 10], # Adjusted range\n",
        "    'bootstrap': [True, False],\n",
        "}\n",
        "```\n",
        "\n",
        "**Important Considerations**\n",
        "\n",
        "* **Justification is Key:** Explain *why* you chose the specific values for each hyperparameter. Your reasoning should be based on your understanding of how these parameters affect the model's bias-variance trade-off, complexity, and potential for overfitting or underfitting.\n",
        "* **Cross-Validation Strategy:** Use an appropriate cross-validation strategy (e.g., Stratified K-Fold since it's likely a classification task with potentially imbalanced classes) to get a reliable estimate of the model's performance for each hyperparameter combination.\n",
        "* **Computational Limits:** Keep in mind the constraints on their hyperparameter choices and grid size and computational time. You might need to start with a smaller grid and iterate if time permits.\n",
        "\n",
        "Random forests create many decision trees that sample data. The bootstrap hyperparameter sets sampling with or without replacement.\n",
        "\n",
        "**Rule**: Never make adjustments to your model based on test set results.\n",
        "            "
      ],
      "id": "wgqHLJYAaoq6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60uMb6Eqaoq7"
      },
      "source": [
        "### Random Forest Model with Grid Search"
      ],
      "id": "60uMb6Eqaoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "920nHbfOaoq7"
      },
      "outputs": [],
      "source": [
        "# # create dataframe from sklearn iris dataset; print shape, info, and head\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import load_iris\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# df['species'] = y\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "id": "920nHbfOaoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WjRu5NLaoq7"
      },
      "outputs": [],
      "source": [
        "# # train test split using 25% for test size; print X_train shape and head\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('species', axis=1),\n",
        "#                                                     df['species'],\n",
        "#                                                     test_size=0.25,\n",
        "#                                                     random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_train.head())"
      ],
      "id": "0WjRu5NLaoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "298UVX_Taoq7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# hyperparameters = {\n",
        "#     'n_estimators': [50, 150],\n",
        "#     'criterion': ['entropy', 'gini'],\n",
        "#     'max_depth': [3, 5],\n",
        "#     'max_leaf_nodes': [6, 10],\n",
        "#     'bootstrap': [True, False],\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(estimator = RandomForestClassifier(),\n",
        "#                            param_grid = hyperparameters,\n",
        "#                            scoring = 'accuracy',\n",
        "#                            cv = 10)\n",
        "\n",
        "# grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# best_accuracy = grid_search.best_score_\n",
        "# best_parameters = grid_search.best_params_\n",
        "\n",
        "# print('best accuracy', best_accuracy)\n",
        "# print('best parameters', best_parameters)"
      ],
      "id": "298UVX_Taoq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrmA7QA2aoq7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # model = RandomForestClassifier(bootstrap = False,\n",
        "# #                                criterion = 'entropy',\n",
        "# #                                max_depth = 3,\n",
        "# #                                min_samples_leaf = 5,\n",
        "# #                                min_samples_split = 4,\n",
        "# #                                random_state = 42)\n",
        "# model = RandomForestClassifier(random_state = 42).set_params(**best_parameters) # * args, ** kwargs\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))"
      ],
      "id": "qrmA7QA2aoq7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isolation Forest"
      ],
      "metadata": {
        "id": "7IfLCUq_tEYc"
      },
      "id": "7IfLCUq_tEYc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outliers"
      ],
      "metadata": {
        "id": "e7eJp0aKtGLh"
      },
      "id": "e7eJp0aKtGLh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are data points that deviate significantly from the general pattern or distribution of the rest of the data. They are observations that are far removed from the \"typical\" or \"expected\" values.\n",
        "\n",
        "**Types of Outliers:**\n",
        "\n",
        "* **Point Outliers:** Single data points that are unusual compared to the rest of the data (e.g., a very high income in a neighborhood).\n",
        "* **Contextual Outliers:** Data points that are outliers only in a specific context (e.g., high electricity usage at night in a residential area).\n",
        "* **Collective Outliers:** A group of data points that are unusual when considered together, even if individual points might not be outliers on their own (e.g., a coordinated series of small fraudulent transactions).\n",
        "\n",
        "**Why are Outliers Important?**\n",
        "\n",
        "* **Data Quality:** They can indicate errors in data collection or measurement.\n",
        "* **Insights:** They can reveal valuable information about rare events or unusual behavior (e.g., fraud, network intrusions).\n",
        "* **Model Accuracy:** They can distort statistical analyses and machine learning models.\n",
        "\n",
        "**Python Methods for Outlier Detection:**\n",
        "\n",
        "Here are some common Python methods for detecting outliers:\n",
        "\n",
        "**1. Statistical Methods:**\n",
        "\n",
        "   * **Z-Score:**\n",
        "        * Calculates how many standard deviations a data point is away from the mean.\n",
        "        * Points with a Z-score above a certain threshold (e.g., 3) are considered outliers."
      ],
      "metadata": {
        "id": "d_Xx8KpKuy9Z"
      },
      "id": "d_Xx8KpKuy9Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# normal_values = np.random.normal(loc=15, scale=3, size=20)\n",
        "# outlier_values = [50, 60]\n",
        "# data = {'value': np.concatenate([normal_values, outlier_values])}\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# df['z_score'] = np.abs(stats.zscore(df['value']))\n",
        "# df['is_outlier'] = df['z_score'] > 3\n",
        "\n",
        "# mean = df['value'].mean()\n",
        "# std = df['value'].std()\n",
        "\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# sns.histplot(df['value'], kde=True)\n",
        "# plt.axvline(mean, color='green', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n",
        "# plt.axvline(mean + std, color='orange', linestyle='dashed', linewidth=1, label=f'+1 Std Dev: {mean + std:.2f}')\n",
        "# plt.axvline(mean - std, color='orange', linestyle='dashed', linewidth=1, label=f'-1 Std Dev: {mean - std:.2f}')\n",
        "# plt.axvline(mean + 2 * std, color='cyan', linestyle='dashed', linewidth=1, label=f'+2 Std Dev: {mean + 2 * std:.2f}')\n",
        "# plt.axvline(mean - 2 * std, color='cyan', linestyle='dashed', linewidth=1, label=f'-2 Std Dev: {mean - 2 * std:.2f}')\n",
        "# plt.axvline(mean + 3 * std, color='magenta', linestyle='dashed', linewidth=1, label=f'+3 Std Dev: {mean + 3 * std:.2f}')\n",
        "# plt.axvline(mean - 3 * std, color='magenta', linestyle='dashed', linewidth=1, label=f'-3 Std Dev: {mean - 3 * std:.2f}')\n",
        "# # plt.axvline(df[df['is_outlier'] == True]['value'].min(), color='red', linestyle='dashed', linewidth=1, label='Outlier Bounds')\n",
        "# # plt.axvline(df[df['is_outlier'] == True]['value'].max(), color='red', linestyle='dashed', linewidth=1)\n",
        "# plt.title('Distribution of Data with Outliers and Standard Deviations')\n",
        "# plt.xlabel('Value')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "mS8gDvWLxgh7"
      },
      "id": "mS8gDvWLxgh7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **IQR (Interquartile Range):**\n",
        "  * Defines a \"normal\" range using the 1st quartile (Q1) and 3rd quartile (Q3).\n",
        "  * Points outside 1.5 * IQR from Q1 or Q3 are considered outliers."
      ],
      "metadata": {
        "id": "95PcaB-Huy9c"
      },
      "id": "95PcaB-Huy9c"
    },
    {
      "cell_type": "code",
      "source": [
        "# def find_iqr_outliers(data):\n",
        "#   q1 = np.percentile(data, 25)\n",
        "#   q3 = np.percentile(data, 75)\n",
        "#   iqr = q3 - q1\n",
        "#   lower_bound = q1 - 1.5 * iqr\n",
        "#   upper_bound = q3 + 1.5 * iqr\n",
        "#   return data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "# outliers = find_iqr_outliers(df['value'])\n",
        "# print(outliers)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.boxplot(y=df['value'])\n",
        "# plt.title('Box Plot with IQR Outlier Visualization')\n",
        "# plt.ylabel('Value')\n",
        "# plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FVflRMtTuy9d"
      },
      "id": "FVflRMtTuy9d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Machine Learning Methods:**\n",
        "\n",
        "   * **Isolation Forest:**\n",
        "        * An unsupervised algorithm that isolates outliers by randomly partitioning the data space.\n",
        "        * Anomalies are \"easier\" to isolate and have shorter path lengths in the trees."
      ],
      "metadata": {
        "id": "toMjFZ2huy9d"
      },
      "id": "toMjFZ2huy9d"
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# model = IsolationForest(contamination=0.1)  # Adjust contamination\n",
        "# model.fit(df[['value']])\n",
        "# df['anomaly'] = model.predict(df[['value']])\n",
        "# outliers = df[df['anomaly'] == -1]\n",
        "# print(outliers)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "u8tntt0huy9d"
      },
      "id": "u8tntt0huy9d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anomaly Detection Through Isolation"
      ],
      "metadata": {
        "id": "vhzFtXW2zeUn"
      },
      "id": "vhzFtXW2zeUn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isolation Forest is an unsupervised machine learning algorithm specifically designed for **anomaly detection**. Unlike many other anomaly detection methods that try to model \"normal\" data and then identify deviations, Isolation Forest takes a different approach: it explicitly tries to **isolate** anomalies.\n",
        "\n",
        "Here's a breakdown of the core ideas behind Isolation Forest:\n",
        "\n",
        "**1. The Principle of Isolation:**\n",
        "\n",
        "* Anomalies are data points that are \"few and different.\" This means they have attribute values that are far from the typical values of the majority of the data.\n",
        "* Because anomalies are different, they should be easier to separate (isolate) from the rest of the data with fewer partitioning steps.\n",
        "\n",
        "**2. Isolation Trees (iTrees):**\n",
        "\n",
        "* Isolation Forest builds an ensemble (a \"forest\") of **Isolation Trees (iTrees)**.\n",
        "* Each iTree is a binary tree constructed by randomly partitioning the data. The partitioning process is as follows:\n",
        "    * **Random Feature Selection:** A feature (attribute) from the dataset is randomly selected.\n",
        "    * **Random Split Value Selection:** A split value within the range of the selected feature is randomly chosen.\n",
        "    * **Data Partitioning:** The data points are divided into two child nodes based on whether their value for the chosen feature is below or above the split value.\n",
        "    * This process is repeated recursively until each data point is isolated in its own leaf node or a predefined tree height limit is reached.\n",
        "\n",
        "**3. Path Length:**\n",
        "\n",
        "* The key concept in Isolation Forest is the **path length** of a data point in an iTree. The path length is the number of edges traversed from the root of the tree to the leaf node containing that data point.\n",
        "* **Anomalies tend to have shorter path lengths.** This is because they are different and can be isolated with fewer random partitions.\n",
        "* **Normal data points tend to have longer path lengths** because they are similar to other points and require more partitions to be isolated.\n",
        "\n",
        "**4. Anomaly Score:**\n",
        "\n",
        "* To determine an anomaly score for each data point, the algorithm calculates the average path length of that point across all the iTrees in the forest.\n",
        "* This average path length is then normalized to produce an **anomaly score** between 0 and 1:\n",
        "    * Scores closer to 1 indicate a higher likelihood of being an anomaly.\n",
        "    * Scores closer to 0 indicate that the data point is likely normal.\n",
        "    * A score around 0.5 typically means the point is neither clearly an anomaly nor a normal instance.\n",
        "\n",
        "**In essence, Isolation Forest works by:**\n",
        "\n",
        "* Randomly partitioning the data in multiple trees.\n",
        "* Measuring how quickly each data point gets isolated.\n",
        "* Assigning an anomaly score based on the average isolation path length across all trees. Data points that are isolated in fewer steps are considered more likely to be anomalies.\n",
        "\n",
        "**Advantages of Isolation Forest:**\n",
        "\n",
        "* **Efficient:** It has a linear time complexity with respect to the number of data points and features, making it suitable for large datasets.\n",
        "* **Fast:** The random partitioning process is computationally inexpensive.\n",
        "* **Effective for High-Dimensional Data:** It can handle datasets with many features.\n",
        "* **Unsupervised:** It doesn't require labeled anomaly data for training.\n",
        "* **Robust to Irrelevant Features:** The random feature selection helps in focusing on the features that contribute to isolating anomalies.\n",
        "* **Low Memory Usage:** The memory requirements are relatively low compared to some other anomaly detection methods.\n",
        "\n",
        "**Use Cases in Cybersecurity (and beyond):**\n",
        "\n",
        "* **Network Intrusion Detection:** Identifying unusual network traffic patterns that might indicate attacks.\n",
        "* **Fraud Detection:** Detecting anomalous financial transactions.\n",
        "* **Endpoint Security:** Identifying unusual process behavior or file modifications on computers.\n",
        "* **System Monitoring:** Detecting abnormal resource usage or system events.\n",
        "* **Industrial Anomaly Detection:** Identifying faulty machinery behavior based on sensor data.\n",
        "\n",
        "By understanding the principles of isolation, path length, and anomaly scoring, you can effectively use and explain the Isolation Forest algorithm in your cybersecurity class for analyzing network packets and detecting anomalies."
      ],
      "metadata": {
        "id": "W-L7ffXf-ur5"
      },
      "id": "W-L7ffXf-ur5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The dataset we are using is a subset from a Machine Learning competition run by Xeek and FORCE 2020 (Bormann et al., 2020). It is released under a NOLD 2.0 licence from the Norwegian Government, details of which can be found here: Norwegian Licence for Open Government Data (NLOD) 2.0.\n",
        "\n",
        "The full dataset can be accessed at the following link: https://doi.org/10.5281/zenodo.4351155."
      ],
      "metadata": {
        "id": "YoqjnuvfJrnK"
      },
      "id": "YoqjnuvfJrnK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b490772f",
      "metadata": {
        "id": "b490772f"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/refs/heads/main/Xeek_Well.csv')\n",
        "# df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc81b8e",
      "metadata": {
        "id": "efc81b8e"
      },
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Context:**\n",
        "\n",
        "The image shows the output of the `df.describe()` method in Python's Pandas library after reading a CSV file named 'Xeek_Well_15-9-15.csv' into a DataFrame called `df`. This method provides descriptive statistics of the numerical columns in the DataFrame.\n",
        "\n",
        "**Dataset Description:**\n",
        "\n",
        "The dataset appears to contain well log data, likely from an oil or gas exploration context. Each row represents a measurement taken at a specific depth in the well, and the columns represent various geophysical measurements.\n",
        "\n",
        "**Columns and Their Descriptive Statistics:**\n",
        "\n",
        "Here's a detailed explanation of each column and the statistics provided:\n",
        "\n",
        "* **DEPTH_MD:**\n",
        "    * This column likely represents the measured depth in meters (or feet) in the well.\n",
        "    * **count:** 17717 - There are 17,717 data points (depth measurements).\n",
        "    * **mean:** 1837.363674 - The average depth is approximately 1837.36 meters.\n",
        "    * **std:** 784.314256 - The standard deviation indicates the spread or variability of the depth measurements.\n",
        "    * **min:** 485.256000 - The minimum depth recorded is 485.256 meters.\n",
        "    * **25%:** 1158.464000 - 25% of the depth measurements are less than or equal to 1158.464 meters.\n",
        "    * **50%:** 1831.672000 - The median depth is 1831.672 meters (half the measurements are above, half are below).\n",
        "    * **75%:** 2515.672000 - 75% of the depth measurements are less than or equal to 2515.672 meters.\n",
        "    * **max:** 3200.128000 - The maximum depth recorded is 3200.128 meters.\n",
        "\n",
        "* **CALI:**\n",
        "    * This column likely represents the caliper log, which measures the diameter of the wellbore.\n",
        "    * **count:** 17635.000000 - There are 17,635 caliper measurements (note: fewer than DEPTH_MD, indicating missing data).\n",
        "    * **mean:** 14.006030 - The average caliper measurement is 14.006030 (units depend on the tool).\n",
        "    * **std:** 3.873367 - The standard deviation indicates the variability in wellbore diameter.\n",
        "    * **min:** 7.325138 - The minimum caliper measurement.\n",
        "    * **25%:** 12.045594\n",
        "    * **50%:** 13.956721\n",
        "    * **75%:** 17.324830\n",
        "    * **max:** 25.717396\n",
        "\n",
        "* **RDEP:**\n",
        "    * This column likely represents the deep resistivity log, which measures the electrical resistance of the formation far from the wellbore.\n",
        "    * **count:** 17717.000000\n",
        "    * **mean:** 1.474893\n",
        "    * **std:** 1.356896\n",
        "    * **min:** 0.264479\n",
        "    * **25%:** 0.760341\n",
        "    * **50%:** 1.007371\n",
        "    * **75%:** 1.554278\n",
        "    * **max:** 14.046203\n",
        "\n",
        "* **RHOB:**\n",
        "    * This column likely represents the bulk density log, which measures the density of the formation.\n",
        "    * **count:** 17521.000000 - Again, missing data.\n",
        "    * **mean:** 2.134858\n",
        "    * **std:** 0.223124\n",
        "    * **min:** 1.438999\n",
        "    * **25%:** 1.978679\n",
        "    * **50%:** 2.042522\n",
        "    * **75%:** 2.333431\n",
        "    * **max:** 2.648847\n",
        "\n",
        "* **GR:**\n",
        "    * This column likely represents the gamma ray log, which measures the natural radioactivity of the formation.\n",
        "    * **count:** 17717.000000\n",
        "    * **mean:** 59.154202\n",
        "    * **std:** 29.483140\n",
        "    * **min:** 6.024419\n",
        "    * **25%:** 41.260944\n",
        "    * **50%:** 62.451527\n",
        "    * **75%:** 75.398460\n",
        "    * **max:** 804.298950 - This high max value suggests potential outliers or unusual readings.\n",
        "\n",
        "* **NPHI:**\n",
        "    * This column likely represents the neutron porosity log, which measures the porosity (pore space) of the formation.\n",
        "    * **count:** 13346.000000 - Significant missing data.\n",
        "    * **mean:** 0.384906\n",
        "    * **std:** 0.152182\n",
        "    * **min:** 0.039013\n",
        "    * **25%:** 0.249594\n",
        "    * **50%:** 0.451589\n",
        "    * **75%:** 0.510851\n",
        "    * **max:** 0.733152\n",
        "\n",
        "* **PEF:**\n",
        "    * This column likely represents the photoelectric effect log, which measures the formation's response to gamma rays.\n",
        "    * **count:** 17662.000000\n",
        "    * **mean:** 4.095857\n",
        "    * **std:** 8.318817 - A relatively high standard deviation suggests significant variability.\n",
        "    * **min:** 1.525528\n",
        "    * **25%:** 2.400372\n",
        "    * **50%:** 2.910137\n",
        "    * **75%:** 4.222030\n",
        "    * **max:** 365.575592 - The extremely high max value points to outliers.\n",
        "\n",
        "* **DTC:**\n",
        "    * This column likely represents the compressional wave slowness (Delta-T Compressional) log, which measures the time it takes for a compressional sound wave to travel through the formation.\n",
        "    * **count:** 17708.000000\n",
        "    * **mean:** 127.240157\n",
        "    * **std:** 36.507057\n",
        "    * **min:** 7.415132\n",
        "    * **25%:** 88.318405\n",
        "    * **50%:** 142.943245\n",
        "    * **75%:** 153.226116\n",
        "    * **max:** 207.382553\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "* **Missing Data:** The `CALI`, `RHOB`, and especially `NPHI` columns have fewer data points than `DEPTH_MD`, indicating missing values. This needs to be addressed before using the data for analysis or modeling.\n",
        "* **Outliers:** The `GR` and `PEF` columns have very high maximum values, suggesting potential outliers that could skew statistical analyses.\n",
        "* **Data Ranges:** The columns have vastly different ranges and units, which is typical for well log data. This often necessitates scaling or normalization before applying machine learning algorithms.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "This dataset contains well log measurements with varying statistical properties and issues like missing data and outliers. Understanding these characteristics is crucial for proper data cleaning, preprocessing, and analysis to extract meaningful insights about the subsurface formation."
      ],
      "metadata": {
        "id": "7sKEErqzLBdK"
      },
      "id": "7sKEErqzLBdK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e12079a",
      "metadata": {
        "id": "4e12079a"
      },
      "outputs": [],
      "source": [
        "# print(df.shape)\n",
        "# df = df.dropna()\n",
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6857727",
      "metadata": {
        "id": "d6857727"
      },
      "outputs": [],
      "source": [
        "# anomaly_inputs = ['NPHI', 'RHOB']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa5f8a8",
      "metadata": {
        "id": "6aa5f8a8"
      },
      "outputs": [],
      "source": [
        "# model_IF = IsolationForest(contamination=float(0.1),random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0370370",
      "metadata": {
        "id": "a0370370"
      },
      "outputs": [],
      "source": [
        "# model_IF.fit(df[anomaly_inputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **`anomaly_inputs = ['NPHI', 'RHOB']`**:\n",
        "    * This line creates a list named `anomaly_inputs`.\n",
        "    * The list contains two strings: 'NPHI' and 'RHOB'.\n",
        "    * These strings likely represent column names in a Pandas DataFrame.\n",
        "    * The code indicates that these two columns ('NPHI' and 'RHOB') will be used as the input features for the anomaly detection model.\n",
        "\n",
        "2.  **`model_IF = IsolationForest(contamination=float(0.1), random_state=42)`**:\n",
        "    * This line creates an instance of the `IsolationForest` class from scikit-learn.\n",
        "    * The instance is assigned to the variable `model_IF`.\n",
        "    * `contamination=float(0.1)`: This parameter tells the Isolation Forest algorithm to expect that approximately 10% of the data points are anomalies. It helps the algorithm determine a threshold for classifying data points as anomalies.\n",
        "    * `random_state=42`: This parameter sets the random seed for the algorithm. Setting a random seed ensures that the results of the algorithm are reproducible. If you run the code multiple times with the same random seed, you will get the same results.\n",
        "\n",
        "3.  **`model_IF.fit(df[anomaly_inputs])`**:\n",
        "    * This line trains the `model_IF` (the Isolation Forest model) on the data.\n",
        "    * `df[anomaly_inputs]`: This selects the columns specified in the `anomaly_inputs` list (i.e., 'NPHI' and 'RHOB') from a Pandas DataFrame called `df`. It creates a new DataFrame containing only these two columns.\n",
        "    * `model_IF.fit(...)`: The `fit()` method is called on the Isolation Forest model to learn the patterns of \"normal\" data from the selected columns.\n",
        "\n",
        "In summary, the code selects two columns from a DataFrame, configures an Isolation Forest model to detect anomalies assuming 10% contamination, and then trains the model on the selected data."
      ],
      "metadata": {
        "id": "1JRAe_WnLzUP"
      },
      "id": "1JRAe_WnLzUP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477e8d20",
      "metadata": {
        "id": "477e8d20"
      },
      "outputs": [],
      "source": [
        "# df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "# df['anomaly'] = model_IF.predict(df[anomaly_inputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3876d9",
      "metadata": {
        "id": "ab3876d9"
      },
      "outputs": [],
      "source": [
        "# df.loc[:, ['NPHI', 'RHOB','anomaly_scores','anomaly'] ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code performs anomaly detection using a trained Isolation Forest model on a Pandas DataFrame. It calculates anomaly scores and assigns anomaly labels to the data.\n",
        "\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "1.  **`df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])`**:\n",
        "    * This line calculates an anomaly score for each data point using the trained Isolation Forest model (`model_IF`).\n",
        "    * `model_IF.decision_function(df[anomaly_inputs])`:\n",
        "        * `model_IF`: This is the trained Isolation Forest model.\n",
        "        * `decision_function()`: This is a method of the Isolation Forest model that calculates an \"anomaly score\" for each data point. The anomaly score reflects how \"isolated\" a data point is; lower scores generally indicate higher likelihood of being an anomaly.\n",
        "        * `df[anomaly_inputs]`: This selects the columns from the DataFrame `df` that were used as input features during the model training. These are the same columns for which we now want to calculate anomaly scores.\n",
        "    * `df['anomaly_scores'] = ...`: The calculated anomaly scores are then assigned to a new column named `anomaly_scores` in the DataFrame `df`.\n",
        "\n",
        "2.  **`df['anomaly'] = model_IF.predict(df[anomaly_inputs])`**:\n",
        "    * This line assigns anomaly labels to each data point based on the anomaly scores calculated in the previous step.\n",
        "    * `model_IF.predict(df[anomaly_inputs])`:\n",
        "        * `model_IF`: The trained Isolation Forest model.\n",
        "        * `predict()`: This is a method of the Isolation Forest model that assigns a label to each data point. Typically, it assigns:\n",
        "            * `1`: for normal data points (inliers).\n",
        "            * `-1`: for anomalous data points (outliers).\n",
        "        * `df[anomaly_inputs]`: Again, this selects the input feature columns from the DataFrame.\n",
        "    * `df['anomaly'] = ...`: The predicted anomaly labels (1 or -1) are assigned to a new column named `anomaly` in the DataFrame `df`.\n",
        "\n",
        "3.  **`df.loc[:, ['NPHI', 'RHOB', 'anomaly_scores', 'anomaly']]`**:\n",
        "    * This line selects and displays specific columns from the DataFrame `df`.\n",
        "    * `df.loc[:, ... ]`: This is a way to select rows and columns by label in Pandas.\n",
        "        * `:`: Selects all rows.\n",
        "        * `['NPHI', 'RHOB', 'anomaly_scores', 'anomaly']`: Selects the columns named 'NPHI', 'RHOB', 'anomaly_scores', and 'anomaly'.\n",
        "    * The result of this line is a new DataFrame (or a view of the original DataFrame) containing only the specified columns, which is then displayed. This allows you to see the original data ('NPHI', 'RHOB') along with the calculated anomaly information.\n",
        "\n",
        "In summary, the code calculates anomaly scores and labels using the Isolation Forest model and then displays a subset of the DataFrame to show the original data and the anomaly results."
      ],
      "metadata": {
        "id": "_jgYx3bnMNx9"
      },
      "id": "_jgYx3bnMNx9"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# print(\"Mean values for normal and anomalous data:\")\n",
        "# print(df.groupby('anomaly')[['NPHI', 'RHOB', 'anomaly_scores']].mean())\n",
        "\n",
        "# print(\"\\nStandard deviations:\")\n",
        "# print(df.groupby('anomaly')[['NPHI', 'RHOB', 'anomaly_scores']].std())\n",
        "\n",
        "# print(\"\\nDescriptive statistics for normal data:\")\n",
        "# print(df[df['anomaly'] == 1][['NPHI', 'RHOB']].describe())  # Normal data\n",
        "\n",
        "# print(\"\\nDescriptive statistics for anomalous data:\")\n",
        "# print(df[df['anomaly'] == -1][['NPHI', 'RHOB']].describe()) # Anomalous data\n",
        "\n",
        "# # Box plots to compare distributions\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.boxplot(x='anomaly', y='NPHI', data=df)\n",
        "# plt.title('Box Plot of NPHI by Anomaly Group')\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.boxplot(x='anomaly', y='RHOB', data=df)\n",
        "# plt.title('Box Plot of RHOB by Anomaly Group')\n",
        "# plt.show()\n",
        "\n",
        "# # Histograms to see frequency distributions\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(data=df, x='anomaly_scores', hue='anomaly', kde=True)\n",
        "# plt.title('Distribution of Anomaly Scores')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "tK3NlgjaMvQK"
      },
      "id": "tK3NlgjaMvQK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c57a5bc0",
      "metadata": {
        "id": "c57a5bc0"
      },
      "outputs": [],
      "source": [
        "# def outlier_plot(data, outlier_method_name, x_var, y_var,\n",
        "#                  xaxis_limits=[0,1], yaxis_limits=[0,1]):\n",
        "\n",
        "#     print(f'Outlier Method: {outlier_method_name}')\n",
        "\n",
        "#     # Create a dynamic title based on the method\n",
        "#     method = f'{outlier_method_name}_anomaly'\n",
        "\n",
        "#     # Print out key statistics\n",
        "#     print(f\"Number of anomalous values {len(data[data['anomaly']==-1])}\")\n",
        "#     print(f\"Number of non anomalous values  {len(data[data['anomaly']== 1])}\")\n",
        "#     print(f'Total Number of Values: {len(data)}')\n",
        "\n",
        "#     # Create the chart using seaborn\n",
        "#     g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[1,-1])\n",
        "#     g.map(sns.scatterplot, x_var, y_var)\n",
        "#     g.fig.suptitle(f'Outlier Method: {outlier_method_name}', y=1.10, fontweight='bold')\n",
        "#     g.set(xlim=xaxis_limits, ylim=yaxis_limits)\n",
        "#     axes = g.axes.flatten()\n",
        "#     axes[0].set_title(f\"Outliers\\n{len(data[data['anomaly']== -1])} points\")\n",
        "#     axes[1].set_title(f\"Inliers\\n {len(data[data['anomaly']==  1])} points\")\n",
        "#     return g"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Function Definition:**\n",
        "\n",
        "```python\n",
        "def outlier_plot(data, outlier_method_name, x_var, y_var, xaxis_limits=[0,1], yaxis_limits=[0,1]):\n",
        "```\n",
        "\n",
        "* This line defines a function named `outlier_plot`.\n",
        "* It takes several arguments:\n",
        "    * `data`: The Pandas DataFrame containing the data.\n",
        "    * `outlier_method_name`: A string representing the name of the outlier detection method used (e.g., \"Isolation Forest\").\n",
        "    * `x_var`: The name of the column in the DataFrame to be used for the x-axis of the plot.\n",
        "    * `y_var`: The name of the column in the DataFrame to be used for the y-axis of the plot.\n",
        "    * `xaxis_limits`: (Optional) A list specifying the x-axis limits for the plot. It defaults to [0, 1].\n",
        "    * `yaxis_limits`: (Optional) A list specifying the y-axis limits for the plot. It defaults to [0, 1].\n",
        "\n",
        "**2. Print Outlier Method Name:**\n",
        "\n",
        "```python\n",
        "print(f'Outlier Method: {outlier_method_name}')\n",
        "```\n",
        "\n",
        "* This line prints the name of the outlier detection method to the console. This helps in identifying the results.\n",
        "\n",
        "**3. Create a Dynamic Title:**\n",
        "\n",
        "```python\n",
        "method = f'{outlier_method_name}_anomaly'\n",
        "```\n",
        "\n",
        "* This line creates a string variable `method` by appending \"_anomaly\" to the `outlier_method_name`. This is likely used to create a more descriptive title for the plot.\n",
        "\n",
        "**4. Print Key Statistics:**\n",
        "\n",
        "```python\n",
        "print(f\"Number of anomalous values {len(data[data['anomaly']== -1])}\")\n",
        "print(f\"Number of non anomalous values {len(data[data['anomaly']== 1])}\")\n",
        "print(f\"Total Number of Values: {len(data)}\")\n",
        "```\n",
        "\n",
        "* These lines print some important statistics about the detected anomalies:\n",
        "    * `len(data[data['anomaly'] == -1])`: Calculates the number of data points labeled as anomalous (assuming -1 represents anomalies).\n",
        "    * `len(data[data['anomaly'] == 1])`: Calculates the number of data points labeled as non-anomalous (assuming 1 represents normal data).\n",
        "    * `len(data)`: Prints the total number of data points in the DataFrame.\n",
        "\n",
        "**5. Create the Chart using Seaborn:**\n",
        "\n",
        "```python\n",
        "g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[1,-1])\n",
        "g.map(sns.scatterplot, x_var, y_var)\n",
        "```\n",
        "\n",
        "* This section uses the Seaborn library to create a visualization:\n",
        "    * `g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[1,-1])`: Creates a `FacetGrid`. This is a multi-plot grid that allows you to create separate plots for different subsets of your data.\n",
        "        * `data`: The DataFrame.\n",
        "        * `col='anomaly'`:  Specifies that the grid should have columns based on the unique values in the 'anomaly' column (i.e., it will create one subplot for anomalies and one for non-anomalies).\n",
        "        * `height=4`: Sets the height of each subplot.\n",
        "        * `hue='anomaly'`:  Colors the data points based on the 'anomaly' column.\n",
        "        * `hue_order=[1,-1]`:  Specifies the order of the hues (colors) in the legend.\n",
        "    * `g.map(sns.scatterplot, x_var, y_var)`:  Maps a scatter plot onto each facet (subplot) of the grid.\n",
        "        * `sns.scatterplot`:  The type of plot (a scatter plot).\n",
        "        * `x_var`:  The column to use for the x-axis.\n",
        "        * `y_var`:  The column to use for the y-axis.\n",
        "\n",
        "**6. Set Chart Titles and Labels:**\n",
        "\n",
        "```python\n",
        "g.fig.suptitle(f'Outlier Method: {outlier_method_name}', y=1.10, fontweight='bold')\n",
        "g.set(xlim=xaxis_limits, ylim=yaxis_limits)\n",
        "\n",
        "axes = g.axes.flatten()\n",
        "axes[0].set_title(f\"Outliers\\n{len(data[data['anomaly'] == -1])} points\")\n",
        "axes[1].set_title(f\"Inliers\\n{len(data[data['anomaly'] == 1])} points\")\n",
        "```\n",
        "\n",
        "* `g.fig.suptitle(...)`: Sets the overall title of the entire figure (the grid of plots).\n",
        "    * `f'Outlier Method: {outlier_method_name}'`:  Uses an f-string to include the name of the outlier detection method in the title.\n",
        "    * `y=1.10`:  Adjusts the vertical position of the title.\n",
        "    * `fontweight='bold'`:  Makes the title bold.\n",
        "* `g.set(xlim=xaxis_limits, ylim=yaxis_limits)`:  Sets the x-axis and y-axis limits for all subplots, using the `xaxis_limits` and `yaxis_limits` arguments passed to the function.\n",
        "* `axes = g.axes.flatten()`:  Gets a 1D array of all the axes (subplots) in the grid.\n",
        "* `axes[0].set_title(...)`: Sets the title of the first subplot (likely the one showing outliers). It includes the number of outlier points.\n",
        "* `axes[1].set_title(...)`: Sets the title of the second subplot (likely the one showing inliers/normal data). It includes the number of inlier points.\n",
        "\n",
        "**7. Return the Plot:**\n",
        "\n",
        "```python\n",
        "return g\n",
        "```\n",
        "\n",
        "* The function returns the `FacetGrid` object (`g`), which can be further customized or displayed.\n",
        "\n",
        "**In summary, this `outlier_plot` function takes data, the outlier detection method name, and two variables to plot, and then generates a visualization that:**\n",
        "\n",
        "* Shows the data points in a scatter plot.\n",
        "* Separates the data into two plots: one for outliers and one for normal data.\n",
        "* Provides titles and labels that clearly indicate the outlier detection method and the number of points in each group.\n",
        "\n",
        "This function is a useful tool for visualizing and understanding the results of outlier detection algorithms."
      ],
      "metadata": {
        "id": "tNBYP9jBNUot"
      },
      "id": "tNBYP9jBNUot"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d80ca8a",
      "metadata": {
        "id": "1d80ca8a"
      },
      "outputs": [],
      "source": [
        "# outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Function Call:**\n",
        "\n",
        "    ```python\n",
        "    outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);\n",
        "    ```\n",
        "\n",
        "    * The `outlier_plot` function is called.\n",
        "    * It's passed the following arguments:\n",
        "        * `df`:  A Pandas DataFrame containing the data.\n",
        "        * `'Isolation Forest'`:  The name of the outlier detection method used.\n",
        "        * `'NPHI'`:  The column from the DataFrame to be used for the x-axis of the plot.\n",
        "        * `'RHOB'`: The column from the DataFrame to be used for the y-axis of the plot.\n",
        "        * `[0, 0.8]`:  The x-axis limits for the plot.\n",
        "        * `[3, 1.5]`: The y-axis limits for the plot.\n",
        "\n",
        "2.  **Function Execution:**\n",
        "\n",
        "    Inside the `outlier_plot` function (as defined in a previous snippet), the following actions are performed:\n",
        "\n",
        "    * **Print Outlier Method Name:** The string 'Outlier Method: Isolation Forest' is printed.\n",
        "    * **Print Key Statistics:**\n",
        "        * The number of anomalous values is calculated and printed. This is done by counting the rows in the DataFrame where the 'anomaly' column is equal to -1.\n",
        "        * The number of non-anomalous values is calculated and printed. This is done by counting the rows in the DataFrame where the 'anomaly' column is equal to 1.\n",
        "        * The total number of values (rows) in the DataFrame is printed.\n",
        "    * **Create the Chart:**\n",
        "        * A Seaborn `FacetGrid` is created, which will generate a multi-plot grid. The data is split into two columns based on the 'anomaly' column. Data points are colored based on the 'anomaly' column.\n",
        "        * A scatter plot is mapped onto each facet of the grid, using 'NPHI' for the x-axis and 'RHOB' for the y-axis.\n",
        "        * Titles and labels are set for the figure and the individual subplots. The number of anomalous and non-anomalous points is included in the subplot titles.\n",
        "\n",
        "3.  **Output:**\n",
        "\n",
        "    The function generates a visualization (and prints some statistics) that shows:\n",
        "\n",
        "    * A scatter plot of 'NPHI' versus 'RHOB'.\n",
        "    * Two separate plots: one for data points identified as outliers, and one for data points identified as inliers (normal data).\n",
        "    * The number of data points classified as outliers and inliers.\n",
        "    * The name of the outlier detection method used (Isolation Forest).\n",
        "\n",
        "In summary, this code segment calls a plotting function to visualize and summarize the results of an Isolation Forest anomaly detection analysis, displaying the relationship between two variables ('NPHI' and 'RHOB') and highlighting the separation between detected outliers and normal data."
      ],
      "metadata": {
        "id": "qJSrQwIsOD59"
      },
      "id": "qJSrQwIsOD59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e350e18",
      "metadata": {
        "id": "1e350e18"
      },
      "outputs": [],
      "source": [
        "# model_IF = IsolationForest(contamination=float(0.3), random_state=42)\n",
        "# model_IF.fit(df[anomaly_inputs])\n",
        "\n",
        "# df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "# df['anomaly'] = model_IF.predict(df[anomaly_inputs])\n",
        "# outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Model Initialization and Training:**\n",
        "\n",
        "    ```python\n",
        "    model_IF = IsolationForest(contamination=float(0.3), random_state=42)\n",
        "    model_IF.fit(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * An Isolation Forest model (`model_IF`) is created.\n",
        "    * `contamination=float(0.3)`: This parameter is set, indicating the model is expected to find approximately 30% of the data points as anomalies.\n",
        "    * `random_state=42`: A seed is set for the random number generator, ensuring the model's behavior is reproducible across different runs.\n",
        "    * `model_IF.fit(df[anomaly_inputs])`: The model is trained using the data specified by the `anomaly_inputs` variable. This means the model learns what \"normal\" data looks like.\n",
        "\n",
        "2.  **Anomaly Score and Label Assignment:**\n",
        "\n",
        "    ```python\n",
        "    df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "    df['anomaly'] = model_IF.predict(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * `df['anomaly_scores'] = ...`:  Anomaly scores are calculated for each data point using the trained model's `decision_function()`. Lower scores indicate a higher likelihood of being an anomaly. These scores are stored in a new column named `anomaly_scores` in the DataFrame.\n",
        "    * `df['anomaly'] = ...`: Anomaly labels are assigned to each data point using the trained model's `predict()` method.  The typical output is 1 for normal data points and -1 for anomalies. These labels are stored in a new column named `anomaly`.\n",
        "\n",
        "3.  **Visualization (Implied):**\n",
        "\n",
        "    ```python\n",
        "    outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);\n",
        "    ```\n",
        "\n",
        "    * A function `outlier_plot` is called to visualize the results.\n",
        "    * This function likely generates a scatter plot of the data, separating the data points labeled as anomalies from those labeled as normal.\n",
        "    * The plot uses the 'NPHI' and 'RHOB' columns as x and y axes, respectively.\n",
        "    * The x-axis limits are set to [0, 0.8], and the y-axis limits are set to [3, 1.5].\n",
        "    * The function also likely prints some summary statistics.\n",
        "\n",
        "4.  **Output Statistics:**\n",
        "\n",
        "    ```\n",
        "    Outlier Method: Isolation Forest\n",
        "    Number of anomalous values 3986\n",
        "    Number of non anomalous values 9304\n",
        "    Total Number of Values: 13290\n",
        "    ```\n",
        "\n",
        "    * These lines print:\n",
        "        * The name of the method used: \"Isolation Forest.\"\n",
        "        * The number of data points classified as anomalies: 3986.\n",
        "        * The number of data points classified as non-anomalous: 9304.\n",
        "        * The total number of data points: 13290.\n",
        "\n",
        "**In essence, the code:**\n",
        "\n",
        "1.  Trains an Isolation Forest model on a subset of the data.\n",
        "2.  Uses that model to assign anomaly scores and labels to the entire dataset.\n",
        "3.  Visualizes the results and provides summary statistics about the number of anomalies detected.\n",
        "\n",
        "The key takeaway is that the Isolation Forest algorithm identifies data points that deviate significantly from the learned \"normal\" patterns, and these deviations are then highlighted and quantified."
      ],
      "metadata": {
        "id": "YaM9D_49OfNE"
      },
      "id": "YaM9D_49OfNE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The `contamination` Parameter**\n",
        "\n",
        "The primary driver of the difference in the number of anomalous values is the `contamination` parameter of the `IsolationForest` algorithm in scikit-learn.\n",
        "\n",
        "* **What it does:** The `contamination` parameter is an estimate of the proportion of outliers in the dataset. It's a crucial parameter that influences how the Isolation Forest algorithm determines the threshold for classifying data points as anomalies.\n",
        "* **How it affects the results:**\n",
        "    * If you set a higher `contamination` value (e.g., 0.3, meaning you expect 30% outliers), the algorithm will be more aggressive in flagging data points as anomalies. It will find more points that it considers \"different\" to reach that target.\n",
        "    * If you set a lower `contamination` value (e.g., 0.01, meaning you expect 1% outliers), the algorithm will be more conservative and flag fewer points as anomalies. It will only identify the most extreme deviations.\n",
        "\n",
        "**Why the Numbers Change in Your Examples**\n",
        "\n",
        "The different numbers of anomalous and non-anomalous values indicate that the `contamination` parameter was likely set to different values in the different runs of the Isolation Forest algorithm.\n",
        "\n",
        "* **Example 1 (1329 Anomalies):** In the first example, the code shows:\n",
        "\n",
        "    ```\n",
        "    Number of anomalous values 1329\n",
        "    Number of non anomalous values 11961\n",
        "    Total Number of Values: 13290\n",
        "    ```\n",
        "\n",
        "    This suggests a relatively low contamination setting, as only a small fraction of the 13290 points are classified as outliers.\n",
        "\n",
        "* **Example 2 (3986 Anomalies):** In the second example, the code shows:\n",
        "\n",
        "    ```\n",
        "    Number of anomalous values 3986\n",
        "    Number of non anomalous values 9304\n",
        "    Total Number of Values : 13290\n",
        "    ```\n",
        "\n",
        "    This indicates a higher contamination setting, as a significantly larger proportion of the data is now labeled as anomalous.\n",
        "\n",
        "**Important Considerations**\n",
        "\n",
        "* **Choosing `contamination`:**\n",
        "    * Ideally, you should have some prior knowledge or estimate of the actual proportion of outliers in your data.\n",
        "    * If you don't have a good estimate, you might need to experiment with different `contamination` values and evaluate the results based on your domain knowledge and the visual patterns in the data.\n",
        "* **Impact on Interpretation:** The `contamination` parameter heavily influences how you interpret the results. A higher value might highlight more subtle deviations, while a lower value will focus on the most extreme outliers.\n",
        "\n",
        "In conclusion, the difference in the number of detected anomalies is directly caused by the different settings of the `contamination` parameter in the Isolation Forest algorithm."
      ],
      "metadata": {
        "id": "JDAmVdicQPYJ"
      },
      "id": "JDAmVdicQPYJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a20a6a",
      "metadata": {
        "id": "17a20a6a"
      },
      "outputs": [],
      "source": [
        "# anomaly_inputs = ['NPHI', 'RHOB', 'GR', 'CALI', 'PEF', 'DTC']\n",
        "# model_IF = IsolationForest(contamination=0.1, random_state=42)\n",
        "# model_IF.fit(df[anomaly_inputs])\n",
        "# df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "# df['anomaly'] = model_IF.predict(df[anomaly_inputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8338481",
      "metadata": {
        "id": "a8338481"
      },
      "outputs": [],
      "source": [
        "# outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Feature Selection:**\n",
        "\n",
        "    ```python\n",
        "    anomaly_inputs = ['NPHI', 'RHOB', 'GR', 'CALI', 'PEF', 'DTC']\n",
        "    ```\n",
        "\n",
        "    * A list named `anomaly_inputs` is created, containing the names of six columns: 'NPHI', 'RHOB', 'GR', 'CALI', 'PEF', and 'DTC'. These column names likely represent different well log measurements. This means the Isolation Forest model will now consider these six variables to identify anomalies, instead of just 'NPHI' and 'RHOB' as in previous examples.\n",
        "\n",
        "2.  **Model Initialization and Training:**\n",
        "\n",
        "    ```python\n",
        "    model_IF = IsolationForest(contamination=0.1, random_state=42)\n",
        "    model_IF.fit(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * An Isolation Forest model (`model_IF`) is created.\n",
        "    * `contamination=0.1`: The model is configured to expect 10% of the data points to be anomalies.\n",
        "    * `random_state=42`: A random seed is set for reproducibility.\n",
        "    * `model_IF.fit(df[anomaly_inputs])`: The model is trained using the specified columns from the DataFrame `df`.\n",
        "\n",
        "3.  **Anomaly Score and Label Assignment:**\n",
        "\n",
        "    ```python\n",
        "    df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n",
        "    df['anomaly'] = model_IF.predict(df[anomaly_inputs])\n",
        "    ```\n",
        "\n",
        "    * `df['anomaly_scores'] = ...`: Anomaly scores are calculated for each data point using the trained model. Lower scores indicate a higher likelihood of being an anomaly. These scores are stored in the `anomaly_scores` column.\n",
        "    * `df['anomaly'] = ...`: Anomaly labels (1 for normal, -1 for anomaly) are assigned to each data point and stored in the `anomaly` column.\n",
        "\n",
        "4.  **Visualization (Implied):**\n",
        "\n",
        "    ```python\n",
        "    outlier_plot(df, 'Isolation Forest', 'NPHI', 'RHOB', [0, 0.8], [3, 1.5]);\n",
        "    ```\n",
        "\n",
        "    * The `outlier_plot` function is called to visualize the results.\n",
        "    * It's passed the DataFrame, the method name, 'NPHI' and 'RHOB' as the variables to plot, and the x and y axis limits.\n",
        "\n",
        "5.  **Output Statistics:**\n",
        "\n",
        "    ```\n",
        "    Outlier Method: Isolation Forest\n",
        "    Number of anomalous values 1329\n",
        "    Number of non anomalous values 11961\n",
        "    Total Number of Values: 13290\n",
        "    ```\n",
        "\n",
        "    * The output provides:\n",
        "        * The name of the method: \"Isolation Forest.\"\n",
        "        * The number of anomalous values: 1329.\n",
        "        * The number of non-anomalous values: 11961.\n",
        "        * The total number of values: 13290.\n",
        "\n",
        "**Key Differences from Previous Examples:**\n",
        "\n",
        "The main difference here is the **input features** used for anomaly detection. In this case, the Isolation Forest model is trained and makes predictions based on all six columns: 'NPHI', 'RHOB', 'GR', 'CALI', 'PEF', and 'DTC'. Previous examples might have used only 'NPHI' and 'RHOB'.\n",
        "\n",
        "This change in input features can significantly affect the results of the anomaly detection. The model now considers more information to determine what is \"normal\" and what is \"anomalous.\""
      ],
      "metadata": {
        "id": "cyf6KrZSQ09C"
      },
      "id": "cyf6KrZSQ09C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d1fea4",
      "metadata": {
        "id": "52d1fea4"
      },
      "outputs": [],
      "source": [
        "# palette = ['#ff7f0e', '#1f77b4']\n",
        "# sns.pairplot(df, vars=anomaly_inputs, hue='anomaly', palette=palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose of a Pairplot**\n",
        "\n",
        "A pairplot is a powerful visualization tool that helps you understand the relationships between multiple variables in your dataset. It creates a matrix of plots:\n",
        "\n",
        "* **Diagonal Subplots:** On the diagonal, you'll typically find the univariate distribution of each variable (e.g., histograms or kernel density estimates). This shows you the shape and spread of individual variables.\n",
        "* **Off-Diagonal Subplots:** The off-diagonal subplots are scatter plots. Each scatter plot shows the relationship between two different variables.\n",
        "\n",
        "**Why Pairplots Are Useful in Anomaly Detection**\n",
        "\n",
        "1.  **Visualizing Relationships:**\n",
        "    * Anomaly detection often relies on the assumption that anomalies deviate from the normal relationships between variables. Pairplots help you visually identify these deviations.\n",
        "    * For example, if two variables are normally strongly correlated, anomalies might appear as points that fall far off the general trend.\n",
        "\n",
        "2.  **Identifying Potential Separability:**\n",
        "    * Pairplots can reveal if anomalies tend to cluster in certain regions of the data space. This can help you understand if the anomaly detection algorithm is effectively separating anomalies from normal data.\n",
        "\n",
        "3.  **Feature Analysis:**\n",
        "    * Pairplots can give you insights into which features might be most useful for distinguishing anomalies. Variables that show clear separation between normal and anomalous points in the scatter plots are likely to be important.\n",
        "\n",
        "4.  **Data Exploration:**\n",
        "    * Before or after anomaly detection, pairplots are helpful for general data exploration. They can reveal patterns, correlations, and potential issues (e.g., outliers) in the data itself, which might be relevant to the anomaly detection process.\n",
        "\n",
        "**In the context of the image, the pairplot is likely used to:**\n",
        "\n",
        "* Visually explore the relationships between the features used in the anomaly detection.\n",
        "* See how the anomalies (likely colored differently, as indicated by the legend) are distributed with respect to these relationships.\n",
        "* Gain a better understanding of the characteristics that make the anomalies different from the normal data.\n",
        "\n",
        "**Left Plot (Vertical Pattern):**\n",
        "\n",
        "* **X-axis:** This axis has a very limited range, with most data points clustered at the leftmost side. The scale suggests it might represent a variable with low values.\n",
        "* **Y-axis:** This axis has a wider range of values.\n",
        "* **Relationship:** There's a strong vertical pattern.\n",
        "    * The \"normal\" data (blue) is concentrated in a narrow band on the left.\n",
        "    * The \"anomalies\" (orange) are scattered more widely along the vertical axis, but mostly on the right side of the blue cluster.\n",
        "\n",
        "    **Interpretation:** This suggests that the variable on the X-axis is a strong discriminator between normal and anomalous data. The normal data has a very specific, low range of values for this variable. The anomalous data has a wider range, including higher values, indicating that deviations in this variable are key to identifying outliers.\n",
        "\n",
        "**Right Plot (Diagonal/Curved Pattern):**\n",
        "\n",
        "* **X-axis:** This axis has a wider range of values.\n",
        "* **Y-axis:** This axis also has a wider range of values.\n",
        "* **Relationship:** There's a more complex, diagonal or curved relationship.\n",
        "    * The \"normal\" data (blue) forms a fairly dense, elongated cluster with a noticeable curve.\n",
        "    * The \"anomalies\" (orange) are scattered around this cluster, often forming a boundary or a less dense outer ring.\n",
        "\n",
        "    **Interpretation:** This indicates that both variables are important for distinguishing anomalies. The normal data exhibits a specific, non-linear correlation. The anomalous data deviates from this relationship, suggesting that outliers have unusual combinations of values for these two variables.\n",
        "\n",
        "**Overall Implications for Anomaly Detection:**\n",
        "\n",
        "* These plots show that the anomaly detection algorithm is likely identifying outliers based on deviations from the typical relationships between the variables.\n",
        "* The left plot suggests that one variable (X-axis) might be a stronger indicator of anomalies when it has higher values.\n",
        "* The right plot implies that a combination of both variables, specifically deviations from their curved relationship, is important for spotting outliers.\n",
        "\n",
        "To give you a more specific interpretation, I would need to know what the X and Y axes represent (the names of the variables)."
      ],
      "metadata": {
        "id": "2BiX3e7rR_YY"
      },
      "id": "2BiX3e7rR_YY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare / Contrast Random Forest and Isolation Forest\n",
        "Random Forest and Isolation Forest are both powerful ensemble learning algorithms that utilize decision trees. However, they are designed for fundamentally different tasks: **classification/regression** (Random Forest) and **anomaly detection** (Isolation Forest). Here's a comparison and contrast of the two:\n",
        "\n",
        "**Random Forest:**\n",
        "\n",
        "* **Primary Task:** Supervised learning for **classification** (predicting a categorical label) and **regression** (predicting a continuous value).\n",
        "* **Goal:** To build a robust and accurate predictive model by learning the relationships between features and the target variable from labeled data.\n",
        "* **Mechanism:**\n",
        "    * Constructs multiple decision trees (a \"forest\") on different subsets of the training data (bootstrapping).\n",
        "    * When building each tree, it considers only a random subset of features at each split.\n",
        "    * The final prediction is made by aggregating the predictions of all the trees (e.g., majority vote for classification, average for regression).\n",
        "* **Key Characteristics:**\n",
        "    * **Supervised Learning:** Requires labeled training data (features and corresponding target variable).\n",
        "    * **Prediction-Focused:** Aims to predict the value of a target variable for new, unseen data.\n",
        "    * **Ensemble of Deep Trees:** Individual trees can grow relatively deep to capture complex relationships.\n",
        "    * **Feature Importance:** Can provide insights into which features are most important for the prediction task.\n",
        "    * **Handles High Dimensionality:** Performs well with a large number of features.\n",
        "    * **Robust to Overfitting:** The ensemble nature and random feature selection help prevent overfitting to the training data.\n",
        "* **Output:** A predicted class label (classification) or a predicted numerical value (regression).\n",
        "\n",
        "**Isolation Forest:**\n",
        "\n",
        "* **Primary Task:** Unsupervised learning for **anomaly detection** (identifying data points that deviate significantly from the normal data).\n",
        "* **Goal:** To isolate anomalous data points more easily than normal data points.\n",
        "* **Mechanism:**\n",
        "    * Builds multiple isolation trees (iTrees) by randomly partitioning the data.\n",
        "    * For each tree, it randomly selects a feature and then randomly selects a split value within the range of that feature.\n",
        "    * Anomalies, being rare and different, tend to be isolated in fewer splits (shorter path length in the trees).\n",
        "    * A score is calculated for each data point based on its average path length across all the iTrees. Shorter average path lengths indicate a higher likelihood of being an anomaly.\n",
        "* **Key Characteristics:**\n",
        "    * **Unsupervised Learning:** Does not require labeled anomaly data. It learns the \"normal\" data distribution implicitly.\n",
        "    * **Isolation-Focused:** Explicitly tries to isolate anomalies rather than modeling the normal data.\n",
        "    * **Ensemble of Shallow Trees:** Individual trees are typically kept shallow to isolate anomalies quickly.\n",
        "    * **Anomaly Score:** Outputs an anomaly score for each data point, indicating its degree of abnormality.\n",
        "    * **Effective for High-Dimensional Data:** Can handle datasets with many features.\n",
        "    * **Efficient:** Generally has lower computational complexity compared to some other anomaly detection methods.\n",
        "* **Output:** An anomaly score (typically between -1 and 1, where values closer to -1 indicate a higher probability of being an anomaly).\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature           | Random Forest                       | Isolation Forest                     |\n",
        "| :---------------- | :---------------------------------- | :----------------------------------- |\n",
        "| **Primary Task** | Classification, Regression          | Anomaly Detection                    |\n",
        "| **Learning Type** | Supervised                          | Unsupervised                         |\n",
        "| **Goal** | Prediction of a target variable     | Isolation of anomalies               |\n",
        "| **Data Labels** | Requires labeled data               | Does not require labeled anomalies   |\n",
        "| **Tree Depth** | Typically deep                      | Typically shallow                    |\n",
        "| **Splitting** | Based on maximizing information gain (or minimizing error) w.r.t. target | Based on random feature and split value |\n",
        "| **Output** | Class label or numerical value      | Anomaly score                        |\n",
        "| **Anomaly Handling** | Can be used for anomaly detection if anomalies are labeled as a separate class, but not its primary purpose. | Specifically designed for anomaly detection. |\n",
        "\n",
        "**Similarities:**\n",
        "\n",
        "* **Ensemble Methods:** Both algorithms rely on creating an ensemble (forest) of decision trees.\n",
        "* **Tree-Based:** Both utilize decision tree structures as their base learners.\n",
        "* **Randomness:** Both introduce randomness in the tree building process (subsampling of data and/or features) to improve robustness and generalization.\n",
        "* **Handle High Dimensionality:** Both can effectively handle datasets with a large number of features.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "* **Random Forest learns the relationship between features and a known outcome.** It's like learning what a \"cat\" looks like from labeled images of cats and non-cats.\n",
        "* **Isolation Forest learns what \"normal\" data looks like by seeing how easily data points can be isolated.** Anomalies are those points that are easily separated because they are different from the majority. It's like finding the unusual-looking object in a collection of similar items without explicitly defining what \"unusual\" means beforehand.\n",
        "\n",
        "Understanding these fundamental differences is crucial for choosing the appropriate algorithm for a given machine learning task. If you have labeled data and want to predict a specific outcome, Random Forest is likely the better choice. If you have unlabeled data and want to identify rare and unusual instances, Isolation Forest is a more suitable algorithm."
      ],
      "metadata": {
        "id": "QgtJURtdnje5"
      },
      "id": "QgtJURtdnje5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNSW_NB15 Isolation Forest\n",
        "\n",
        "https://colab.research.google.com/drive/1O07CpGmQLe9BzI3QLd2KnH3wWD_MMiEB?usp=sharing"
      ],
      "metadata": {
        "id": "zyapEr3u96yt"
      },
      "id": "zyapEr3u96yt"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}